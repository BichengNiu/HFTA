# -*- coding: utf-8 -*-
print("[DEBUG] Script starting...")
import sys
import os

# --- BEGIN UPDATED SYS.PATH SETUP ---
# Get the directory of the current file (run_nowcasting_evolution.py, which is in news_analysis folder)
current_script_dir = os.path.dirname(os.path.abspath(__file__))
# Get the DFM directory (parent of news_analysis)
dfm_directory = os.path.abspath(os.path.join(current_script_dir, '..'))
# Get the project root directory (parent of DFM)
project_root_dir = os.path.abspath(os.path.join(dfm_directory, '..', '..'))
# Get the dashboard directory (parent of DFM)
dashboard_actual_dir = os.path.abspath(os.path.join(dfm_directory, '..'))

# Add project root directory to sys.path
if project_root_dir not in sys.path:
    sys.path.insert(0, project_root_dir)
    print(f"[run_nowcasting_evolution] Added project_root ('{project_root_dir}') to sys.path for local modules.")

# ğŸ”¥ å…³é”®ä¿®å¤ï¼šåˆ›å»ºæ¨¡å—åˆ«åä»¥å…¼å®¹joblibæ–‡ä»¶ä¸­çš„æ—§è·¯å¾„
try:
    import train_model.DynamicFactorModel as DynamicFactorModel
    import train_model.DiscreteKalmanFilter as DiscreteKalmanFilter
    sys.modules['DynamicFactorModel'] = DynamicFactorModel
    sys.modules['DiscreteKalmanFilter'] = DiscreteKalmanFilter
    print("[run_nowcasting_evolution] æ¨¡å—åˆ«åå·²è®¾ç½®ï¼Œå¯å…¼å®¹æ—§çš„joblibæ–‡ä»¶")
except ImportError as e:
    print(f"[run_nowcasting_evolution] æ¨¡å—åˆ«åè®¾ç½®å¤±è´¥: {e}")

# Add DFM directory to sys.path for potential imports from DFM or other subdirectories like news_analysis
if dfm_directory not in sys.path:
    sys.path.insert(0, dfm_directory)
    # print(f"[run_nowcasting_evolution] Added dfm_directory ('{dfm_directory}') to sys.path.")

# Add dashboard directory to sys.path
if dashboard_actual_dir not in sys.path:
    sys.path.insert(0, dashboard_actual_dir)

# --- å¯¼å…¥ä¿®å¤ ---
# å¤„ç†é…ç½®æ–‡ä»¶å¯¼å…¥ï¼ˆä¹‹å‰æœ‰ç›¸å…³é€»è¾‘ï¼‰
# ä½¿ç”¨æœ¬åœ°é…ç½®ï¼Œé¡¹ç›®å·²å®Œå…¨è¿ç§»åˆ° dashboard ä½“ç³»
try:
    # æ‰€æœ‰åŠŸèƒ½å·²è¿ç§»åˆ°æœ¬åœ° train_modelï¼Œæ— éœ€å¤–éƒ¨ä¾èµ–
    # æˆ‘ä»¬ä½¿ç”¨ç›¸å¯¹å¯¼å…¥æˆ–è€…é€šè¿‡ dashboard. å‰ç¼€çš„ç»å¯¹å¯¼å…¥
    pass  
except ImportError:
    pass

# å°è¯•å¯¼å…¥æ–°è·¯å¾„ï¼ˆä»é‡æ„åçš„dashboardç»“æ„å¯¼å…¥ï¼‰
try:
    # ç›´æ¥ä»æœ¬åœ°æ¨¡å—å¯¼å…¥
    from train_model.DynamicFactorModel import DFMEMResultsWrapper
    from train_model.DiscreteKalmanFilter import KalmanFilter, KalmanFilterResultsWrapper # éœ€è¦é‡æ–°è¿è¡Œ KF
    from data_prep.data_preparation import prepare_data, load_mappings # ä»ç„¶éœ€è¦åŠ è½½å…ƒæ•°æ®
    from data_prep.data_preparation import apply_stationarity_transforms # å¯èƒ½ä¸éœ€è¦äº†ï¼Œä½†ä¿ç•™ä»¥é˜²ä¸‡ä¸€
    DFMEMResultsWrapper_import_ok = True
    print("[run_nowcasting_evolution] æˆåŠŸä»æœ¬åœ°æ¨¡å—è·¯å¾„å¯¼å…¥")
except ImportError as e_local:
    print(f"[run_nowcasting_evolution] æ— æ³•ä»æœ¬åœ°æ¨¡å—è·¯å¾„å¯¼å…¥: {e_local}")
    try:
        # å¤‡ç”¨ï¼šå°è¯•ç›´æ¥å¯¼å…¥ï¼ˆå¦‚æœæ–‡ä»¶åœ¨åŒä¸€ç›®å½•æˆ–sys.pathä¸­ï¼‰
        from DynamicFactorModel import DFMEMResultsWrapper
        from DiscreteKalmanFilter import KalmanFilter, KalmanFilterResultsWrapper
        print("[run_nowcasting_evolution] æˆåŠŸä½¿ç”¨å¤‡ç”¨å¯¼å…¥è·¯å¾„")
        DFMEMResultsWrapper_import_ok = True
        # å¯¹äºdata_prepæ¨¡å—ï¼Œå¯èƒ½éœ€è¦ä¸åŒçš„å¤„ç†
        try:
            from data_preparation import prepare_data, load_mappings, apply_stationarity_transforms
            print("[run_nowcasting_evolution] æˆåŠŸå¯¼å…¥data_preparationæ¨¡å—")
        except ImportError:
            print("[run_nowcasting_evolution] è­¦å‘Šï¼šæ— æ³•å¯¼å…¥data_preparationæ¨¡å—ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½å—é™")
    except ImportError as e_backup:
        print(f"[run_nowcasting_evolution] å¤‡ç”¨å¯¼å…¥ä¹Ÿå¤±è´¥: {e_backup}")
        DFMEMResultsWrapper_import_ok = False

# --- END UPDATED SYS.PATH SETUP ---

"""
run_nowcasting_evolution.py

åŠ è½½æœ€ç»ˆçš„ DFM æ¨¡å‹ç»“æœï¼Œé‡æ–°è¿è¡Œ Kalman æ»¤æ³¢å™¨ä»¥è·å–ä¸­é—´çŠ¶æ€ï¼Œ
è®¡ç®—ç‰¹å®šç›®æ ‡æœˆä»½çš„ Nowcast æ¼”å˜å’Œæ–°é—»è´¡çŒ®ï¼Œå¹¶ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ã€‚
"""

print("[DEBUG] Importing standard libraries...")
import pandas as pd
import numpy as np
import pickle
import joblib # ç”¨äºåŠ è½½æ¨¡å‹å¯¹è±¡
import tempfile
from datetime import datetime, timedelta
import plotly.graph_objects as go # <<< æ–°å¢ Plotly
from plotly.subplots import make_subplots # <<< æ–°å¢ Plotly (å¦‚æœéœ€è¦)
import plotly.io as pio # <<< æ–°å¢ Plotly
import seaborn as sns # No longer needed (comment out or remove if truly unused)
import argparse
from typing import Optional, Dict, List, Tuple
from collections import defaultdict
import unicodedata
print("[DEBUG] Standard libraries imported.")

# --- å¯¼å…¥å¿…è¦çš„ç±» ---
print("[DEBUG] Importing custom classes...")
try:
    # è‡ªèº«çš„ Nowcasting ç±» - ä¸å†ç›´æ¥ä½¿ç”¨ DFMNowcastModel çš„ apply å’Œ news
    # from nowcasting.DFM_Nowcasting import DFMNowcastModel # <--- æ­¤å¯¼å…¥å¦‚æœ DFM_Nowcasting.py åŒåœ¨ news_analysis/ ä¸‹ï¼Œéœ€è¦è°ƒæ•´
    # --- è°ƒæ•´å¯¹ DFM_Nowcasting çš„å¯¼å…¥è·¯å¾„ (å¦‚æœå®ƒä¹Ÿè¢«ç§»åŠ¨åˆ° news_analysis) ---
    # å‡è®¾ DFM_Nowcasting.py ä¸æ­¤è„šæœ¬åœ¨åŒä¸€ç›®å½• (news_analysis/)
    from DFM_Nowcasting import DFMNowcastModel 

    # ä»æœ¬åœ°æ¨¡å—å¯¼å…¥
    # from dashboard.DFM.train_model.DynamicFactorModel import DFMEMResultsWrapper
    # from dashboard.DFM.train_model.DiscreteKalmanFilter import KalmanFilter, KalmanFilterResultsWrapper # éœ€è¦é‡æ–°è¿è¡Œ KF
    # from dashboard.DFM.data_prep.data_preparation import prepare_data, load_mappings # ä»ç„¶éœ€è¦åŠ è½½å…ƒæ•°æ®
    # from dashboard.DFM.data_prep.data_preparation import apply_stationarity_transforms # å¯èƒ½ä¸éœ€è¦äº†ï¼Œä½†ä¿ç•™ä»¥é˜²ä¸‡ä¸€
    # print("[run_nowcasting_evolution] Successfully imported modules from local dashboard") # ç§»é™¤é‡å¤æ‰“å°ä¿¡æ¯
except ImportError as e:
    print(f"å¯¼å…¥æœ¬åœ°æ¨¡å—å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿æœ¬åœ°æ¨¡å—å­˜åœ¨ä¸”åœ¨ Python è·¯å¾„ä¸­ï¼Œå¹¶ä¸” DFM_Nowcasting.py åœ¨æ­¤è„šæœ¬åŒçº§ç›®å½•ã€‚")
    sys.exit(1)

# --- å¯¼å…¥é…ç½®æ–‡ä»¶ ---
print("[DEBUG] Importing config file...")
try:
    # é¦–å…ˆå°è¯•å¯¼å…¥DFMé…ç½®æ¨¡å—
    import config as dfm_config
    print("[run_nowcasting_evolution] Successfully imported DFM config module")
    
    # åˆ›å»ºé…ç½®åŒ…è£…ç±»ä»¥ä¿æŒå…¼å®¹æ€§
    class ConfigWrapper:
        def __init__(self):
            # ä»DFMé…ç½®æ–‡ä»¶å¯¼å…¥æ‰€éœ€çš„é…ç½®
            self.TARGET_VARIABLE = dfm_config.DataDefaults.TARGET_VARIABLE
            self.TARGET_FREQ = 'M'  # æœˆåº¦é¢‘ç‡
            self.SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
            self.NOWCAST_MODEL_INPUT_DIR = 'models'
            self.NOWCAST_MODEL_FILENAME = 'final_dfm_model.joblib'
            self.NOWCAST_METADATA_FILENAME = 'final_model_metadata.pkl'
            self.DEFAULT_MODEL_FREQUENCY_FOR_NEWS = dfm_config.NewsAnalysisDefaults.DEFAULT_MODEL_FREQUENCY
            # ğŸ”¥ ä¿®å¤ï¼šä¸å†ä½¿ç”¨å›ºå®šè¾“å‡ºç›®å½•ï¼Œä½¿ç”¨ä¸´æ—¶ç›®å½•
            self.NOWCAST_EVOLUTION_OUTPUT_DIR = tempfile.mkdtemp(prefix="nowcast_evolution_")
            
    config = ConfigWrapper()
    print("[run_nowcasting_evolution] Using DFM config with wrapper")
    
except ImportError as e_import:
    print(f"[run_nowcasting_evolution] æ— æ³•å¯¼å…¥DFMé…ç½®æ¨¡å—: {e_import}")
    print("[run_nowcasting_evolution] å›é€€åˆ°ç¡¬ç¼–ç é…ç½®")
    
    # å›é€€åˆ°ç¡¬ç¼–ç é…ç½®
    class LocalConfig:
        TARGET_VARIABLE = 'è§„æ¨¡ä»¥ä¸Šå·¥ä¸šå¢åŠ å€¼:å½“æœˆåŒæ¯”'
        TARGET_FREQ = 'M'  # æœˆåº¦é¢‘ç‡
        SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
        NOWCAST_MODEL_INPUT_DIR = 'models'
        NOWCAST_MODEL_FILENAME = 'final_dfm_model.joblib'
        NOWCAST_METADATA_FILENAME = 'final_model_metadata.pkl'
        DEFAULT_MODEL_FREQUENCY_FOR_NEWS = 'M'
        NOWCAST_EVOLUTION_OUTPUT_DIR = tempfile.mkdtemp(prefix="nowcast_evolution_")
        
    config = LocalConfig()
    print("[run_nowcasting_evolution] Using fallback hardcoded config")
    
except Exception as e_config:
    print(f"é”™è¯¯ï¼šé…ç½®è®¾ç½®å¤±è´¥: {e_config}")
    sys.exit(1)

# --- é…ç½®ç»˜å›¾ä¸­æ–‡æ˜¾ç¤º --- 
print("[DEBUG] Configuring matplotlib fonts...")
try:
    # plt.rcParams["font.sans-serif"] = [config.PLOT_FONT_FAMILY] # <<< ç§»é™¤ Matplotlib ç›¸å…³é…ç½®
    # plt.rcParams["axes.unicode_minus"] = False # <<< ç§»é™¤ Matplotlib ç›¸å…³é…ç½®
    # print("[DEBUG] Matplotlib fonts configured using font:", config.PLOT_FONT_FAMILY) # <<< ç§»é™¤
    print("[DEBUG] Matplotlib font configuration skipped as Plotly is used.")
except Exception as e_font:
    # print(f"[DEBUG] WARNING: Failed to set Chinese font '{config.PLOT_FONT_FAMILY}': {e_font}") # <<< ç§»é™¤
    # print(f"è­¦å‘Šï¼šè®¾ç½®ä¸­æ–‡å­—ä½“å¤±è´¥: {e_font}ã€‚å›¾å½¢ä¸­çš„ä¸­æ–‡å¯èƒ½æ— æ³•æ­£ç¡®æ˜¾ç¤ºã€‚") # <<< ç§»é™¤
    print(f"[DEBUG] WARNING: Error during font configuration (though likely not critical for Plotly): {e_font}")

# --- é…ç½® --- (ä» config.py åŠ è½½)
print("[DEBUG] Loading configuration from config module...")
# --- ä¿®æ”¹ï¼šTUNING_OUTPUT_DIR å°†ç”±æ–°çš„å‘½ä»¤è¡Œå‚æ•°æ§åˆ¶ï¼Œæ­¤å¤„ä¿ç•™é»˜è®¤æˆ–ä½œä¸ºåå¤‡ --- 
DEFAULT_TUNING_OUTPUT_DIR = os.path.join(config.SCRIPT_DIR, config.NOWCAST_MODEL_INPUT_DIR)
MODEL_FILENAME = config.NOWCAST_MODEL_FILENAME
METADATA_FILENAME = config.NOWCAST_METADATA_FILENAME
TARGET_VARIABLE = config.TARGET_VARIABLE
TARGET_FREQ = config.TARGET_FREQ
print("[DEBUG] Configuration loaded.")
# print(f"[DEBUG]   Default TUNING_OUTPUT_DIR: {DEFAULT_TUNING_OUTPUT_DIR}") # å°†åœ¨ä¸»é€»è¾‘ä¸­ç¡®å®šå®é™…ä½¿ç”¨çš„è·¯å¾„
print(f"[DEBUG]   MODEL_FILENAME: {MODEL_FILENAME}")
print(f"[DEBUG]   METADATA_FILENAME: {METADATA_FILENAME}")
print(f"[DEBUG]   TARGET_VARIABLE: {TARGET_VARIABLE}")

# --- è¾…åŠ©å‡½æ•°ï¼šè·å–è¡Œä¸šæ˜ å°„ ---
def get_industry_group(var_name: str, mapping: Dict[str, str]) -> str:
    norm_name = unicodedata.normalize('NFKC', str(var_name)).strip().lower()
    return mapping.get(norm_name, 'å…¶ä»–æœªåˆ†ç±»')

# --- ç»˜å›¾å‡½æ•°å®šä¹‰ (ä½¿ç”¨ Plotly é‡å†™) --- 
def plot_news_decomposition(
    input_dir: str,
    # output_file: str, # å°†æ˜¯ .html æ–‡ä»¶ (ä¿®æ”¹ï¼šæ­¤å‚æ•°å°†ä½œä¸ºåŸºç¡€å)
    base_output_filename: str, # ä¾‹å¦‚ï¼šnews_analysis_plot_backend (ä¸å«æ‰©å±•å)
    output_dir: str, # è¾“å‡ºç›®å½•
    plot_start_date: Optional[str] = None,
    plot_end_date: Optional[str] = None,
    target_variable_name: str = "ç›®æ ‡å˜é‡"
) -> Dict[str, Optional[str]]: # è¿”å›åŒ…å«ä¸¤ä¸ªå›¾è¡¨è·¯å¾„çš„å­—å…¸
    print(f"  [Plotting] å¼€å§‹ç”Ÿæˆæ–°é—»åˆ†è§£å›¾å’Œæ¼”å˜å›¾ (Plotly) åˆ°ç›®å½•: {output_dir}")
    print(f"  [Plotting] ä½¿ç”¨åŸºç¡€æ–‡ä»¶å: {base_output_filename}")

    output_paths = {
        "evolution_plot_path": None,
        "decomposition_plot_path": None
    }

    evolution_file_path = os.path.join(output_dir, f"{base_output_filename}_evo.html")
    decomposition_file_path = os.path.join(output_dir, f"{base_output_filename}_decomp.html")

    evolution_file = os.path.join(input_dir, 'nowcast_evolution_data_T.csv')
    news_file = os.path.join(input_dir, 'news_decomposition_grouped.csv')

    if not os.path.exists(evolution_file):
        raise FileNotFoundError(f"ç»˜å›¾æ‰€éœ€çš„ Nowcast æ¼”å˜æ–‡ä»¶æœªæ‰¾åˆ°: {evolution_file}")
    if not os.path.exists(news_file):
        # å¦‚æœæ–°é—»æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥ç»˜åˆ¶æ¼”å˜å›¾
        print(f"  [Plotting] æ–°é—»åˆ†è§£æ–‡ä»¶æœªæ‰¾åˆ°: {news_file}. åªç»˜åˆ¶ Nowcast æ¼”å˜å›¾ã€‚")
        news_df = pd.DataFrame() # åˆ›å»ºç©ºçš„ DataFrame
    else:
        try:
            # ğŸ”¥ ä¿®å¤ç¼–ç é—®é¢˜ï¼šå°è¯•ä¸åŒçš„ç¼–ç æ–¹å¼è¯»å–CSV
            try:
                news_df = pd.read_csv(news_file, index_col=0, parse_dates=True, encoding='utf-8')
            except UnicodeDecodeError:
                print(f"  [Plotting] UTF-8ç¼–ç å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨gbkç¼–ç è¯»å–æ–°é—»åˆ†è§£CSV...")
                news_df = pd.read_csv(news_file, index_col=0, parse_dates=True, encoding='gbk')
            except Exception:
                print(f"  [Plotting] gbkç¼–ç ä¹Ÿå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨latin-1ç¼–ç è¯»å–æ–°é—»åˆ†è§£CSV...")
                news_df = pd.read_csv(news_file, index_col=0, parse_dates=True, encoding='latin-1')
        except Exception as e_load_news:
            print(f"  [Plotting] åŠ è½½æ–°é—»åˆ†è§£ CSV æ•°æ®æ—¶å‡ºé”™: {e_load_news}. åªç»˜åˆ¶ Nowcast æ¼”å˜å›¾ã€‚")
            news_df = pd.DataFrame()


    try:
        nowcast_df = pd.read_csv(evolution_file, index_col=0, parse_dates=True)
    except Exception as e_load_evo:
        # If evolution_file is missing or unreadable, treat as empty for plotting
        print(f"  [Plotting] è­¦å‘Š: åŠ è½½ Nowcast æ¼”å˜ CSV æ•°æ®æ—¶å‡ºé”™: {e_load_evo}. å°†å°è¯•ç”Ÿæˆç©ºæ¼”å˜å›¾ã€‚")
        nowcast_df = pd.DataFrame()

    if not nowcast_df.empty and 'nowcast_descaled' in nowcast_df.columns:
         nowcast_col_name = 'nowcast_descaled'
    elif not nowcast_df.empty and 'nowcast_orig' in nowcast_df.columns:
         nowcast_col_name = 'nowcast_orig'
    else:
         # å¦‚æœ nowcast_df ä¸ºç©ºï¼Œæˆ–å¿…è¦çš„åˆ—ä¸å­˜åœ¨ï¼Œè®¾ç½®ä¸€ä¸ªé»˜è®¤å€¼ä»¥é¿å…åç»­é”™è¯¯
         nowcast_col_name = 'nowcast_descaled' # Fallback, will be empty if df is empty
         if not nowcast_df.empty: # ä»…åœ¨dféç©ºä½†åˆ—ç¼ºå¤±æ—¶å‘Šè­¦
            print(f"  [Plotting] è­¦å‘Š: åœ¨ Nowcast æ¼”å˜æ•°æ®ä¸­æ‰¾ä¸åˆ° 'nowcast_descaled' æˆ– 'nowcast_orig' åˆ—ã€‚æ¼”å˜å›¾å¯èƒ½ä¸æ­£ç¡®ã€‚")

    plot_news = not news_df.empty
    
    # ç›®æ ‡æœˆä»½å­—ç¬¦ä¸²ï¼Œç”¨äºæ ‡é¢˜
    min_date_for_title = None
    if not nowcast_df.empty and isinstance(nowcast_df.index, pd.DatetimeIndex) and not nowcast_df.index.empty:
        min_date_for_title = nowcast_df.index.min()
    elif plot_news and not news_df.empty and isinstance(news_df.index, pd.DatetimeIndex) and not news_df.index.empty:
        min_date_for_title = news_df.index.min()
    
    if min_date_for_title is not None:
        inferred_month_str = min_date_for_title.strftime('%Y-%m')
    else:
        inferred_month_str = "ç›®æ ‡æœˆä»½" # åå¤‡æ ‡é¢˜


    # æ—¥æœŸè¿‡æ»¤ - ä¿®å¤ï¼šè€ƒè™‘ä¸¤ä¸ªæ•°æ®é›†çš„æ—¶é—´èŒƒå›´ä»¥ç¡®ä¿æ—¶é—´è½´ä¸€è‡´
    # min_allowable_date = pd.Timestamp('1900-01-01')
    # max_allowable_date = pd.Timestamp('2200-01-01')

    default_start_dt = None
    default_end_dt = None

    # æ”¶é›†æ‰€æœ‰å¯ç”¨æ•°æ®çš„æ—¶é—´èŒƒå›´
    all_start_times = []
    all_end_times = []

    # ä» nowcast_df è·å–æ—¶é—´èŒƒå›´
    if not nowcast_df.empty and isinstance(nowcast_df.index, pd.DatetimeIndex) and not nowcast_df.index.empty:
        nowcast_min = nowcast_df.index.min()
        nowcast_max = nowcast_df.index.max()
        if pd.notna(nowcast_min):
            all_start_times.append(nowcast_min)
        if pd.notna(nowcast_max):
            all_end_times.append(nowcast_max)

    # ä» news_df è·å–æ—¶é—´èŒƒå›´
    if plot_news and not news_df.empty and isinstance(news_df.index, pd.DatetimeIndex) and not news_df.index.empty:
        news_min = news_df.index.min()
        news_max = news_df.index.max()
        if pd.notna(news_min):
            all_start_times.append(news_min)
        if pd.notna(news_max):
            all_end_times.append(news_max)

    # ä½¿ç”¨ä¸¤ä¸ªæ•°æ®é›†çš„äº¤é›†æ—¶é—´èŒƒå›´ä»¥ç¡®ä¿æ—¶é—´è½´ä¸€è‡´
    if all_start_times and all_end_times:
        default_start_dt = max(all_start_times)  # ä½¿ç”¨è¾ƒæ™šçš„å¼€å§‹æ—¶é—´
        default_end_dt = min(all_end_times)      # ä½¿ç”¨è¾ƒæ—©çš„ç»“æŸæ—¶é—´
        print(f"  [Plotting] åŸºäºä¸¤ä¸ªæ•°æ®é›†çš„äº¤é›†ç¡®å®šæ—¶é—´èŒƒå›´: {default_start_dt.strftime('%Y-%m-%d')} åˆ° {default_end_dt.strftime('%Y-%m-%d')}")

    # å¦‚æœç»è¿‡ä¸Šè¿°æ­¥éª¤åï¼Œé»˜è®¤æ—¥æœŸä»ç„¶æ˜¯None (ä¾‹å¦‚ä¸¤ä¸ªæ•°æ®é›†éƒ½ä¸ºç©º)
    # è®¾ç½®ä¸€ä¸ªéå¸¸åŸºç¡€çš„å›é€€ï¼Œä¾‹å¦‚åŸºäºå½“å‰æ—¶é—´
    if default_start_dt is None:
        default_start_dt = pd.Timestamp.now().normalize() - pd.DateOffset(months=3) # ä¾‹å¦‚è¿‡å»3ä¸ªæœˆ
        print(f"  [Plotting] æ— æ³•ä»æ•°æ®é›†è·å–é»˜è®¤å¼€å§‹æ—¥æœŸï¼Œå›é€€åˆ°: {default_start_dt.strftime('%Y-%m-%d')}")
    if default_end_dt is None:
        default_end_dt = pd.Timestamp.now().normalize()
        print(f"  [Plotting] æ— æ³•ä»æ•°æ®é›†è·å–é»˜è®¤ç»“æŸæ—¥æœŸï¼Œå›é€€åˆ°: {default_end_dt.strftime('%Y-%m-%d')}")

    # ç¡®ä¿å¼€å§‹æ—¶é—´ä¸æ™šäºç»“æŸæ—¶é—´
    if default_start_dt > default_end_dt:
        print(f"  [Plotting] è­¦å‘Š: äº¤é›†æ—¶é—´èŒƒå›´æ— æ•ˆ (å¼€å§‹æ—¶é—´æ™šäºç»“æŸæ—¶é—´)ï¼Œä½¿ç”¨å¹¶é›†èŒƒå›´")
        if all_start_times and all_end_times:
            default_start_dt = min(all_start_times)  # ä½¿ç”¨æœ€æ—©çš„å¼€å§‹æ—¶é—´
            default_end_dt = max(all_end_times)      # ä½¿ç”¨æœ€æ™šçš„ç»“æŸæ—¶é—´
            print(f"  [Plotting] æ”¹ç”¨å¹¶é›†æ—¶é—´èŒƒå›´: {default_start_dt.strftime('%Y-%m-%d')} åˆ° {default_end_dt.strftime('%Y-%m-%d')}")

    start_dt = pd.to_datetime(plot_start_date) if plot_start_date else default_start_dt
    end_dt = pd.to_datetime(plot_end_date) if plot_end_date else default_end_dt

    # å†æ¬¡ç¡®ä¿ start_dt å’Œ end_dt æ˜¯æœ‰æ•ˆçš„ Timestamp å¯¹è±¡ï¼Œå¹¶ä¸” start_dt <= end_dt
    valid_dates = True
    if not isinstance(start_dt, pd.Timestamp) or pd.isna(start_dt):
        print(f"  [Plotting] è­¦å‘Š: è§£æåçš„ start_dt ('{plot_start_date}') æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼: {default_start_dt.strftime('%Y-%m-%d')}")
        start_dt = default_start_dt
        valid_dates = False
    if not isinstance(end_dt, pd.Timestamp) or pd.isna(end_dt):
        print(f"  [Plotting] è­¦å‘Š: è§£æåçš„ end_dt ('{plot_end_date}') æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼: {default_end_dt.strftime('%Y-%m-%d')}")
        end_dt = default_end_dt
        valid_dates = False

    if start_dt > end_dt:
        print(f"  [Plotting] è­¦å‘Š: start_dt ({start_dt.strftime('%Y-%m-%d')}) æ™šäº end_dt ({end_dt.strftime('%Y-%m-%d')})ã€‚å°†äº¤æ¢å®ƒä»¬æˆ–ä½¿ç”¨é»˜è®¤èŒƒå›´ã€‚")
        # ç®€å•å¤„ç†ï¼šå¦‚æœæ— æ•ˆï¼Œéƒ½ç”¨é»˜è®¤å€¼ï¼Œæˆ–è€…åªäº¤æ¢
        start_dt, end_dt = min(default_start_dt, default_end_dt), max(default_start_dt, default_end_dt) 
        print(f"  [Plotting] æ—¥æœŸèŒƒå›´å·²é‡ç½®ä¸º: {start_dt.strftime('%Y-%m-%d')} åˆ° {end_dt.strftime('%Y-%m-%d')}")
        valid_dates = False

    print(f"  [Plotting] ä½¿ç”¨çš„ç»˜å›¾æ—¥æœŸèŒƒå›´: {start_dt.strftime('%Y-%m-%d')} åˆ° {end_dt.strftime('%Y-%m-%d')}")
    print(f"  [Plotting] start_dt ç±»å‹: {type(start_dt)}, end_dt ç±»å‹: {type(end_dt)}")
    
    nowcast_plot_data = nowcast_df[(nowcast_df.index >= start_dt) & (nowcast_df.index <= end_dt)]
    if plot_news:
        news_plot_data = news_df[(news_df.index >= start_dt) & (news_df.index <= end_dt)]
        if news_plot_data.empty: plot_news = False # å¦‚æœè¿‡æ»¤åä¸ºç©ºï¼Œåˆ™ä¸ç»˜åˆ¶æ–°é—»
    else:
        news_plot_data = pd.DataFrame()

    if nowcast_plot_data.empty:
         # raise ValueError(f"åœ¨æŒ‡å®šçš„æ—¥æœŸèŒƒå›´ ({start_dt.strftime('%Y-%m-%d')} åˆ° {end_dt.strftime('%Y-%m-%d')}) å†…æ²¡æœ‰æ‰¾åˆ° Nowcast æ¼”å˜æ•°æ®ã€‚")
         # ç¡®ä¿è¿™é‡Œçš„ start_dt å’Œ end_dt æ˜¯æœ‰æ•ˆçš„ Timestamp å¯¹è±¡æ‰èƒ½è°ƒç”¨ strftime
         # è¿™ä¸ªæ‰“å°è¯­å¥åœ¨ä¹‹å‰çš„æ—¥å¿—ä¸­å¯¼è‡´äº† AttributeErrorï¼Œå› ä¸ºé‚£æ—¶çš„ start_dt/end_dt å¯èƒ½æ˜¯ float
         # ç°åœ¨çš„é€»è¾‘åº”è¯¥èƒ½ç¡®ä¿å®ƒä»¬æ˜¯ Timestamp
         print(f"  [Plotting] è­¦å‘Š: åœ¨æŒ‡å®šçš„æ—¥æœŸèŒƒå›´ ({start_dt.strftime('%Y-%m-%d')} åˆ° {end_dt.strftime('%Y-%m-%d')}) å†…æ²¡æœ‰æ‰¾åˆ° Nowcast æ¼”å˜æ•°æ®ã€‚æ¼”å˜å›¾å¯èƒ½ä¸ºç©ºã€‚")
         # å³ä½¿æ•°æ®ä¸ºç©ºï¼Œä¹Ÿå°è¯•ç”Ÿæˆä¸€ä¸ªç©ºçš„æ¼”å˜å›¾ï¼Œè€Œä¸æ˜¯æŠ›å‡ºé”™è¯¯å¯¼è‡´åç»­åˆ†è§£å›¾ä¹Ÿä¸ç”Ÿæˆ

    # --- å›¾è¡¨ä¸€ï¼šNowcast æ¼”å˜å›¾ --- 
    fig_evo = go.Figure()

    if not nowcast_plot_data.empty and nowcast_col_name in nowcast_plot_data.columns:
        fig_evo.add_trace(go.Scatter(
            x=nowcast_plot_data.index,
            y=nowcast_plot_data[nowcast_col_name],
            mode='lines+markers+text',
            name=f'Nowcast ({target_variable_name})',
            line=dict(color='black', width=2),
            marker=dict(size=7),
            text=[f'{val:.2f}' for val in nowcast_plot_data[nowcast_col_name]],
            textposition="top center",
            textfont=dict(size=10, color='black')
        ))
        title_text_evo = f"{inferred_month_str} {target_variable_name} Nowcast æ¼”å˜"
    else:
        fig_evo.add_annotation(text="æ²¡æœ‰å¯ç”¨çš„ Nowcast æ¼”å˜æ•°æ®æ˜¾ç¤ºã€‚", showarrow=False, font=dict(size=16))
        title_text_evo = f"{inferred_month_str} {target_variable_name} Nowcast æ¼”å˜ (æ— æ•°æ®)"

    fig_evo.update_layout(
        title=dict(text=title_text_evo, font=dict(size=16, color='#333333'), x=0.5, xanchor='center'),
        xaxis_title="ä¿¡æ¯æˆªæ­¢æ—¥æœŸ (Vintage t)",
        yaxis_title=f"{target_variable_name} (åæ ‡å‡†åŒ–å€¼)",
        legend_title_text="å›¾ä¾‹",
        legend=dict(orientation="h", yanchor="top", y=-0.2, xanchor="center", x=0.5),
        margin=dict(b=120, t=80, l=50, r=50),
        xaxis_showgrid=True, yaxis_showgrid=True,
        # gridwidth=1, gridcolor='LightGrey' # åœ¨è½´ä¸Šè®¾ç½®
    )
    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç¡®ä¿æ¼”å˜å›¾å’Œåˆ†è§£å›¾ä½¿ç”¨å®Œå…¨ç›¸åŒçš„æ—¶é—´è½´è®¾ç½®
    # ç¡®ä¿ start_dt å’Œ end_dt æ˜¯ Pandas Timestamp ç±»å‹
    range_start = pd.to_datetime(start_dt) if not isinstance(start_dt, pd.Timestamp) else start_dt
    range_end = pd.to_datetime(end_dt) if not isinstance(end_dt, pd.Timestamp) else end_dt

    print(f"  [Plotting] æ¼”å˜å›¾ range è®¾ç½®: [{range_start}] åˆ° [{range_end}] (ç±»å‹: {type(range_start)}, {type(range_end)})")

    # ğŸ”¥ æ–°å¢ï¼šå®šä¹‰ç»Ÿä¸€çš„ x è½´é…ç½®ï¼Œç¡®ä¿ä¸¤ä¸ªå›¾è¡¨å®Œå…¨ä¸€è‡´
    unified_xaxis_config = dict(
        range=[range_start, range_end],  # å¼ºåˆ¶ä½¿ç”¨ç›¸åŒçš„æ—¶é—´èŒƒå›´
        tickformat="%m-%d",             # ç»Ÿä¸€çš„æ—¥æœŸæ ¼å¼
        tickangle=0,                    # ç»Ÿä¸€çš„åˆ»åº¦è§’åº¦
        showgrid=True,                  # æ˜¾ç¤ºç½‘æ ¼
        gridwidth=1,                    # ç½‘æ ¼å®½åº¦
        gridcolor='LightGrey',          # ç½‘æ ¼é¢œè‰²
        dtick=7*24*60*60*1000,         # ğŸ”¥ å…³é”®ï¼šå›ºå®š7å¤©é—´éš”ï¼ˆæ¯«ç§’ï¼‰
        tick0=range_start,              # ğŸ”¥ å…³é”®ï¼šä»å¼€å§‹æ—¥æœŸå¼€å§‹åˆ»åº¦
        tickmode='linear',              # ğŸ”¥ å…³é”®ï¼šçº¿æ€§åˆ»åº¦æ¨¡å¼
        nticks=10                       # æœ€å¤§åˆ»åº¦æ•°
    )

    fig_evo.update_xaxes(**unified_xaxis_config)
    fig_evo.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGrey')

    try:
        html_content_evo = fig_evo.to_html(full_html=True, include_plotlyjs='cdn')
        with open(evolution_file_path, 'w', encoding='utf-8') as f:
            f.write(html_content_evo)
        print(f"  [Plotting] Plotly æ¼”å˜å›¾å·²ä¿å­˜åˆ°: {evolution_file_path}")
        output_paths["evolution_plot_path"] = evolution_file_path
    except Exception as e_save_evo:
        print(f"  [Plotting] ä¿å­˜ Plotly æ¼”å˜å›¾ä¸º HTML æ—¶å‡ºé”™: {e_save_evo}")
        # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸ç»§ç»­ç”Ÿæˆåˆ†è§£å›¾

    # --- å›¾è¡¨äºŒï¼šæ–°é—»è´¡çŒ®åˆ†è§£å›¾ --- 
    print(f"  [Plotting DEBUG] --- Before Decomposition Plot ---") # Removed non-ASCII chars
    print(f"  [Plotting DEBUG] news_df (original loaded news data):")
    print(f"    Is empty: {news_df.empty}")
    if not news_df.empty:
        print(f"    Shape: {news_df.shape}")
        print(f"    Head:\n{news_df.head().to_string()}")
        print(f"    NaN counts per column:\n{news_df.isna().sum().to_string()}")
        print(f"    Data types:\n{news_df.dtypes.to_string()}")
    print(f"  [Plotting DEBUG] plot_news (boolean flag): {plot_news}")
    print(f"  [Plotting DEBUG] news_plot_data (after date filtering - should be same as news_df now):")
    print(f"    Is empty: {news_plot_data.empty}")
    if not news_plot_data.empty:
        print(f"    Shape: {news_plot_data.shape}")
        print(f"    Head:\n{news_plot_data.head().to_string()}")
        print(f"    NaN counts per column:\n{news_plot_data.isna().sum().to_string()}")

    fig_decomp = go.Figure()
    if plot_news and not news_plot_data.empty: # <<< å¢åŠ å¯¹ news_plot_data æ˜¯å¦ä¸ºç©ºçš„æ£€æŸ¥
        group_cols = [col for col in news_plot_data.columns if col != 'total_news' and col != 'residual_news']
        print(f"  [Plotting DEBUG] group_cols for plotting: {group_cols}") # Added print for group_cols
        if group_cols: # ç¡®ä¿æœ‰åˆ—å¯ä¾›ç»˜åˆ¶
            try:
                import plotly.express as px
                # --- æ–°å¢ï¼šå®šä¹‰å›ºå®šé¢œè‰²æ˜ å°„ (å°è¯•åŒ¹é…ç¾è”å‚¨æˆªå›¾é…è‰²) ---
                # è¯·ç¡®ä¿è¿™é‡Œçš„é”®åä¸ news_decomposition_grouped.csv ä¸­çš„åˆ—åå®Œå…¨ä¸€è‡´
                color_map = {
                    # åŸºäºç¾è”å‚¨Nowcastingæˆªå›¾çš„é¢œè‰²æ¨æ–­ (å…·ä½“ç±»åˆ«å¯¹åº”éœ€è¦æ ¹æ®å®é™…æ•°æ®è°ƒæ•´)
                    "åŒ–å­¦åŒ–å·¥": "#B22222",  # Firebrick (ç±»ä¼¼æˆªå›¾ä¸­çš„æ·±çº¢è‰²ç³»)
                    "é’¢é“": "#4682B4",    # SteelBlue (ç±»ä¼¼æˆªå›¾ä¸­çš„è“è‰²ç³»)
                    "ç…¤ç‚­": "#556B2F",    # DarkOliveGreen (ç±»ä¼¼æˆªå›¾ä¸­çš„ç»¿è‰²ç³»)
                    "æœ‰è‰²é‡‘å±": "#DAA520",  # Goldenrod (ç±»ä¼¼æˆªå›¾ä¸­çš„é»„è‰²/æ©™è‰²ç³»)
                    "å»ºæ": "#708090",    # SlateGray (ä¸­æ€§ç°è‰²ç³»)
                    
                    "PMI": "#ADD8E6",      # LightBlue (æµ…è“è‰²)
                    "å·¥ä¸šå¢åŠ å€¼": "#8FBC8F",  # DarkSeaGreen (ä¸€ç§è¾ƒæŸ”å’Œçš„ç»¿è‰²)
                    "æ²¹æ°”": "#FFD700",      # Gold (ç”¨ä¸€ä¸ªè¾ƒäº®çš„é»„è‰²ä»£è¡¨æ²¹æ°”)
                    "ç”µåŠ›": "#A9A9A9",      # DarkGray (ç°è‰²ç³»)
                    "è¿è¾“": "#BA55D3",      # MediumOrchid (ç´«è‰²ç³»)
                    
                    "æ±½è½¦": "#87CEEB",      # SkyBlue (å¦ä¸€ç§æµ…è“è‰²)
                    "æ©¡èƒ¶å¡‘æ–™": "#3CB371",  # MediumSeaGreen (ä¸­ç»¿è‰²)
                    "åŒ–çº¤": "#6A5ACD",      # SlateBlue (è“ç´«è‰²)
                    
                    # å›é€€/é»˜è®¤é¢œè‰²
                    "å…¶ä»–æœªåˆ†ç±»": "#D3D3D3",  # LightGray
                    "residual_news": "#F0F0F0" # éå¸¸æµ…çš„ç°è‰²ï¼Œæ¥è¿‘ç™½è‰²
                }
                # ä¸ºæ•°æ®ä¸­å­˜åœ¨ä½† color_map ä¸­æ²¡æœ‰çš„åˆ—æä¾›ä¸€ä¸ªé»˜è®¤é¢œè‰²åˆ—è¡¨å¾ªç¯ä½¿ç”¨
                default_colors_for_unmapped = px.colors.qualitative.Pastel 
                # --- ç»“æŸæ–°å¢é¢œè‰²æ˜ å°„ ---

                current_color_index = 0 # ç”¨äºä» default_colors_for_unmapped ä¸­å–è‰²

                for col_name in group_cols:
                    fig_decomp.add_trace(go.Bar(
                        x=news_plot_data.index,
                        y=news_plot_data[col_name].fillna(0),
                        name=col_name,
                        marker_color=color_map.get(col_name, default_colors_for_unmapped[current_color_index % len(default_colors_for_unmapped)])
                    ))
                    current_color_index += 1
            except ImportError:
                basic_colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink', 'gray', 'olive', 'cyan']
                colors = [basic_colors[i % len(basic_colors)] for i in range(len(group_cols))]

            fig_decomp.update_layout(barmode='relative')
            title_text_decomp = f"{inferred_month_str} {target_variable_name} æ–°é—»è´¡çŒ®åˆ†è§£"
        else:
            fig_decomp.add_annotation(text="æ²¡æœ‰å¯ä¾›åˆ†è§£çš„æ–°é—»ç»„æ•°æ®ã€‚", showarrow=False, font=dict(size=16))
            title_text_decomp = f"{inferred_month_str} {target_variable_name} æ–°é—»è´¡çŒ®åˆ†è§£ (æ— æ•°æ®åˆ—)"
    else:
        print(f"  [Plotting DEBUG] Condition (plot_news and not news_plot_data.empty) is FALSE. Adding 'no data' annotation.") # Added print for this path
        # å¦‚æœæ²¡æœ‰æ–°é—»æ•°æ® (plot_news is False æˆ– news_plot_data ä¸ºç©º)
        fig_decomp.add_annotation(text="æ²¡æœ‰å¯ç”¨çš„æ–°é—»åˆ†è§£æ•°æ®ã€‚", showarrow=False, font=dict(size=16))
        title_text_decomp = f"{inferred_month_str} {target_variable_name} æ–°é—»è´¡çŒ®åˆ†è§£ (æ— æ•°æ®)"

    fig_decomp.update_layout(
        title=dict(text=title_text_decomp, font=dict(size=16, color='#333333'), x=0.5, xanchor='center'),
        xaxis_title="ä¿¡æ¯æˆªæ­¢æ—¥æœŸ (Vintage t)",
        yaxis_title="å¯¹ Nowcast çš„è´¡çŒ® (åæ ‡å‡†åŒ–å€¼)", # Yè½´å«ä¹‰å¯èƒ½éœ€è¦æ ¹æ®å®é™…è®¡ç®—è°ƒæ•´
        legend_title_text="å›¾ä¾‹",
        legend=dict(orientation="h", yanchor="top", y=-0.2, xanchor="center", x=0.5),
        margin=dict(b=120, t=80, l=50, r=50),
        xaxis_showgrid=True, yaxis_showgrid=True,
    )
    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šåˆ†è§£å›¾ä½¿ç”¨ä¸æ¼”å˜å›¾å®Œå…¨ç›¸åŒçš„æ—¶é—´è½´è®¾ç½®
    print(f"  [Plotting] åˆ†è§£å›¾åº”ç”¨ç»Ÿä¸€çš„ x è½´é…ç½®")

    # ğŸ”¥ é‡è¦ï¼šä½¿ç”¨ä¸æ¼”å˜å›¾å®Œå…¨ç›¸åŒçš„ unified_xaxis_config
    fig_decomp.update_xaxes(**unified_xaxis_config)

    # ğŸ”¥ éªŒè¯ï¼šæ£€æŸ¥ä¸¤ä¸ªå›¾è¡¨çš„ x è½´è®¾ç½®æ˜¯å¦å®Œå…¨ä¸€è‡´
    print(f"  [Plotting] === æ—¶é—´è½´è®¾ç½®éªŒè¯ ===")
    print(f"  [Plotting] æ¼”å˜å›¾ x è½´èŒƒå›´: {fig_evo.layout.xaxis.range}")
    print(f"  [Plotting] åˆ†è§£å›¾ x è½´èŒƒå›´: {fig_decomp.layout.xaxis.range}")
    print(f"  [Plotting] æ¼”å˜å›¾ dtick: {fig_evo.layout.xaxis.dtick}")
    print(f"  [Plotting] åˆ†è§£å›¾ dtick: {fig_decomp.layout.xaxis.dtick}")
    print(f"  [Plotting] æ¼”å˜å›¾ tick0: {fig_evo.layout.xaxis.tick0}")
    print(f"  [Plotting] åˆ†è§£å›¾ tick0: {fig_decomp.layout.xaxis.tick0}")

    # æ£€æŸ¥æ˜¯å¦å®Œå…¨ä¸€è‡´
    if (fig_evo.layout.xaxis.range == fig_decomp.layout.xaxis.range and
        fig_evo.layout.xaxis.dtick == fig_decomp.layout.xaxis.dtick and
        fig_evo.layout.xaxis.tick0 == fig_decomp.layout.xaxis.tick0):
        print(f"  [Plotting] [OK] æ—¶é—´è½´è®¾ç½®å®Œå…¨ä¸€è‡´")
    else:
        print(f"  [Plotting] [ERROR] æ—¶é—´è½´è®¾ç½®ä¸ä¸€è‡´ï¼Œéœ€è¦æ£€æŸ¥")
    fig_decomp.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGrey')

    try:
        print(f"  [Plotting DEBUG] å°è¯•ä¿å­˜åˆ†è§£å›¾åˆ°: {decomposition_file_path}")
        html_content_decomp = fig_decomp.to_html(full_html=True, include_plotlyjs='cdn')
        print(f"  [Plotting DEBUG] HTMLå†…å®¹ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(html_content_decomp)}")
        with open(decomposition_file_path, 'w', encoding='utf-8') as f:
            f.write(html_content_decomp)
        print(f"  [Plotting] Plotly åˆ†è§£å›¾å·²ä¿å­˜åˆ°: {decomposition_file_path}")
        output_paths["decomposition_plot_path"] = decomposition_file_path
        print(f"  [Plotting DEBUG] åˆ†è§£å›¾è·¯å¾„å·²è®¾ç½®: {output_paths['decomposition_plot_path']}")
    except Exception as e_save_decomp:
        print(f"  [Plotting] ä¿å­˜ Plotly åˆ†è§£å›¾ä¸º HTML æ—¶å‡ºé”™: {e_save_decomp}")
        import traceback
        traceback.print_exc()

    return output_paths # è¿”å›ä¸¤ä¸ªå›¾è¡¨çš„è·¯å¾„

# <<< END PLOTTING FUNCTION (PLOTLY) >>> 

if __name__ == "__main__":
    print("[DEBUG] Entering main execution block.")
    parser = argparse.ArgumentParser(description='è¿è¡Œ Nowcasting æ¼”å˜ä¸æ–°é—»è´¡çŒ®åˆ†æï¼Œå¹¶è‡ªåŠ¨ç”Ÿæˆå›¾è¡¨ã€‚')
    # --- æ–°å¢ï¼šç”¨äºæŒ‡å®šDFMæ¨¡å‹å’Œå…ƒæ•°æ®è¾“å…¥ç›®å½•çš„å‚æ•° ---
    parser.add_argument(
        '--model_files_dir', # å‚æ•°åä¿®æ”¹ï¼Œä»¥é¿å…ä¸ä¹‹å‰çš„æ··æ·†ï¼Œå¹¶å¼ºè°ƒå…¶ç”¨é€”
        type=str,
        default=None, # åç«¯å°†ä¼ é€’æ­¤å‚æ•°ï¼Œå‘½ä»¤è¡Œç›´æ¥è¿è¡Œæ—¶å¯é€‰ï¼ˆè‹¥ä¸æä¾›åˆ™ä½¿ç”¨ DEFAULT_TUNING_OUTPUT_DIRï¼‰
        help='æŒ‡å®š DFM æ¨¡å‹ (.joblib) å’Œå…ƒæ•°æ® (.pkl) æ–‡ä»¶æ‰€åœ¨çš„è¾“å…¥ç›®å½•ã€‚'
    )
    parser.add_argument(
        '--evolution_output_dir',
        type=str,
        default=config.NOWCAST_EVOLUTION_OUTPUT_DIR,
        help=f'Nowcasting è®¡ç®—ç»“æœçš„è¾“å‡ºç›®å½• (é»˜è®¤ä¸º config.py ä¸­è®¾ç½®çš„å€¼: {config.NOWCAST_EVOLUTION_OUTPUT_DIR})'
    )
    parser.add_argument(
        '--target_month',
        type=str,
        default=None,
        help='æŒ‡å®šç›®æ ‡æœˆä»½ (æ ¼å¼ YYYY-MM)ã€‚å¦‚æœæœªæŒ‡å®šï¼Œåˆ™ä½¿ç”¨ config.py ä¸­çš„ NEWS_TARGET_MONTH æˆ–è‡ªåŠ¨ç¡®å®šã€‚'
    )
    parser.add_argument(
        '--plot_output_file',
        type=str,
        default=None, # å°†ç”± news_analysis_backend.py ä¼ é€’
        help=f'è¾“å‡ºå›¾è¡¨çš„ HTML æ–‡ä»¶è·¯å¾„ (é»˜è®¤ä¸ºè®¡ç®—ç»“æœç›®å½•ä¸‹çš„ news_analysis_plot.html)ã€‚' # <<< ä¿®æ”¹å¸®åŠ©æ–‡æœ¬
    )
    parser.add_argument(
        '--plot_start_date',
        type=str,
        default=None,
        help='ç»˜å›¾çš„å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)ã€‚é»˜è®¤ä¸ºåˆ†æå‘¨æœŸçš„å¼€å§‹æ—¥æœŸã€‚'
    )
    parser.add_argument(
        '--plot_end_date',
        type=str,
        default=None,
        help='ç»˜å›¾çš„ç»“æŸæ—¥æœŸ (YYYY-MM-DD)ã€‚é»˜è®¤ä¸ºåˆ†æå‘¨æœŸçš„ç»“æŸæ—¥æœŸã€‚'
    )
    args = parser.parse_args()
    EVOLUTION_OUTPUT_DIR = args.evolution_output_dir
    SPECIFIED_TARGET_MONTH_STR = args.target_month

    kf_results_rerun = None # <<< ç¡®ä¿è¿™æ˜¯ kf_results_rerun åœ¨ __main__ å—ä¸­çš„å”¯ä¸€åˆå§‹åŒ–ä½ç½®

    # --- ç¡®å®š TUNING_OUTPUT_DIR (æ¨¡å‹æ–‡ä»¶åŠ è½½è·¯å¾„) ---
    if args.model_files_dir:
        TUNING_OUTPUT_DIR = args.model_files_dir
        print(f"[DEBUG] Using --model_files_dir for model/metadata input: {TUNING_OUTPUT_DIR}")
    else:
        TUNING_OUTPUT_DIR = DEFAULT_TUNING_OUTPUT_DIR # ä½¿ç”¨è„šæœ¬é¡¶éƒ¨çš„é»˜è®¤å€¼
        print(f"[DEBUG] Using default directory for model/metadata input (from config): {TUNING_OUTPUT_DIR}")

    print(f"[DEBUG] Command line arguments parsed.")
    print(f"[DEBUG]   SPECIFIED_TARGET_MONTH_STR (from --target_month): {SPECIFIED_TARGET_MONTH_STR}")
    print(f"[DEBUG]   Effective TUNING_OUTPUT_DIR: {TUNING_OUTPUT_DIR}")
    print(f"[DEBUG]   Using effective EVOLUTION_OUTPUT_DIR: {EVOLUTION_OUTPUT_DIR}")
    
    print("--- å¼€å§‹è¿è¡Œ Nowcast æ¼”å˜ä¸æ–°é—»è´¡çŒ®åˆ†æ ---")
    # ... (è„šæœ¬å…¶ä½™ä¸»è¦é€»è¾‘ä¿æŒä¸å˜) ...
    print(f"ç»“æœå°†ä¿å­˜åˆ°: {EVOLUTION_OUTPUT_DIR}")
    os.makedirs(EVOLUTION_OUTPUT_DIR, exist_ok=True)
    print(f"--- æ­¥éª¤ 1: ä» '{TUNING_OUTPUT_DIR}' åŠ è½½æ¨¡å‹ç»“æœå’Œå…ƒæ•°æ® ---")
    model_load_path = os.path.join(TUNING_OUTPUT_DIR, MODEL_FILENAME)
    metadata_load_path = os.path.join(TUNING_OUTPUT_DIR, METADATA_FILENAME)
    try:
        print(f"  [Main Logic] æ­£åœ¨åŠ è½½æ¨¡å‹: {model_load_path}")
        final_dfm_results = joblib.load(model_load_path)
        print(f"  [Main Logic] æ­£åœ¨åŠ è½½å…ƒæ•°æ®: {metadata_load_path}")
        with open(metadata_load_path, 'rb') as f:
            metadata = pickle.load(f)
        print("  [Main Logic] æ¨¡å‹å’Œå…ƒæ•°æ®åŠ è½½æˆåŠŸã€‚")
        # ... (æå– A, Lambda, Q, R, x0, P0 ç­‰)
        # ç¡®ä¿æå– TARGET_VARIABLE_FROM_METADATA (å¦‚æœå…ƒæ•°æ®ä¸­æœ‰)
        TARGET_VARIABLE_FROM_METADATA = metadata.get('target_variable', TARGET_VARIABLE)
        print(f"  [Main Logic] ä½¿ç”¨çš„ç›®æ ‡å˜é‡ (æ¥è‡ªå…ƒæ•°æ®æˆ–é…ç½®): {TARGET_VARIABLE_FROM_METADATA}")

        # --- æå– z_for_rerun (è§‚æµ‹æ•°æ®) --- 
        z_for_rerun = metadata.get('processed_data_for_model') # ä¼˜å…ˆä½¿ç”¨å…ƒæ•°æ®ä¸­ä¸“é—¨å­˜å‚¨çš„ç”¨äºå›æº¯çš„ data
        if z_for_rerun is None and hasattr(final_dfm_results, 'z') and final_dfm_results.z is not None:
            print("  [Main Logic] è­¦å‘Š: å…ƒæ•°æ®ä¸­æœªæ‰¾åˆ° 'processed_data_for_model'ï¼Œå›é€€ä½¿ç”¨ final_dfm_results.z")
            z_for_rerun = final_dfm_results.z
        if z_for_rerun is None:
            print("  [Main Logic] é”™è¯¯: æ— æ³•è·å–ç”¨äºé‡æ–°è¿è¡Œå¡å°”æ›¼æ»¤æ³¢çš„è§‚æµ‹æ•°æ® (z)ã€‚è„šæœ¬å°†é€€å‡ºã€‚")
            sys.exit(1)
        if not isinstance(z_for_rerun.index, pd.DatetimeIndex):
            try:
                z_for_rerun.index = pd.to_datetime(z_for_rerun.index)
            except Exception as e_dt_z:
                print(f"  [Main Logic] é”™è¯¯: æ— æ³•å°† z_for_rerun çš„ç´¢å¼•è½¬æ¢ä¸º DatetimeIndex: {e_dt_z}ã€‚è„šæœ¬å°†é€€å‡ºã€‚")
                sys.exit(1)
        # --- ç»“æŸæå– z_for_rerun ---

        # --- æå– Lambda_np å’Œ obs_names_from_lambda ---
        Lambda_from_model = final_dfm_results.Lambda
        best_variables_from_metadata = metadata.get('best_variables')
        if isinstance(Lambda_from_model, pd.DataFrame):
            obs_names_from_lambda = Lambda_from_model.index.tolist()
            Lambda_np = Lambda_from_model.to_numpy()
        elif isinstance(Lambda_from_model, np.ndarray):
            Lambda_np = Lambda_from_model
            if best_variables_from_metadata and len(best_variables_from_metadata) == Lambda_np.shape[0]:
                obs_names_from_lambda = best_variables_from_metadata
            else:
                print("  [Main Logic] é”™è¯¯: Lambda æ˜¯ NumPy æ•°ç»„ï¼Œä½†æ— æ³•ä»å…ƒæ•°æ®ç¡®å®šå…¶å¯¹åº”çš„è§‚æµ‹å˜é‡é¡ºåºã€‚è„šæœ¬å°†é€€å‡ºã€‚")
                sys.exit(1)
        else:
            print(f"  [Main Logic] é”™è¯¯: final_dfm_results.Lambda çš„ç±»å‹æ— æ³•è¯†åˆ«: {type(Lambda_from_model)}ã€‚è„šæœ¬å°†é€€å‡ºã€‚")
            sys.exit(1)
        # --- ç»“æŸæå– Lambda_np ---

        n_factors = final_dfm_results.A.shape[0]
        state_names = [f"Factor_{i+1}" for i in range(n_factors)]

    except FileNotFoundError as e_fnf:
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹æˆ–å…ƒæ•°æ®æ–‡ä»¶: {e_fnf}")
        sys.exit(1)
    except Exception as e_load:
        print(f"åŠ è½½æ¨¡å‹/å…ƒæ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e_load}")
        sys.exit(1)

    # --- æ­¥éª¤ 2: é‡æ–°è¿è¡Œå¡å°”æ›¼æ»¤æ³¢ä»¥è·å–å®Œæ•´çš„ä¸­é—´çŠ¶æ€å’Œå¢ç›Š ---
    print("--- æ­¥éª¤ 2: é‡æ–°è¿è¡Œå¡å°”æ›¼æ»¤æ³¢ä»¥è·å–å®Œæ•´çš„ä¸­é—´çŠ¶æ€å’Œå¢ç›Š ---")
    try:
        print(f"  [Main Logic] z_for_rerun çš„ç´¢å¼•ç±»å‹: {type(z_for_rerun.index)}")

        # --- ä¸ºå¡å°”æ›¼æ»¤æ³¢å™¨å‡†å¤‡Uå’ŒBå‚æ•° (å‡è®¾DFMä¸­æ²¡æœ‰æ˜¾å¼æ§åˆ¶è¾“å…¥) ---
        n_time_kf = len(z_for_rerun.index)
        n_state_kf = final_dfm_results.A.shape[0]
        
        # åˆ›å»º U_for_kf: (n_time, 1) DataFrame of zeros
        # ä½¿ç”¨ä¸€ä¸ªä¸å¤ªå¯èƒ½ä¸ z_for_rerun.columnså†²çªçš„åˆ—å
        dummy_col_name_u = 'dummy_ctrl_in'
        if dummy_col_name_u in z_for_rerun.columns: # è¿›ä¸€æ­¥é¿å…åˆ—åå†²çª
            dummy_col_name_u = 'alt_dummy_ctrl_in'
        U_for_kf = pd.DataFrame(np.zeros((n_time_kf, 1)), index=z_for_rerun.index, columns=[dummy_col_name_u])

        # åˆ›å»º B_for_kf: (n_state, 1) np.array of zeros
        B_for_kf = np.zeros((n_state_kf, 1))
        # --- U å’Œ B å‚æ•°å‡†å¤‡å®Œæ¯• ---

        print(f"  [Main Logic] è°ƒç”¨ KalmanFilter - Z shape: {z_for_rerun.shape}, U shape: {U_for_kf.shape}, A shape: {final_dfm_results.A.shape}, B shape: {B_for_kf.shape}, H shape: {Lambda_np.shape if 'Lambda_np' in locals() and Lambda_np is not None else 'Lambda_np not defined'}, Q shape: {final_dfm_results.Q.shape}, R shape: {final_dfm_results.R.shape}")

        kf_results_rerun = KalmanFilter(
            Z=z_for_rerun,                    # è§‚æµ‹æ•°æ®
            U=U_for_kf,                       # æ§åˆ¶è¾“å…¥å‘é‡ (æ­¤å¤„ä¸ºå ä½ç¬¦)
            A=final_dfm_results.A,            # çŠ¶æ€è½¬ç§»çŸ©é˜µ
            B=B_for_kf,                       # æ§åˆ¶è¾“å…¥çŸ©é˜µ (æ­¤å¤„ä¸ºå ä½ç¬¦)
            H=Lambda_np,                      # è§‚æµ‹çŸ©é˜µ
            state_names=state_names,          # çŠ¶æ€åç§°åˆ—è¡¨
            x0=final_dfm_results.x0,          # åˆå§‹çŠ¶æ€ä¼°è®¡
            P0=final_dfm_results.P0,          # åˆå§‹è¯¯å·®åæ–¹å·®
            Q=final_dfm_results.Q,            # è¿‡ç¨‹å™ªå£°åæ–¹å·®
            R=final_dfm_results.R             # æµ‹é‡å™ªå£°åæ–¹å·®
        )
        print("  [Main Logic] å¡å°”æ›¼æ»¤æ³¢é‡æ–°è¿è¡Œå®Œæˆã€‚")
        if kf_results_rerun is None:
            print("  [Main Logic] ä¸¥é‡é”™è¯¯: kf_rerun.filter(z_for_rerun) è¿”å›äº† Noneã€‚æ— æ³•ç»§ç»­ã€‚")
            sys.exit(1)
        if kf_results_rerun.x_minus is None or kf_results_rerun.Kalman_gain is None:
            print("  [Main Logic] ä¸¥é‡é”™è¯¯: é‡æ–°è¿è¡Œçš„å¡å°”æ›¼æ»¤æ³¢ç»“æœä¸­ç¼ºå°‘ x_minus æˆ– Kalman_gainã€‚æ— æ³•ç»§ç»­ã€‚")
            sys.exit(1)
        else:
            print(f"  [Main Logic] KF Rerun x_minus (shape {kf_results_rerun.x_minus.shape}) and Kalman_gain (len {len(kf_results_rerun.Kalman_gain)}) çœ‹èµ·æ¥æ˜¯æœ‰æ•ˆçš„ã€‚")

    except Exception as e_kf_rerun:
        print(f"é‡æ–°è¿è¡Œå¡å°”æ›¼æ»¤æ³¢æ—¶å‘ç”Ÿé”™è¯¯: {e_kf_rerun}")
        import traceback
        traceback.print_exc()
        # å³ä½¿ KF é‡æ–°è¿è¡Œå¤±è´¥ï¼Œæˆ‘ä»¬ä¹Ÿå…è®¸è„šæœ¬ç»§ç»­ï¼Œä½†åç»­çš„æ–°é—»åˆ†æå¯èƒ½æ— æ³•è¿›è¡Œ
        # kf_results_rerun å°†ä¿æŒä¸º None (æˆ–å…¶åˆå§‹å€¼)
        print("  [Main Logic] è­¦å‘Š: å¡å°”æ›¼æ»¤æ³¢é‡æ–°è¿è¡Œå¤±è´¥ï¼Œæ–°é—»è´¡çŒ®åˆ†æå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ã€‚")
        kf_results_rerun = None  # ç¡®ä¿è®¾ç½®ä¸º None
        # sys.exit(1) # æ”¹ä¸ºä¸é€€å‡º


    print("--- æ­¥éª¤ 3: ç¡®å®šç›®æ ‡æœˆä»½ã€ç›®æ ‡æ—¥æœŸ T å’Œæœˆå†…åˆ†æå‘¨æœŸ ---")
    all_available_dates = final_dfm_results.x.index # å‡è®¾ final_dfm_results.x åŒ…å«æ‰€æœ‰æ—¥æœŸçš„å› å­çŠ¶æ€
    target_period_m = None
    effective_target_month_str = SPECIFIED_TARGET_MONTH_STR
    target_month_source = "N/A"

    if effective_target_month_str:
        try:
            target_period_m = pd.Period(effective_target_month_str, freq='M')
            target_month_source = "Command Line"
            print(f"  [Main Logic] ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„ç›®æ ‡æœˆä»½: {effective_target_month_str}, è§£æä¸º Period: {target_period_m}")
        except ValueError as e_period:
            print(f"  [Main Logic] è­¦å‘Šï¼šæ— æ³•å°†å‘½ä»¤è¡Œç›®æ ‡æœˆä»½ '{effective_target_month_str}' è§£æä¸º Period: {e_period}ã€‚å°†å°è¯•è‡ªåŠ¨ç¡®å®šã€‚")
            effective_target_month_str = None # æ¸…é™¤æ— æ•ˆå€¼ï¼Œä»¥ä¾¿åç»­å›é€€
    
    if target_period_m is None and hasattr(config, 'NEWS_TARGET_MONTH') and config.NEWS_TARGET_MONTH:
        effective_target_month_str = config.NEWS_TARGET_MONTH
        try:
            target_period_m = pd.Period(effective_target_month_str, freq='M')
            target_month_source = "Config File"
            print(f"  [Main Logic] ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ç›®æ ‡æœˆä»½: {effective_target_month_str}, è§£æä¸º Period: {target_period_m}")
        except ValueError as e_period_config:
            print(f"  [Main Logic] è­¦å‘Šï¼šæ— æ³•å°†é…ç½®æ–‡ä»¶ç›®æ ‡æœˆä»½ '{effective_target_month_str}' è§£æä¸º Period: {e_period_config}ã€‚å°†å°è¯•è‡ªåŠ¨ç¡®å®šã€‚")
            effective_target_month_str = None # æ¸…é™¤æ— æ•ˆå€¼

    if target_period_m is None: # è‡ªåŠ¨ç¡®å®šé€»è¾‘
        target_month_source = "Auto (Fallback)"
        last_date_in_data = all_available_dates.max()
        # ç®€å•çš„è‡ªåŠ¨é€»è¾‘ï¼šå–æ•°æ®ä¸­æœ€æ–°æ—¥æœŸæ‰€åœ¨æœˆä»½çš„å‰ä¸€ä¸ªæœˆä½œä¸ºç›®æ ‡ï¼Œå¦‚æœæœ€æ–°æ—¥æœŸæ˜¯å½“å‰æœˆä»½çš„è¯
        # æˆ–è€…ç›´æ¥å–æœ€æ–°æ•°æ®æ‰€åœ¨çš„æœˆä»½ (éœ€è¦æ¨¡å‹æ”¯æŒé¢„æµ‹å½“æœˆ)
        # current_period = pd.Timestamp('today').to_period('M')
        # last_data_period = last_date_in_data.to_period('M')
        # if last_data_period == current_period:
        #     target_period_m = last_data_period - 1 
        # else:
        #     target_period_m = last_data_period
        # æ›´ç®€å•çš„é€»è¾‘ï¼šç›´æ¥ä½¿ç”¨æ•°æ®ä¸­æœ€æ–°çš„æœˆä»½ï¼Œå‡è®¾æˆ‘ä»¬æ€»æ˜¯åœ¨é¢„æµ‹æœ€æ–°çš„æˆ–æ¥è¿‘æœ€æ–°çš„æœˆä»½
        target_period_m = last_date_in_data.to_period('M')
        print(f"  [Main Logic] è‡ªåŠ¨ç¡®å®šç›®æ ‡æœˆä»½ Period: {target_period_m} (åŸºäºæ•°æ®æœ€æ–°æ—¥æœŸ: {last_date_in_data.strftime('%Y-%m-%d')})")

    # --- åç»­ä½¿ç”¨ target_period_m æ¥ç¡®å®š target_prediction_date å’Œ analysis_period_dates ---
    # ... (è¿™éƒ¨åˆ†é€»è¾‘éœ€è¦éå¸¸å°å¿ƒï¼Œç¡®ä¿å®ƒåŸºäº target_period_m å·¥ä½œ)
    print(f"  [Main Logic] **æœ€ç»ˆç”¨äºè®¡ç®—çš„ç›®æ ‡æœˆä»½ Period: {target_period_m} (æ¥æº: {target_month_source})**")

    target_month_start = target_period_m.start_time
    target_month_end = target_period_m.end_time
    print(f"  [Main Logic] è®¡ç®—ç”¨ç›®æ ‡æœˆä»½èŒƒå›´: {target_month_start.strftime('%Y-%m-%d')} to {target_month_end.strftime('%Y-%m-%d')}")

    # å®é™…ç”¨äºè®¡ç®—å’Œæ•°æ®æå–çš„æ—¥æœŸèŒƒå›´ï¼Œåº”ä¸¥æ ¼é™åˆ¶åœ¨ç›®æ ‡æœˆä»½å†…æˆ–ç”± analysis_period_dates å®šä¹‰
    # æˆ‘ä»¬å·²ç»æœ‰äº† analysis_period_datesï¼Œå®ƒæ˜¯ç›®æ ‡æœˆä»½å†…çš„æœ‰æ•ˆå‘¨äº”
    # ç¡®ä¿æ‰€æœ‰åç»­çš„æ•°æ®è®¿é—®éƒ½åŸºäºè¿™ä¸ªèŒƒå›´

    fridays_in_target_month = pd.date_range(start=target_month_start, end=target_month_end, freq='W-FRI')
    # Filter fridays that are actually in the model's data index
    analysis_period_dates_all_model_data = all_available_dates[all_available_dates.dayofweek == 4] # æ‰€æœ‰æ¨¡å‹æ•°æ®ä¸­çš„å‘¨äº”
    analysis_period_dates = fridays_in_target_month[fridays_in_target_month.isin(analysis_period_dates_all_model_data)]

    if analysis_period_dates.empty:
        print(f"  [Main Logic] é”™è¯¯: åœ¨ç›®æ ‡æœˆä»½ {target_period_m.strftime('%Y-%m')} ({target_month_source}) å†…ï¼Œæ•°æ®ä¸­æ‰¾ä¸åˆ°ä»»ä½•å‘¨äº”ä½œä¸ºåˆ†ææ—¶ç‚¹ã€‚")
        # åˆ›å»ºç©ºçš„CSVä»¥é¿å…ç»˜å›¾å‡½æ•°å› æ–‡ä»¶ä¸å­˜åœ¨è€Œå¤±è´¥
        pd.DataFrame().to_csv(os.path.join(EVOLUTION_OUTPUT_DIR, "nowcast_evolution_data_T.csv"))
        pd.DataFrame().to_csv(os.path.join(EVOLUTION_OUTPUT_DIR, "news_decomposition_grouped.csv"))
        print(f"  [Main Logic] å·²åˆ›å»ºç©ºçš„CSVæ–‡ä»¶ï¼Œå›¾è¡¨å°†ä¸ºç©ºæˆ–æç¤ºæ— æ•°æ®ã€‚")
        # è¿™é‡Œå¯ä»¥é€‰æ‹© sys.exit(1) æˆ–è€…è®©å®ƒç»§ç»­ç”Ÿæˆç©ºå›¾
    else:
        print(f"  [Main Logic] æœˆå†…åˆ†æ Vintage æ—¥æœŸ (åŸºäºç›®æ ‡æœˆä»½ {target_period_m.strftime('%Y-%m')}): {len(analysis_period_dates)} ä¸ªç‚¹, ä» {analysis_period_dates.min().strftime('%Y-%m-%d')} åˆ° {analysis_period_dates.max().strftime('%Y-%m-%d')}")

    # target_prediction_date é€šå¸¸æ˜¯ target_period_m çš„æœ€åä¸€ä¸ªå‘¨äº” (å¦‚æœæ•°æ®å­˜åœ¨)
    if not analysis_period_dates.empty:
        target_prediction_date = analysis_period_dates.max()
        print(f"  [Main Logic] é¢„æµ‹ç›®æ ‡æ—¥æœŸ T (åŸºäºç›®æ ‡æœˆä»½çš„æœ€åä¸€ä¸ªå¯ç”¨Vintage): {target_prediction_date.strftime('%Y-%m-%d')}")
    else:
        # å¦‚æœ analysis_period_dates ä¸ºç©ºï¼Œéœ€è¦ä¸€ä¸ªå›é€€ï¼Œå¦åˆ™åç»­è®¡ç®—ä¼šå‡ºé”™
        target_prediction_date = target_month_end # æˆ–å…¶ä»–åˆç†çš„å›é€€
        print(f"  [Main Logic] è­¦å‘Š: å› æ— å¯ç”¨Vintageï¼Œé¢„æµ‹ç›®æ ‡æ—¥æœŸ T å›é€€ä¸ºæœˆä»½ç»“æŸ: {target_prediction_date.strftime('%Y-%m-%d')}")

    # --- æ­¥éª¤ 4 & 5 (Nowcastæ¼”å˜ å’Œ æ–°é—»è´¡çŒ®è®¡ç®—) ---
    # åœ¨æ­¤ä¹‹å‰ï¼Œç¡®ä¿æ ¸å¿ƒæ•°æ®å¯¹è±¡ (å¦‚ final_filtered_x, x_minus_df, z_observed) è¢«é€‚å½“åœ°ç­›é€‰
    # ä»¥åŒ¹é… analysis_period_dates æˆ–è‡³å°‘ target_month_start/end

    # å‡è®¾ final_dfm_results.x (å³ final_filtered_x), kf_results_rerun.x_minus, å’Œ z_for_rerun (å³ z_observed)
    # å·²ç»æ˜¯åŠ è½½çš„å®Œæ•´æ•°æ®é›†ï¼Œæˆ‘ä»¬éœ€è¦ä»è¿™é‡Œç­›é€‰å‡ºç›®æ ‡æœˆä»½çš„æ•°æ®
    # æ³¨æ„ï¼šç­›é€‰åº”åŸºäº analysis_period_dates çš„æœ€å°å’Œæœ€å¤§æ—¥æœŸï¼Œæˆ– target_month_start/end
    # ä¸ºç¡®ä¿ä¸ analysis_period_dates ä¸€è‡´ï¼Œä½¿ç”¨å…¶èŒƒå›´
    if not analysis_period_dates.empty:
        calc_start_date = analysis_period_dates.min()
        calc_end_date = analysis_period_dates.max()
    else: # å¦‚æœæ²¡æœ‰åˆ†ææ—¥æœŸï¼Œä½¿ç”¨æœˆä»½èŒƒå›´ï¼Œåç»­è®¡ç®—ä¼šäº§ç”Ÿç©ºç»“æœ
        calc_start_date = target_month_start
        calc_end_date = target_month_end

    print(f"  [Main Logic] ç”¨äºæ ¸å¿ƒè®¡ç®—çš„æ•°æ®ç­›é€‰èŒƒå›´: {calc_start_date.strftime('%Y-%m-%d')} to {calc_end_date.strftime('%Y-%m-%d')}")

    # ç­›é€‰æ ¸å¿ƒæ•°æ®å¸§
    # final_filtered_x æ˜¯ final_dfm_results.x
    current_month_final_filtered_x = final_dfm_results.x[(final_dfm_results.x.index >= calc_start_date) & (final_dfm_results.x.index <= calc_end_date)]
    
    # --- åœ¨ä½¿ç”¨ kf_results_rerun ä¹‹å‰æ£€æŸ¥å…¶æœ‰æ•ˆæ€§ ---
    if kf_results_rerun and hasattr(kf_results_rerun, 'x_minus') and kf_results_rerun.x_minus is not None:
        current_month_x_minus_df = kf_results_rerun.x_minus[(kf_results_rerun.x_minus.index >= calc_start_date) & (kf_results_rerun.x_minus.index <= calc_end_date)]
    else:
        print("  [Main Logic] è­¦å‘Š: kf_results_rerun æ— æ•ˆæˆ–ç¼ºå°‘ x_minusï¼Œæ— æ³•ç­›é€‰ current_month_x_minus_dfã€‚å°†ä½¿ç”¨ç©ºDataFrameã€‚")
        current_month_x_minus_df = pd.DataFrame(index=pd.to_datetime([])) # åˆ›å»ºä¸€ä¸ªç©ºçš„å¸¦DatetimeIndexçš„DataFrame

    # z_observed æ˜¯ z_for_rerun
    current_month_z_observed = z_for_rerun[(z_for_rerun.index >= calc_start_date) & (z_for_rerun.index <= calc_end_date)]
    
    # Kalman_gain_list éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶é•¿åº¦åº”ä¸ z_observed çš„è¡Œæ•°å¯¹åº”
    # å¦‚æœ z_observed è¢«æˆªæ–­ï¼ŒKalman_gain_list ä¹Ÿéœ€è¦ç›¸åº”æˆªæ–­
    # å‡è®¾åŸå§‹ kalman_gain_list å¯¹åº”åŸå§‹ z_for_rerun
    original_z_dates = z_for_rerun.index
    kalman_gain_list_for_month = []
    if not current_month_z_observed.empty:
        # --- åŒæ ·æ£€æŸ¥ kf_results_rerun å’Œ Kalman_gain çš„æœ‰æ•ˆæ€§ ---
        if kf_results_rerun and hasattr(kf_results_rerun, 'Kalman_gain') and kf_results_rerun.Kalman_gain is not None:
            for date_in_month in current_month_z_observed.index:
                try:
                    original_idx = original_z_dates.get_loc(date_in_month)
                    if original_idx < len(kf_results_rerun.Kalman_gain):
                         kalman_gain_list_for_month.append(kf_results_rerun.Kalman_gain[original_idx])
                    else:
                        print(f"  [Main Logic] è­¦å‘Š: æ— æ³•ä¸ºæ—¥æœŸ {date_in_month} æ‰¾åˆ°å¯¹åº”çš„Kalmanå¢ç›Š (ç´¢å¼•è¶…å‡ºèŒƒå›´)")
                except KeyError:
                    print(f"  [Main Logic] è­¦å‘Š: æ—¥æœŸ {date_in_month} ä¸åœ¨åŸå§‹ z_for_rerun.index ä¸­ï¼Œæ— æ³•è·å–Kalmanå¢ç›Šã€‚")
        else:
            print("  [Main Logic] è­¦å‘Š: kf_results_rerun æ— æ•ˆæˆ–ç¼ºå°‘ Kalman_gainï¼Œæ— æ³•å¡«å…… kalman_gain_list_for_monthã€‚")
    else:
        print("  [Main Logic] current_month_z_observed ä¸ºç©ºï¼Œkalman_gain_list_for_month å°†ä¸ºç©ºã€‚")


    print(f"--- æ­¥éª¤ 4: è®¡ç®—ç›®æ ‡æ—¥æœŸ {target_prediction_date.strftime('%Y-%m-%d')} (é’ˆå¯¹æœˆä»½ {target_period_m.strftime('%Y-%m')}) çš„ Nowcast æ¼”å˜åºåˆ— ---")
    nowcast_evolution_list = []
    # ... nowcast_evolution_list çš„è®¡ç®—åº”è¯¥ä½¿ç”¨ current_month_final_filtered_x å’Œ analysis_period_dates ...
    # ä¾‹å¦‚: for t_vintage in analysis_period_dates:
    #           x_t = current_month_final_filtered_x.loc[t_vintage].values (å¦‚æœ t_vintage ä»åœ¨ current_month_final_filtered_x.index ä¸­)
    #           ...
    # --- ç¡®ä¿å¾ªç¯åŸºäº analysis_period_datesï¼Œå¹¶ä¸”æ•°æ®ä» current_month_... DataFrame è·å– ---
    A_pow_k_cache = {} # åœ¨å¾ªç¯å¤–å®šä¹‰
    lambda_target_row = metadata.get('lambda_target_row') # å‡è®¾è¿™ä¸ªä»å…ƒæ•°æ®ä¸­é¢„å­˜æˆ–è®¡ç®—
    if lambda_target_row is None:
        # ä» Lambda_np (å¦‚æœå·²åŠ è½½) ä¸­è·å–
        if 'Lambda_np' in locals() and 'TARGET_VARIABLE_FROM_METADATA' in locals() and 'obs_names_from_lambda' in locals():
            try:
                target_var_idx = obs_names_from_lambda.index(TARGET_VARIABLE_FROM_METADATA)
                lambda_target_row = Lambda_np[target_var_idx, :]
            except (ValueError, IndexError) as e_lambda_idx:
                print(f"  [Main Logic] é”™è¯¯: æ— æ³•ä»Lambdaä¸­æ‰¾åˆ°ç›®æ ‡å˜é‡ '{TARGET_VARIABLE_FROM_METADATA}' çš„è¡Œ: {e_lambda_idx}")
                lambda_target_row = np.array([]) # è®¾ç½®ä¸ºç©ºï¼Œåç»­ä¼šå®‰å…¨å¤±è´¥
        else:
            print("  [Main Logic] é”™è¯¯: Lambda_np æˆ–ç›¸å…³å˜é‡æœªå®šä¹‰ï¼Œæ— æ³•è·å– lambda_target_row.")
            lambda_target_row = np.array([])

    target_mean_original = metadata.get('target_mean_original')
    target_std_original = metadata.get('target_std_original')
    n_factors = final_dfm_results.A.shape[0] # ä»åŠ è½½çš„AçŸ©é˜µè·å–
    A_matrix = final_dfm_results.A # ä»åŠ è½½çš„AçŸ©é˜µè·å–

    if lambda_target_row.size > 0 and target_mean_original is not None and target_std_original is not None:
        for t_vintage in analysis_period_dates: # è¿™é‡Œçš„ analysis_period_dates å·²ç»æ˜¯ç›®æ ‡æœˆä»½çš„æ—¥æœŸ
            # ğŸ”¥ ä¿®å¤ï¼šç¡®ä¿æ¯ä¸ªæ—¶é—´ç‚¹éƒ½è¢«è®°å½•ï¼Œä¿æŒä¸æ–°é—»æ•°æ®çš„æ—¶é—´ç‚¹ä¸€è‡´æ€§
            days_diff = (target_prediction_date - t_vintage).days
            weeks_diff = int(np.round(days_diff / 7))
            weeks_diff = max(0, weeks_diff)

            if t_vintage in current_month_final_filtered_x.index:
                x_t = current_month_final_filtered_x.loc[t_vintage].values

                if weeks_diff == 0:
                    A_pow_k = np.eye(n_factors)
                elif weeks_diff in A_pow_k_cache:
                    A_pow_k = A_pow_k_cache[weeks_diff]
                else:
                    A_pow_k = np.linalg.matrix_power(A_matrix, weeks_diff)
                    A_pow_k_cache[weeks_diff] = A_pow_k

                x_T_given_t = A_pow_k @ x_t
                nowcast_std_val = lambda_target_row @ x_T_given_t
                nowcast_orig_val = (nowcast_std_val * target_std_original + target_mean_original) if pd.notna(target_std_original) and pd.notna(target_mean_original) and target_std_original != 0 else nowcast_std_val

                nowcast_evolution_list.append({
                    'date': t_vintage,
                    'target_prediction_date': target_prediction_date,
                    'nowcast_orig': nowcast_orig_val,
                    'nowcast_std': nowcast_std_val,
                    'forecast_horizon_weeks': weeks_diff
                })
            else:
                # å³ä½¿æ•°æ®ä¸å¯ç”¨ï¼Œä¹Ÿæ·»åŠ ä¸€ä¸ªå ä½ç¬¦è®°å½•ä»¥ä¿æŒæ—¶é—´ç‚¹ä¸€è‡´æ€§
                print(f"  [Main Logic] è­¦å‘Š (æ¼”å˜è®¡ç®—): Vintageæ—¥æœŸ {t_vintage.strftime('%Y-%m-%d')} ä¸åœ¨ç­›é€‰åçš„å› å­æ•°æ®ä¸­ï¼Œæ·»åŠ NaNå ä½ç¬¦ã€‚")
                nowcast_evolution_list.append({
                    'date': t_vintage,
                    'target_prediction_date': target_prediction_date,
                    'nowcast_orig': np.nan,
                    'nowcast_std': np.nan,
                    'forecast_horizon_weeks': weeks_diff
                })
    else:
        print("  [Main Logic] æ¼”å˜è®¡ç®—æ‰€éœ€å‚æ•° (lambda_target_row, mean, std) ä¸å®Œæ•´ï¼Œåˆ›å»ºå ä½ç¬¦æ•°æ®ä»¥ä¿æŒæ—¶é—´ç‚¹ä¸€è‡´æ€§ã€‚")
        # å³ä½¿å‚æ•°ä¸å®Œæ•´ï¼Œä¹Ÿä¸ºæ‰€æœ‰æ—¶é—´ç‚¹åˆ›å»ºå ä½ç¬¦è®°å½•
        for t_vintage in analysis_period_dates:
            days_diff = (target_prediction_date - t_vintage).days
            weeks_diff = int(np.round(days_diff / 7))
            weeks_diff = max(0, weeks_diff)
            nowcast_evolution_list.append({
                'date': t_vintage,
                'target_prediction_date': target_prediction_date,
                'nowcast_orig': np.nan,
                'nowcast_std': np.nan,
                'forecast_horizon_weeks': weeks_diff
            })

    nowcast_forecast_df = pd.DataFrame(nowcast_evolution_list).set_index('date') if nowcast_evolution_list else pd.DataFrame(columns=['target_prediction_date', 'nowcast_orig', 'nowcast_std', 'forecast_horizon_weeks'])

    nowcast_output_path = os.path.join(EVOLUTION_OUTPUT_DIR, "nowcast_evolution_data_T.csv")
    if 'nowcast_forecast_df' in locals() and not nowcast_forecast_df.empty:
        nowcast_forecast_df.to_csv(nowcast_output_path, index_label='date')
        print(f"  [Main Logic] Nowcast æ¼”å˜æ•°æ®å·²ä¿å­˜åˆ°: {nowcast_output_path} (é’ˆå¯¹ç›®æ ‡æœˆä»½: {target_period_m.strftime('%Y-%m')})")
    else:
        pd.DataFrame().to_csv(nowcast_output_path) # ä¿å­˜ç©ºæ–‡ä»¶ä»¥é˜²ç»˜å›¾å‡ºé”™
        print(f"  [Main Logic] Nowcast æ¼”å˜æ•°æ®ä¸ºç©ºï¼Œå·²ä¿å­˜ç©ºCSVåˆ°: {nowcast_output_path}")

    print(f"--- æ­¥éª¤ 5: è®¡ç®—ç›®æ ‡æ—¥æœŸ {target_prediction_date.strftime('%Y-%m-%d')} (é’ˆå¯¹æœˆä»½ {target_period_m.strftime('%Y-%m')}) çš„æ–°é—»è´¡çŒ® ---")
    news_decomposition_list = []
    # ... news_decomposition_list çš„è®¡ç®—åº”è¯¥ä½¿ç”¨ current_month_x_minus_df, current_month_z_observed, kalman_gain_list_for_month å’Œ analysis_period_dates ...
    # ä¾‹å¦‚: for t_idx, t_vintage in enumerate(analysis_period_dates): 
    #           x_t_minus_1 = current_month_x_minus_df.loc[t_vintage].values (å¦‚æœå­˜åœ¨)
    #           K_t = kalman_gain_list_for_month[t_idx] (å¦‚æœå­˜åœ¨)
    #           z_t = current_month_z_observed.loc[t_vintage].values (å¦‚æœå­˜åœ¨)
    #           ...
    # ç¡®ä¿ obs_names_from_lambda å’Œ Lambda_np åœ¨æ­¤ä½œç”¨åŸŸå†…å¯ç”¨
    var_industry_map = metadata.get('var_industry_map', {})
    industry_groups = list(set(var_industry_map.values())) + ['å…¶ä»–æœªåˆ†ç±»']
    var_to_group = {var: get_industry_group(var, var_industry_map) for var in obs_names_from_lambda} if 'obs_names_from_lambda' in locals() else {}
    n_obs = Lambda_np.shape[0] if 'Lambda_np' in locals() else 0

    if lambda_target_row.size > 0 and target_std_original is not None and 'Lambda_np' in locals():
        print(f"  [Main Logic] å¼€å§‹æ–°é—»è´¡çŒ®è®¡ç®—å¾ªç¯ï¼Œå…± {len(analysis_period_dates)} ä¸ªVintageæ—¥æœŸ...")
        for t_idx, t_vintage in enumerate(analysis_period_dates):
            print(f"    [News Calc Loop] å¤„ç† Vintage: {t_vintage.strftime('%Y-%m-%d')} (ç´¢å¼• {t_idx}) ")

            # ğŸ”¥ ä¿®å¤ï¼šç¡®ä¿æ¯ä¸ªæ—¶é—´ç‚¹éƒ½è¢«è®°å½•ï¼Œä¿æŒä¸æ¼”å˜æ•°æ®çš„æ—¶é—´ç‚¹ä¸€è‡´æ€§
            # å…ˆåˆ›å»ºé»˜è®¤çš„é›¶è´¡çŒ®è®°å½•
            news_data_row = {'date': t_vintage}
            for group in industry_groups:
                news_data_row[group] = 0.0

            # å°è¯•è®¡ç®—å®é™…çš„æ–°é—»è´¡çŒ®
            if (t_vintage in current_month_x_minus_df.index and \
                t_vintage in current_month_z_observed.index and \
                t_idx < len(kalman_gain_list_for_month)):

                x_t_minus_1 = current_month_x_minus_df.loc[t_vintage].values
                K_t = kalman_gain_list_for_month[t_idx]
                z_t_actual = current_month_z_observed.loc[t_vintage].values

                if K_t is not None and K_t.shape[0] == n_factors and K_t.shape[1] == n_obs:
                    z_t_pred = Lambda_np @ x_t_minus_1
                    nu_t = np.full(n_obs, np.nan)
                    valid_obs_mask = ~np.isnan(z_t_actual)
                    # ä¿®å¤ç¬¦å·é”™è¯¯ï¼šæ–°é—»åº”è¯¥æ˜¯é¢„æµ‹å€¼å‡å»å®é™…å€¼ï¼Œä½¿ç¬¦å·ä¸nowcastå˜åŒ–ä¸€è‡´
                    nu_t[valid_obs_mask] = z_t_pred[valid_obs_mask] - z_t_actual[valid_obs_mask]

                    current_vintage_evo_data = nowcast_forecast_df[nowcast_forecast_df.index == t_vintage]
                    if not current_vintage_evo_data.empty:
                        weeks_diff = int(current_vintage_evo_data['forecast_horizon_weeks'].iloc[0])
                    else:
                        days_diff_news = (target_prediction_date - t_vintage).days
                        weeks_diff = int(np.round(days_diff_news / 7))
                        weeks_diff = max(0, weeks_diff)

                    if weeks_diff == 0: A_pow_k = np.eye(n_factors)
                    elif weeks_diff in A_pow_k_cache: A_pow_k = A_pow_k_cache[weeks_diff]
                    else: A_pow_k = np.linalg.matrix_power(A_matrix, weeks_diff); A_pow_k_cache[weeks_diff] = A_pow_k

                    term1 = lambda_target_row @ A_pow_k
                    weights_w = term1 @ K_t
                    news_contributions_i = weights_w * nu_t
                    news_contributions_i_orig = news_contributions_i * target_std_original if pd.notna(target_std_original) and target_std_original != 0 else news_contributions_i

                    grouped_news = defaultdict(float)
                    if 'obs_names_from_lambda' in locals():
                        for i, var_name in enumerate(obs_names_from_lambda):
                            contribution = news_contributions_i_orig[i]
                            if not pd.isna(contribution) and abs(contribution) > 1e-6: # ä»…è®°å½•énanä¸”æœ‰æ˜¾è‘—å½±å“çš„è´¡çŒ®
                                grouped_news[var_to_group.get(var_name, 'å…¶ä»–æœªåˆ†ç±»')] += contribution

                    # æ›´æ–°è®°å½•ä¸­çš„å®é™…è´¡çŒ®å€¼
                    for group in industry_groups:
                        news_data_row[group] = grouped_news.get(group, 0.0)

                    # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºæ˜¯å¦æœ‰æ˜¾è‘—è´¡çŒ®
                    if grouped_news:
                        print(f"      [News Calc Loop] ä¸º {t_vintage.strftime('%Y-%m-%d')} æ·»åŠ äº†æ–°é—»åˆ†è§£æ•°æ®: {dict(grouped_news)}")
                    else:
                        print(f"      [News Calc Loop] ä¸º {t_vintage.strftime('%Y-%m-%d')} æ·»åŠ äº†é›¶è´¡çŒ®æ•°æ®ï¼ˆä¿æŒæ—¶é—´ç‚¹ä¸€è‡´æ€§ï¼‰")
                else:
                    print(f"      [News Calc Loop] è­¦å‘Š: Vintage {t_vintage.strftime('%Y-%m-%d')} çš„Kalmanå¢ç›Šå½¢çŠ¶ä¸æ­£ç¡®æˆ–ä¸ºNoneï¼Œä½¿ç”¨é›¶è´¡çŒ®ã€‚")
            else:
                print(f"    [News Calc Loop] è­¦å‘Š: Vintageæ—¥æœŸ {t_vintage.strftime('%Y-%m-%d')} æ•°æ®ä¸å®Œæ•´ï¼Œä½¿ç”¨é›¶è´¡çŒ®ä¿æŒæ—¶é—´ç‚¹ä¸€è‡´æ€§ã€‚")

            # æ— è®ºå¦‚ä½•éƒ½æ·»åŠ è®°å½•ï¼Œç¡®ä¿æ—¶é—´ç‚¹ä¸€è‡´æ€§
            news_decomposition_list.append(news_data_row)
    else:
        print("  [Main Logic] æ–°é—»è´¡çŒ®è®¡ç®—æ‰€éœ€å‚æ•° (lambda_target_row, std, Lambda_np) ä¸å®Œæ•´ï¼Œåˆ›å»ºå ä½ç¬¦æ•°æ®ä»¥ä¿æŒæ—¶é—´ç‚¹ä¸€è‡´æ€§ã€‚")
        # å³ä½¿å‚æ•°ä¸å®Œæ•´ï¼Œä¹Ÿä¸ºæ‰€æœ‰æ—¶é—´ç‚¹åˆ›å»ºå ä½ç¬¦è®°å½•
        for t_vintage in analysis_period_dates:
            news_data_row = {'date': t_vintage}
            for group in industry_groups:
                news_data_row[group] = 0.0
            news_decomposition_list.append(news_data_row)

    news_decomposition_df = pd.DataFrame(news_decomposition_list).set_index('date') if news_decomposition_list else pd.DataFrame(columns=industry_groups)
    if not news_decomposition_df.empty:
        for group in industry_groups: # ç¡®ä¿æ‰€æœ‰é¢„æœŸçš„è¡Œä¸šç»„åˆ—éƒ½å­˜åœ¨ï¼Œå³ä½¿å®ƒä»¬åœ¨è¯¥æœˆä»½æ²¡æœ‰è´¡çŒ®
            if group not in news_decomposition_df.columns:
                news_decomposition_df[group] = 0.0
        news_decomposition_df = news_decomposition_df[industry_groups] # ç¡®ä¿åˆ—é¡ºåºä¸€è‡´æ€§
        print("  [Main Logic] ç”Ÿæˆçš„ news_decomposition_df (å‰5è¡Œ):\n", news_decomposition_df.head())
    else:
        print("  [Main Logic] news_decomposition_df ä¸ºç©º (æ²¡æœ‰è®¡ç®—å‡ºä»»ä½•æ–°é—»è´¡çŒ®)ã€‚")

    news_output_path = os.path.join(EVOLUTION_OUTPUT_DIR, "news_decomposition_grouped.csv")
    if 'news_decomposition_df' in locals() and not news_decomposition_df.empty:
        # ğŸ”¥ ä¿®å¤ç¼–ç é—®é¢˜ï¼šä½¿ç”¨UTF-8ç¼–ç ä¿å­˜CSV
        news_decomposition_df.to_csv(news_output_path, index_label='date', encoding='utf-8')
        print(f"  [Main Logic] æ–°é—»åˆ†è§£æ•°æ®å·²ä¿å­˜åˆ°: {news_output_path} (é’ˆå¯¹ç›®æ ‡æœˆä»½: {target_period_m.strftime('%Y-%m')})")
    else:
        # ğŸ”¥ ä¿®å¤ç¼–ç é—®é¢˜ï¼šç©ºæ–‡ä»¶ä¹Ÿä½¿ç”¨UTF-8ç¼–ç 
        pd.DataFrame().to_csv(news_output_path, encoding='utf-8') # ä¿å­˜ç©ºæ–‡ä»¶
        print(f"  [Main Logic] æ–°é—»åˆ†è§£æ•°æ®ä¸ºç©ºï¼Œå·²ä¿å­˜ç©ºCSVåˆ°: {news_output_path}")


    # --- æ­¥éª¤ 6 (æ—§ numbering): ç”Ÿæˆå›¾è¡¨ ---
    plot_output_file_arg = args.plot_output_file 
    
    plot_base_name_for_func = "news_analysis_plot" # é»˜è®¤åŸºç¡€å
    plot_output_dir_for_func = EVOLUTION_OUTPUT_DIR # é»˜è®¤è¾“å‡ºç›®å½•

    if plot_output_file_arg:
        plot_output_dir_for_func = os.path.dirname(plot_output_file_arg)
        base_name_with_ext = os.path.basename(plot_output_file_arg)
        plot_base_name_for_func = os.path.splitext(base_name_with_ext)[0]
        # å¦‚æœ backend ä¼ æ¥çš„åå­—æ˜¯ news_analysis_plot_backend.html, é‚£ä¹ˆ plot_base_name_for_func ä¼šæ˜¯ news_analysis_plot_backend
        print(f"  [Main] ä½¿ç”¨æ¥è‡ªå‚æ•°çš„è¾“å‡ºç›®å½•: {plot_output_dir_for_func} å’ŒåŸºç¡€æ–‡ä»¶å: {plot_base_name_for_func}")
    else:
        # å¦‚æœåç«¯æ²¡æœ‰æä¾›æ˜ç¡®çš„æ–‡ä»¶å (ç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸º backend.py ä¼šæä¾›)
        print(f"  [Main] æœªæŒ‡å®š --plot_output_file, å°†ä½¿ç”¨é»˜è®¤è¾“å‡ºç›®å½•: {plot_output_dir_for_func} å’ŒåŸºç¡€æ–‡ä»¶å: {plot_base_name_for_func}")
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(plot_output_dir_for_func, exist_ok=True)

    print(f"--- æ­¥éª¤ 5: è°ƒç”¨ç»˜å›¾å‡½æ•°ç”Ÿæˆæ–°é—»åˆ†è§£å›¾å’Œæ¼”å˜å›¾ (åŸºç¡€å: {plot_base_name_for_func}) ---")
    target_var_display_name = metadata.get('target_variable_label', TARGET_VARIABLE) \
        if 'metadata' in locals() and metadata else TARGET_VARIABLE

    try:
        # è°ƒç”¨ä¿®æ”¹åçš„ç»˜å›¾å‡½æ•°
        generated_plot_paths = plot_news_decomposition(
            input_dir=EVOLUTION_OUTPUT_DIR, 
            # output_file=plot_output_file_arg, # <<< ä¸å†ç›´æ¥ä½¿ç”¨è¿™ä¸ªå‚æ•°
            base_output_filename=plot_base_name_for_func,
            output_dir=plot_output_dir_for_func,
            plot_start_date=args.plot_start_date,
            plot_end_date=args.plot_end_date,
            target_variable_name=target_var_display_name
        )
        # print(f"å›¾è¡¨å·²ç”Ÿæˆ: {plot_output_file_arg}") # æ—§çš„æ‰“å°ä¿¡æ¯
        if generated_plot_paths["evolution_plot_path"]:
            print(f"  æ¼”å˜å›¾å·²ç”Ÿæˆ: {generated_plot_paths['evolution_plot_path']}")
        else:
            print("  æ¼”å˜å›¾ç”Ÿæˆå¤±è´¥æˆ–æœªä¿å­˜ã€‚")
        if generated_plot_paths["decomposition_plot_path"]:
            print(f"  åˆ†è§£å›¾å·²ç”Ÿæˆ: {generated_plot_paths['decomposition_plot_path']}")
        else:
            print("  åˆ†è§£å›¾ç”Ÿæˆå¤±è´¥æˆ–æœªä¿å­˜ã€‚")
            
    except FileNotFoundError as e_plot_fnf:
        print(f"ç»˜å›¾å¤±è´¥: æ‰€éœ€æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {e_plot_fnf}")
        # sys.exit(1) # æ ¹æ®æƒ…å†µå†³å®šæ˜¯å¦å› ä¸ºç»˜å›¾å¤±è´¥è€Œé€€å‡º
    except ValueError as e_plot_val:
        print(f"ç»˜å›¾å¤±è´¥: æ•°æ®å€¼é”™è¯¯æˆ–æ—¥æœŸèŒƒå›´æ— æ•ˆ: {e_plot_val}")
    except RuntimeError as e_plot_rt:
        print(f"ç»˜å›¾å¤±è´¥: è¿è¡Œæ—¶é”™è¯¯: {e_plot_rt}")
    except Exception as e_plot:
        print(f"ç”Ÿæˆå›¾è¡¨æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e_plot}")
        import traceback
        traceback.print_exc()


    print("--- Nowcast æ¼”å˜ä¸æ–°é—»è´¡çŒ®åˆ†æå®Œæˆ ---")
    print("[DEBUG] Script finished.") 