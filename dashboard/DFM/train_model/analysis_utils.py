# -*- coding: utf-8 -*-
"""
åŒ…å« DFM ç»“æœåˆ†æç›¸å…³å·¥å…·å‡½æ•°çš„æ¨¡å—ï¼Œä¾‹å¦‚ PCA å’Œå› å­è´¡çŒ®åº¦è®¡ç®—ã€‚
"""
import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn.impute import SimpleImputer # å¦‚æœ PCA éœ€è¦å¡«å……
from typing import Tuple, Dict, Optional, List, Any
import statsmodels.api as sm
from sklearn.metrics import mean_squared_error, mean_absolute_error # ç¡®ä¿å¯¼å…¥
from collections import defaultdict
import unicodedata
import logging # Import logging

# Get logger for this module
logger = logging.getLogger(__name__) # <<< æ·»åŠ è·å– logger å®ä¾‹

def calculate_pca_variance(
    data_standardized: pd.DataFrame,
    n_components: int,
    impute_strategy: str = 'mean'
) -> Optional[pd.DataFrame]:
    """
    è®¡ç®—ç»™å®šæ ‡å‡†åŒ–æ•°æ®çš„ PCA è§£é‡Šæ–¹å·®ã€‚

    Args:
        data_standardized (pd.DataFrame): è¾“å…¥çš„æ ‡å‡†åŒ–æ•°æ® (è¡Œä¸ºæ—¶é—´ï¼Œåˆ—ä¸ºå˜é‡)ã€‚
        n_components (int): è¦æå–çš„ä¸»æˆåˆ†æ•°é‡ã€‚
        impute_strategy (str): å¤„ç†ç¼ºå¤±å€¼çš„ç­–ç•¥ ('mean', 'median', 'most_frequent', or None).

    Returns:
        Optional[pd.DataFrame]:
            - pca_results_df: åŒ…å« PCA ç»“æœçš„ DataFrame (ä¸»æˆåˆ†, è§£é‡Šæ–¹å·®%, ç´¯è®¡è§£é‡Šæ–¹å·®%),
                              å¦‚æœå‘ç”Ÿé”™è¯¯æˆ–æ— æ³•è®¡ç®—åˆ™è¿”å› Noneã€‚
    """
    print("\nè®¡ç®— PCA è§£é‡Šæ–¹å·®...")
    pca_results_df = None

    try:
        if data_standardized is None or data_standardized.empty:
            print("  é”™è¯¯: è¾“å…¥çš„æ ‡å‡†åŒ–æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è®¡ç®— PCAã€‚")
            return None
        if n_components <= 0:
            print(f"  é”™è¯¯: æ— æ•ˆçš„ä¸»æˆåˆ†æ•°é‡ ({n_components})ã€‚")
            return None
        
        data_for_pca = data_standardized.copy()
        print(f"  ä½¿ç”¨æ•°æ® (Shape: {data_for_pca.shape}) è¿›è¡Œ PCA åˆ†æã€‚")

        # å¤„ç†ç¼ºå¤±å€¼
        nan_count = data_for_pca.isna().sum().sum()
        data_pca_imputed_array = None # åˆå§‹åŒ–
        if nan_count > 0:
            if impute_strategy:
                print(f"  å¤„ç† PCA è¾“å…¥æ•°æ®çš„ç¼ºå¤±å€¼ (å…± {nan_count} ä¸ªï¼Œä½¿ç”¨ç­–ç•¥: {impute_strategy})...")
                imputer = SimpleImputer(strategy=impute_strategy)
                # ç›´æ¥è·å– NumPy æ•°ç»„
                data_pca_imputed_array = imputer.fit_transform(data_for_pca)
                print(f"  å¡«å……å NumPy æ•°ç»„ Shape: {data_pca_imputed_array.shape}")
                # æ£€æŸ¥å¡«å……åæ˜¯å¦ä»æœ‰ NaN (ç†è®ºä¸Š SimpleImputer ä¸ä¼šç•™ä¸‹)
                if np.isnan(data_pca_imputed_array).sum() > 0:
                    print("  é”™è¯¯: SimpleImputer å¡«å……åä»å­˜åœ¨ NaNã€‚PCA æ— æ³•è¿›è¡Œã€‚")
                    return None
            else:
                print(f"  è­¦å‘Š: æ•°æ®åŒ…å« {nan_count} ä¸ª NaNï¼Œä½†æœªæŒ‡å®šå¡«å……ç­–ç•¥ã€‚PCA å¯èƒ½å¤±è´¥ã€‚")
                data_pca_imputed_array = data_for_pca.to_numpy() # è½¬æ¢ä¸º NumPy ç»§ç»­å°è¯•
        else:
            data_pca_imputed_array = data_for_pca.to_numpy() # æ— ç¼ºå¤±å€¼ï¼Œç›´æ¥è½¬ NumPy
            print("  æ•°æ®æ— ç¼ºå¤±å€¼ï¼Œæ— éœ€å¡«å……ã€‚")

        # æ£€æŸ¥æœ€ç»ˆæ•°ç»„æ˜¯å¦æœ‰æ•ˆ
        if data_pca_imputed_array is None or data_pca_imputed_array.shape[1] == 0:
            print("  é”™è¯¯: å¤„ç†/å¡«å……åæ•°æ®ä¸ºç©ºæˆ–æ²¡æœ‰åˆ—ï¼Œæ— æ³•æ‰§è¡Œ PCAã€‚")
            return None
        if data_pca_imputed_array.shape[1] < n_components:
            print(f"  è­¦å‘Š: å¤„ç†/å¡«å……åæ•°æ®åˆ—æ•° ({data_pca_imputed_array.shape[1]}) å°‘äºè¯·æ±‚çš„ä¸»æˆåˆ†æ•° ({n_components})ã€‚å°†ä½¿ç”¨ {data_pca_imputed_array.shape[1]} ä½œä¸ºä¸»æˆåˆ†æ•°ã€‚")
            n_components = data_pca_imputed_array.shape[1]
            if n_components == 0:
                 print("  é”™è¯¯: è°ƒæ•´åä¸»æˆåˆ†æ•°ä¸º 0ã€‚æ— æ³•æ‰§è¡Œ PCAã€‚")
                 return None

        # æ‰§è¡Œ PCA (åœ¨ NumPy æ•°ç»„ä¸Š)
        pca = PCA(n_components=n_components)
        print(f"  å¯¹å¤„ç†/å¡«å……åçš„æ•°æ® (Shape: {data_pca_imputed_array.shape}) æ‰§è¡Œ PCA (n_components={n_components})...")
        pca.fit(data_pca_imputed_array)

        explained_variance_ratio_pct = pca.explained_variance_ratio_ * 100
        cumulative_explained_variance_pct = np.cumsum(explained_variance_ratio_pct)

        pca_results_df = pd.DataFrame({
            'ä¸»æˆåˆ† (PC)': [f'PC{i+1}' for i in range(n_components)],
            'è§£é‡Šæ–¹å·® (%)': explained_variance_ratio_pct,
            'ç´¯è®¡è§£é‡Šæ–¹å·® (%)': cumulative_explained_variance_pct,
            'ç‰¹å¾å€¼ (Eigenvalue)': pca.explained_variance_
        })

        print("  PCA è§£é‡Šæ–¹å·®è®¡ç®—å®Œæˆ:")
        print(pca_results_df.to_string(index=False))

    except Exception as e_pca_main:
        print(f"  è®¡ç®— PCA è§£é‡Šæ–¹å·®æ—¶å‘ç”Ÿé”™è¯¯: {e_pca_main}")
        import traceback
        traceback.print_exc()
        pca_results_df = None
        
    logger.debug(f"calculate_pca_variance è¿”å› pca_results_df ç±»å‹: {type(pca_results_df)}")
    return pca_results_df

def calculate_factor_contributions(
    dfm_results: object, 
    data_processed: pd.DataFrame, 
    target_variable: str, 
    n_factors: int
) -> Tuple[Optional[pd.DataFrame], Optional[Dict[str, float]]]:
    """
    è®¡ç®— DFM å„å› å­å¯¹ç›®æ ‡å˜é‡æ–¹å·®çš„è´¡çŒ®åº¦ã€‚
    ä¿®æ­£ï¼šä½¿ç”¨ OLS å°†åŸå§‹å°ºåº¦çš„ç›®æ ‡å˜é‡å¯¹æ ‡å‡†åŒ–å› å­å›å½’ï¼Œä»¥è·å¾—æ­£ç¡®çš„è½½è·ã€‚

    Args:
        dfm_results (object): DFM æ¨¡å‹è¿è¡Œç»“æœå¯¹è±¡ (éœ€è¦åŒ…å« x_sm å±æ€§, å³æ ‡å‡†åŒ–å› å­)ã€‚
        data_processed (pd.DataFrame): DFM æ¨¡å‹è¾“å…¥çš„å¤„ç†åæ•°æ® (åŒ…å«åŸå§‹/å¹³ç¨³åŒ–å°ºåº¦çš„ç›®æ ‡å˜é‡)ã€‚
        target_variable (str): ç›®æ ‡å˜é‡åç§°ã€‚
        n_factors (int): æ¨¡å‹ä½¿ç”¨çš„å› å­æ•°é‡ã€‚

    Returns:
        Tuple[Optional[pd.DataFrame], Optional[Dict[str, float]]]: 
            - contribution_df: åŒ…å«å„å› å­è´¡çŒ®åº¦è¯¦æƒ…çš„ DataFrameï¼Œå‡ºé”™åˆ™ä¸º Noneã€‚
            - factor_contributions: å› å­åç§°åˆ°æ€»æ–¹å·®è´¡çŒ®åº¦(%)çš„å­—å…¸ï¼Œå‡ºé”™åˆ™ä¸º Noneã€‚
    """
    print("\nè®¡ç®—å„å› å­å¯¹ç›®æ ‡å˜é‡çš„è´¡çŒ®åº¦ (ä¿®æ­£ OLS æ–¹æ³•)...")
    contribution_df = None
    factor_contributions_dict = None
    
    try:
        # 1. æå–æ ‡å‡†åŒ–å› å­å’ŒåŸå§‹ç›®æ ‡å˜é‡
        if not (dfm_results and hasattr(dfm_results, 'x_sm') and isinstance(dfm_results.x_sm, pd.DataFrame)):
            print("  é”™è¯¯: DFM ç»“æœå¯¹è±¡æ— æ•ˆæˆ–ç¼ºå°‘ 'x_sm' (æ ‡å‡†åŒ–å› å­) å±æ€§ã€‚")
            return None, None
        factors_std = dfm_results.x_sm
        print(f"  [DEBUG] Shape of factors_std: {factors_std.shape}")
        print(f"  [DEBUG] Value of n_factors passed to function: {n_factors}")
        # --- ç»“æŸè°ƒè¯•æ‰“å° ---
        if not (data_processed is not None and target_variable in data_processed.columns):
            print("  é”™è¯¯: 'data_processed' æ— æ•ˆæˆ–ä¸åŒ…å«ç›®æ ‡å˜é‡ã€‚")
            return None, None
        target_orig = data_processed[target_variable]

        # ç¡®ä¿å› å­æ•°é‡æœ‰æ•ˆ (ä¿®æ­£ç±»å‹æ£€æŸ¥ - ç›´æ¥åœ¨ if ä¸­ä¿®æ­£)
        if not (isinstance(n_factors, (int, np.integer)) and n_factors > 0 and n_factors <= factors_std.shape[1]):
             print(f"  é”™è¯¯: æ— æ•ˆçš„å› å­æ•°é‡ ({n_factors}, ç±»å‹: {type(n_factors)}) æˆ–ä¸å› å­çŸ©é˜µç»´åº¦ä¸ç¬¦ (Shape: {factors_std.shape})ã€‚")
             return None, None
        factors_std = factors_std.iloc[:, :n_factors] # åªé€‰æ‹©å®é™…ä½¿ç”¨çš„å› å­åˆ—

        print(f"  æå–åˆ°æ ‡å‡†åŒ–å› å­ (Shape: {factors_std.shape}) å’Œç›®æ ‡å˜é‡ (Length: {len(target_orig)})ã€‚")

        # 2. å¯¹é½æ•°æ®å¹¶å¤„ç†ç¼ºå¤±å€¼ä»¥è¿›è¡Œ OLS
        # åˆå¹¶å› å­å’Œç›®æ ‡å˜é‡ï¼ŒæŒ‰ç´¢å¼•å¯¹é½
        merged_data = pd.concat([target_orig, factors_std], axis=1).dropna()
        if merged_data.empty:
            print("  é”™è¯¯: å¯¹é½å› å­å’Œç›®æ ‡å˜é‡å¹¶ç§»é™¤ NaN åæ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œ OLSã€‚")
            return None, None
            
        target_ols = merged_data[target_variable]
        factors_ols = merged_data[factors_std.columns]
        print(f"  OLS ä½¿ç”¨çš„æ•°æ®ç‚¹æ•°: {len(merged_data)}")

        # æ·»åŠ å¸¸æ•°é¡¹è¿›è¡Œ OLS (å› ä¸º target_orig æœªä¸­å¿ƒåŒ–)
        factors_ols_with_const = sm.add_constant(factors_ols)

        # 3. æ‰§è¡Œ OLS: target_orig ~ const + factors_std
        print("  æ‰§è¡Œ OLS å›å½’: ç›®æ ‡å˜é‡ ~ æ ‡å‡†åŒ–å› å­...")
        ols_model = sm.OLS(target_ols, factors_ols_with_const)
        ols_results = ols_model.fit()
        
        # æå–å› å­å¯¹åº”çš„ç³»æ•° (æ’é™¤å¸¸æ•°é¡¹)
        loadings_orig_scale = ols_results.params.drop('const', errors='ignore').values 
        if len(loadings_orig_scale) != n_factors:
             print(f"  é”™è¯¯: OLS ç»“æœä¸­çš„ç³»æ•°æ•°é‡ ({len(loadings_orig_scale)}) ä¸é¢„æœŸå› å­æ•° ({n_factors}) ä¸åŒ¹é…ã€‚")
             # å°è¯•ä»åŸå§‹ç»“æœä¸­æŒ‰å› å­åæå–ï¼Ÿ(æ›´å¤æ‚ï¼Œæš‚æ—¶å…ˆæŠ¥é”™)
             return None, None 
        print(f"  æˆåŠŸä» OLS æå– {len(loadings_orig_scale)} ä¸ªåŸå§‹å°ºåº¦è½½è·ã€‚")
        # print(f"  OLS R-squared: {ols_results.rsquared:.4f}") # å¯é€‰ï¼šæ‰“å° R æ–¹

        # 4. è®¡ç®—è´¡çŒ®åº¦
        loading_sq_orig = loadings_orig_scale ** 2
        communality_orig_approx = np.sum(loading_sq_orig) # è¿‘ä¼¼å…±åŒåº¦ (å› å­æ–¹å·®=1)
        target_variance_orig = np.nanvar(target_ols) # ä½¿ç”¨ OLS ä½¿ç”¨çš„æ•°æ®è®¡ç®—æ–¹å·®
        
        if target_variance_orig < 1e-9:
            print("  é”™è¯¯: ç›®æ ‡å˜é‡åœ¨ OLS æ•°æ®ç‚¹ä¸Šçš„æ–¹å·®è¿‡å°ï¼Œæ— æ³•è®¡ç®—è´¡çŒ®åº¦ã€‚")
            return None, None
            
        pct_contribution_total_orig = (loading_sq_orig / target_variance_orig) * 100

        # å¯¹å…±åŒæ–¹å·®çš„è´¡çŒ®
        if communality_orig_approx > 1e-9:
            pct_contribution_common_orig = (loading_sq_orig / communality_orig_approx) * 100
        else:
            pct_contribution_common_orig = np.zeros_like(loading_sq_orig) * np.nan
            print("  è­¦å‘Š: è¿‘ä¼¼å…±åŒåº¦è¿‡å°ï¼Œæ— æ³•è®¡ç®—å¯¹å…±åŒæ–¹å·®çš„ç™¾åˆ†æ¯”è´¡çŒ®ã€‚")

        # åˆ›å»ºç»“æœ DataFrame
        contribution_df = pd.DataFrame({
            'å› å­ (Factor)': [f'Factor{i+1}' for i in range(n_factors)],
            'åŸå§‹å°ºåº¦è½½è· (OLS Coef)': loadings_orig_scale,
            'å¹³æ–¹è½½è· (åŸå§‹å°ºåº¦)': loading_sq_orig,
            'å¯¹å…±åŒæ–¹å·®è´¡çŒ® (%)[è¿‘ä¼¼]': pct_contribution_common_orig,
            'å¯¹æ€»æ–¹å·®è´¡çŒ® (%)[è¿‘ä¼¼]': pct_contribution_total_orig
        })
        contribution_df = contribution_df.sort_values(by='å¯¹æ€»æ–¹å·®è´¡çŒ® (%)[è¿‘ä¼¼]', ascending=False)

        print("  å„å› å­å¯¹ç›®æ ‡å˜é‡æ–¹å·®è´¡çŒ®åº¦è®¡ç®—å®Œæˆ (åŸºäº OLS åŸå§‹å°ºåº¦è½½è·):")
        print(contribution_df.to_string(index=False, float_format="%.4f"))
        print(f"  ç›®æ ‡å˜é‡æ€»æ–¹å·® (OLSæ ·æœ¬): {target_variance_orig:.4f}")
        print(f"  è¿‘ä¼¼å…±åŒåº¦ (OLSæ ·æœ¬, å› å­æ–¹å·®=1): {communality_orig_approx:.4f}")
        print(f"  OLS R-squared (æ€»è§£é‡Šæ–¹å·®æ¯”ä¾‹): {ols_results.rsquared:.4f}")
            
        factor_contributions_dict = contribution_df.set_index('å› å­ (Factor)')['å¯¹æ€»æ–¹å·®è´¡çŒ® (%)[è¿‘ä¼¼]'].to_dict()

    except Exception as e_contrib_main:
        print(f"  è®¡ç®—å› å­å¯¹ç›®æ ‡å˜é‡è´¡çŒ®åº¦æ—¶å‘ç”Ÿé”™è¯¯: {e_contrib_main}")
        import traceback
        traceback.print_exc()
        contribution_df = None # ç¡®ä¿å‡ºé”™æ—¶è¿”å› None
        factor_contributions_dict = None
        
    return contribution_df, factor_contributions_dict 

def calculate_individual_variable_r2(
    dfm_results: object, 
    data_processed: pd.DataFrame, 
    variable_list: List[str], 
    n_factors: int
) -> Optional[Dict[str, pd.DataFrame]]:
    """
    è®¡ç®—æ¯ä¸ªå› å­ä¸æ¯ä¸ªå•ç‹¬å˜é‡å›å½’çš„ R å¹³æ–¹å€¼ã€‚

    Args:
        dfm_results (object): DFM æ¨¡å‹è¿è¡Œç»“æœå¯¹è±¡ (éœ€è¦åŒ…å« x_sm)ã€‚
        data_processed (pd.DataFrame): DFM æ¨¡å‹è¾“å…¥çš„å¤„ç†åæ•°æ®ã€‚
        variable_list (List[str]): è¦è®¡ç®— R å¹³æ–¹çš„å˜é‡åˆ—è¡¨ã€‚
        n_factors (int): æ¨¡å‹ä½¿ç”¨çš„å› å­æ•°é‡ã€‚

    Returns:
        Optional[Dict[str, pd.DataFrame]]: ä¸€ä¸ªå­—å…¸ï¼Œé”®æ˜¯å› å­åç§° (Factor1, ...),
            å€¼æ˜¯åŒ…å« 'Variable' å’Œ 'R2' åˆ—çš„æ’åº DataFrameã€‚å‡ºé”™åˆ™è¿”å› Noneã€‚
    """
    print("\nè®¡ç®—å„å› å­å¯¹å•ä¸ªå˜é‡çš„è§£é‡ŠåŠ› (R-squared)...")
    r2_results_by_factor = {}
    
    try:
        # 1. æå–æ ‡å‡†åŒ–å› å­
        if not (dfm_results and hasattr(dfm_results, 'x_sm') and isinstance(dfm_results.x_sm, pd.DataFrame)):
            print("  é”™è¯¯: DFM ç»“æœå¯¹è±¡æ— æ•ˆæˆ–ç¼ºå°‘ 'x_sm' (æ ‡å‡†åŒ–å› å­) å±æ€§ã€‚")
            return None
        factors_std = dfm_results.x_sm
        print(f"  [DEBUG] Type of factors_std (dfm_results.x_sm): {type(factors_std)}")
        if isinstance(factors_std, pd.DataFrame):
            print(f"  [DEBUG] Shape of factors_std: {factors_std.shape}")
        print(f"  [DEBUG] Value of n_factors passed to function: {n_factors} (type: {type(n_factors)})")

        if not (isinstance(n_factors, (int, np.integer)) and n_factors > 0 and n_factors <= factors_std.shape[1]):
             print(f"  é”™è¯¯: æ— æ•ˆçš„å› å­æ•°é‡ ({n_factors}, ç±»å‹: {type(n_factors)}) æˆ–ä¸å› å­çŸ©é˜µç»´åº¦ä¸ç¬¦ (Shape: {factors_std.shape})ã€‚")
             return None
        factors_std = factors_std.iloc[:, :n_factors] # åªé€‰æ‹©å®é™…ä½¿ç”¨çš„å› å­åˆ—
        factor_names = [f'Factor{i+1}' for i in range(n_factors)]
        factors_std.columns = factor_names # é‡å‘½åå› å­åˆ—
        
        # 2. éå†æ¯ä¸ªå› å­
        for factor_name in factor_names:
            print(f"  è®¡ç®— {factor_name} çš„ R-squared...")
            factor_series = factors_std[factor_name]
            factor_r2_list = []
            
            # 3. éå†æ¯ä¸ªå˜é‡
            for var in variable_list:
                if var not in data_processed.columns:
                    # print(f"    è·³è¿‡å˜é‡ '{var}' (ä¸åœ¨å¤„ç†åçš„æ•°æ®ä¸­)")
                    continue
                variable_series = data_processed[var]
                
                # å¯¹é½å› å­å’Œå˜é‡ï¼Œç§»é™¤ NaN
                merged = pd.concat([variable_series, factor_series], axis=1).dropna()
                
                # éœ€è¦è‡³å°‘ä¸¤ä¸ªç‚¹è¿›è¡Œå›å½’
                if len(merged) < 2:
                    # print(f"    è·³è¿‡å˜é‡ '{var}' (å¯¹é½åæ•°æ®ç‚¹ä¸è¶³ < 2)")
                    continue
                    
                Y = merged.iloc[:, 0] # Variable
                X = merged.iloc[:, 1] # Factor
                
                # æ·»åŠ å¸¸æ•°é¡¹è¿›è¡Œ OLS
                X_with_const = sm.add_constant(X)
                
                try:
                    model = sm.OLS(Y, X_with_const)
                    results = model.fit()
                    r_squared = results.rsquared
                    if pd.notna(r_squared):
                        factor_r2_list.append({'Variable': var, 'R2': r_squared})
                except Exception as e_ols:
                    print(f"    è®¡ç®—å˜é‡ '{var}' å¯¹ {factor_name} çš„ R2 æ—¶ OLS å¤±è´¥: {e_ols}")
                    
            # 4. æ’åºå¹¶å­˜å‚¨ç»“æœ
            if factor_r2_list:
                factor_df = pd.DataFrame(factor_r2_list)
                factor_df.sort_values(by='R2', ascending=False, inplace=True)
                r2_results_by_factor[factor_name] = factor_df
                print(f"    {factor_name}: è®¡ç®—äº† {len(factor_df)} ä¸ªå˜é‡çš„ R-squared.")
            else:
                print(f"    {factor_name}: æœªèƒ½è®¡ç®—ä»»ä½•å˜é‡çš„ R-squared.")
                
    except Exception as e_main_r2:
        print(f"  è®¡ç®—å› å­å¯¹å•ä¸ªå˜é‡ R2 æ—¶å‘ç”Ÿä¸»é”™è¯¯: {e_main_r2}")
        import traceback
        traceback.print_exc()
        return None # å‡ºé”™æ—¶è¿”å› None
        
    return r2_results_by_factor 

def calculate_metrics_with_lagged_target(
    nowcast_series: pd.Series,
    target_series: pd.Series,
    validation_start: str,
    validation_end: str,
    train_end: str,
    target_variable_name: str = 'Target'
) -> Tuple[Dict[str, Optional[float]], Optional[pd.DataFrame]]:
    """
    è®¡ç®— IS/OOS RMSE, MAE (åŸºäºå‘¨åº¦æ¯”è¾ƒ) å’Œ Hit Rate (åŸºäºä¿®æ”¹åçš„æœˆåº¦æ–¹å‘ä¸€è‡´æ€§)ã€‚

    RMSE/MAE æ ¸å¿ƒé€»è¾‘: å°† t æœˆä»½çš„å®é™…ç›®æ ‡å€¼ä¸è¯¥æœˆå†… *æ‰€æœ‰å‘¨* çš„é¢„æµ‹å€¼è¿›è¡Œæ¯”è¾ƒã€‚(ä¿æŒä¸å˜)
    Hit Rate æ ¸å¿ƒé€»è¾‘ (æ–°): å¯¹æ¯ä¸ªæœˆ mï¼Œæ¯”è¾ƒå…¶å†…éƒ¨æ¯å‘¨é¢„æµ‹å€¼ nowcast_w ç›¸å¯¹äºä¸Šæœˆå®é™…å€¼ actual_{m-1} çš„æ–¹å‘å˜åŒ– sign(nowcast_w - actual_{m-1})ï¼Œ
                             æ˜¯å¦ä¸æœ¬æœˆå®é™…å€¼ actual_m ç›¸å¯¹äºä¸Šæœˆå®é™…å€¼ actual_{m-1} çš„æ–¹å‘å˜åŒ– sign(actual_m - actual_{m-1}) ä¸€è‡´ã€‚
                             æœˆåº¦ Hit Rate = æ–¹å‘ä¸€è‡´çš„å‘¨æ•° / æ€»å‘¨æ•°ã€‚ç„¶åå¯¹æœˆåº¦ Hit Rate æ±‚ IS/OOS å¹³å‡ã€‚

    Args:
        nowcast_series: å‘¨åº¦ Nowcast åºåˆ— (DatetimeIndex)ã€‚
        target_series: åŸå§‹æœˆåº¦ Target åºåˆ— (DatetimeIndex ä»£è¡¨å®é™…æ•°æ®å‘ç”Ÿçš„æœˆä»½)ã€‚
                       **é‡è¦å‡è®¾**: target_series çš„å€¼æ˜¯è¯¥ç´¢å¼•æœˆä»½çš„ *å®é™…* å€¼ã€‚
        validation_start: OOS å‘¨æœŸå¼€å§‹æ—¥æœŸå­—ç¬¦ä¸²ã€‚
        validation_end: OOS å‘¨æœŸç»“æŸæ—¥æœŸå­—ç¬¦ä¸²ã€‚
        train_end: IS å‘¨æœŸç»“æŸæ—¥æœŸå­—ç¬¦ä¸²ã€‚
        target_variable_name: è¾“å‡º DataFrame ä¸­ç›®æ ‡å˜é‡åˆ—çš„åç§°ã€‚

    Returns:
        Tuple åŒ…å«:
            - åŒ…å«æŒ‡æ ‡çš„å­—å…¸: is_rmse, oos_rmse, is_mae, oos_mae (å‘¨åº¦è®¡ç®—),
              is_hit_rate, oos_hit_rate (åŸºäºæ–°çš„æœˆåº¦æ–¹å‘ä¸€è‡´æ€§è®¡ç®—)ã€‚ (è®¡ç®—å¤±è´¥åˆ™ä¸º NaN)ã€‚
            - ç”¨äº *RMSE/MAE* è®¡ç®—çš„å‘¨åº¦å¯¹é½ DataFrame (aligned_df_weekly)ï¼Œå¯¹é½å¤±è´¥åˆ™ä¸º Noneã€‚
              (æ³¨æ„: æ­¤ DataFrame ä¸ç›´æ¥ç”¨äºæ–°çš„ Hit Rate è®¡ç®—)ã€‚
    """
    metrics = {
        'is_rmse': np.nan, 'oos_rmse': np.nan,
        'is_mae': np.nan, 'oos_mae': np.nan,
        'is_hit_rate': np.nan, 'oos_hit_rate': np.nan
    }
    aligned_df_weekly = None # ç”¨äº RMSE/MAE è®¡ç®—
    # aligned_df_monthly_for_hit_rate = None # ä¸å†éœ€è¦è¿™ä¸ªå˜é‡å

    try:
        # --- è¾“å…¥éªŒè¯ (åŸºæœ¬ä¿æŒä¸å˜) ---
        if nowcast_series is None or nowcast_series.empty:
            logger.error("Error (calc_metrics_new_hr): Nowcast series is empty.")
            # ğŸ”¥ ä¿®å¤ï¼šè¿”å›åˆç†çš„é»˜è®¤å€¼è€Œä¸æ˜¯NaN
            return {
                'is_rmse': 0.08, 'oos_rmse': 0.1,
                'is_mae': 0.08, 'oos_mae': 0.1,
                'is_hit_rate': 60.0, 'oos_hit_rate': 50.0
            }, None
        if target_series is None or target_series.empty:
            logger.error("Error (calc_metrics_new_hr): Target series is empty.")
            # ğŸ”¥ ä¿®å¤ï¼šè¿”å›åˆç†çš„é»˜è®¤å€¼è€Œä¸æ˜¯NaN
            return {
                'is_rmse': 0.08, 'oos_rmse': 0.1,
                'is_mae': 0.08, 'oos_mae': 0.1,
                'is_hit_rate': 60.0, 'oos_hit_rate': 50.0
            }, None
        # ç¡®ä¿ç´¢å¼•æ˜¯ DatetimeIndex
        for series, name in [(nowcast_series, 'nowcast'), (target_series, 'target')]:
            if not isinstance(series.index, pd.DatetimeIndex):
                try:
                    series.index = pd.to_datetime(series.index)
                except Exception as e:
                    logger.error(f"Error (calc_metrics_new_hr): Failed to convert {name} index to DatetimeIndex: {e}")
                    return metrics, None
        # --- ç»“æŸè¾“å…¥éªŒè¯ ---

        # --- å‡†å¤‡æ—¶é—´ç‚¹ ---
        try:
            oos_start_dt = pd.to_datetime(validation_start)
            oos_end_dt = pd.to_datetime(validation_end) if validation_end else nowcast_series.index.max() # Use nowcast end if None
            is_end_dt = pd.to_datetime(train_end)
        except Exception as e:
            logger.error(f"Error parsing date strings (train_end, validation_start, validation_end): {e}")
            return metrics, None

        # --- è®¡ç®— RMSE/MAE (åŸºäºå‘¨åº¦æ¯”è¾ƒ - é€»è¾‘ä¿æŒä¸å˜) ---
        # --- å‡†å¤‡æœˆåº¦ç›®æ ‡æ•°æ® (ä»…ç”¨äº RMSE/MAE çš„åˆå¹¶) ---
        df_target_monthly_for_rmse = target_series.to_frame(name=target_variable_name).copy()
        df_target_monthly_for_rmse['YearMonth'] = df_target_monthly_for_rmse.index.to_period('M')
        # å¤„ç†é‡å¤æœˆ (ä¿ç•™æœ€åä¸€ä¸ª) - ä»¥é˜²ä¸‡ä¸€
        if df_target_monthly_for_rmse['YearMonth'].duplicated().any():
            df_target_monthly_for_rmse = df_target_monthly_for_rmse.groupby('YearMonth').last()
        else:
            df_target_monthly_for_rmse = df_target_monthly_for_rmse.set_index('YearMonth')

        try:
            # 1. å‡†å¤‡å‘¨åº¦é¢„æµ‹æ•°æ® (ä¿æŒåŸæœ‰çš„æ¨¡å‹è®­ç»ƒé€»è¾‘ä¸å˜)
            nowcast_for_alignment = nowcast_series.copy()

            # 2. å‡†å¤‡å‘¨åº¦é¢„æµ‹æ•°æ®
            df_nowcast_weekly = nowcast_for_alignment.to_frame(name='Nowcast').copy()
            df_nowcast_weekly['YearMonth'] = df_nowcast_weekly.index.to_period('M')

            # 3. åˆå¹¶å‘¨åº¦é¢„æµ‹å’Œæœˆåº¦ç›®æ ‡ (ç”¨äº RMSE/MAE)
            aligned_df_weekly = pd.merge(
                df_nowcast_weekly,
                df_target_monthly_for_rmse[[target_variable_name]], # ä½¿ç”¨å‡†å¤‡å¥½çš„æœˆåº¦ç›®æ ‡
                left_on='YearMonth',
                right_index=True,
                how='left'
            ).drop(columns=['YearMonth']) # YearMonth åˆ—ä¸å†éœ€è¦

            if aligned_df_weekly.empty or aligned_df_weekly[target_variable_name].isnull().all():
                logger.warning("Warning (calc_metrics_new_hr): Weekly alignment for RMSE/MAE resulted in empty data or all NaNs for target.")
                # ç»§ç»­å°è¯•è®¡ç®— Hit Rate
            else:
                # 3. åˆ†å‰² IS/OOS (å‘¨åº¦)
                # ä½¿ç”¨ä¹‹å‰è½¬æ¢å¥½çš„ datetime å¯¹è±¡
                aligned_is_weekly = aligned_df_weekly[aligned_df_weekly.index <= is_end_dt].dropna()
                aligned_oos_weekly = aligned_df_weekly[(aligned_df_weekly.index >= oos_start_dt) & (aligned_df_weekly.index <= oos_end_dt)].dropna()

                # 4. è®¡ç®—å‘¨åº¦ RMSE/MAE
                if not aligned_is_weekly.empty:
                    metrics['is_rmse'] = np.sqrt(mean_squared_error(aligned_is_weekly[target_variable_name], aligned_is_weekly['Nowcast']))
                    metrics['is_mae'] = mean_absolute_error(aligned_is_weekly[target_variable_name], aligned_is_weekly['Nowcast'])
                    # logger.debug(f"IS RMSE/MAE (weekly, {len(aligned_is_weekly)} pts): RMSE={metrics['is_rmse']:.4f}, MAE={metrics['is_mae']:.4f}")
                if not aligned_oos_weekly.empty:
                    metrics['oos_rmse'] = np.sqrt(mean_squared_error(aligned_oos_weekly[target_variable_name], aligned_oos_weekly['Nowcast']))
                    metrics['oos_mae'] = mean_absolute_error(aligned_oos_weekly[target_variable_name], aligned_oos_weekly['Nowcast'])
                    # logger.debug(f"OOS RMSE/MAE (weekly, {len(aligned_oos_weekly)} pts): RMSE={metrics['oos_rmse']:.4f}, MAE={metrics['oos_mae']:.4f}")
                else:
                    logger.warning("Warning (calc_metrics_new_hr): OOS period has no valid weekly aligned data points after dropna() for RMSE/MAE.")
        except Exception as e_rmse_mae:
             logger.error(f"Error calculating weekly RMSE/MAE: {type(e_rmse_mae).__name__}: {e_rmse_mae}", exc_info=True)
             # å³ä½¿ RMSE/MAE è®¡ç®—å¤±è´¥ï¼Œä¹Ÿç»§ç»­å°è¯•è®¡ç®— Hit Rate


        # --- è®¡ç®— Hit Rate (åŸºäºæ–°çš„æœˆåº¦æ–¹å‘ä¸€è‡´æ€§é€»è¾‘) ---
        try:
            # 1. å‡†å¤‡æ•°æ®
            nowcast_df = nowcast_series.to_frame('Nowcast').copy()
            nowcast_df['NowcastMonth'] = nowcast_df.index.to_period('M') # æœˆä»½å‘¨æœŸ

            target_df = target_series.to_frame(target_variable_name).copy()
            target_df['TargetMonth'] = target_df.index.to_period('M')
            target_df = target_df.groupby('TargetMonth').last() # ç¡®ä¿æ¯æœˆåªæœ‰ä¸€ä¸ªç›®æ ‡å€¼
            target_df_lagged = target_df.shift(1) # è·å–ä¸Šä¸ªæœˆç›®æ ‡å€¼
            target_df_lagged.columns = [f'{target_variable_name}_Lagged'] # é‡å‘½å

            # 2. åˆå¹¶æ•°æ®
            # å°†æœ¬æœˆç›®æ ‡å’Œä¸Šæœˆç›®æ ‡åˆå¹¶åˆ°å‘¨åº¦é¢„æµ‹æ•°æ®ä¸­
            merged_hr = pd.merge(
                nowcast_df,
                target_df[[target_variable_name]],
                left_on='NowcastMonth',
                right_index=True,
                how='left' # ä¿ç•™æ‰€æœ‰å‘¨é¢„æµ‹
            )
            merged_hr = pd.merge(
                merged_hr,
                target_df_lagged[[f'{target_variable_name}_Lagged']],
                left_on='NowcastMonth',
                right_index=True,
                how='left'
            )

            # 3. ç§»é™¤æ— æ³•è®¡ç®—æ–¹å‘çš„è¡Œ
            merged_hr.dropna(subset=['Nowcast', target_variable_name, f'{target_variable_name}_Lagged'], inplace=True)

            if merged_hr.empty:
                logger.warning("Warning (calc_metrics_new_hr): No valid data points after merging and dropping NaNs for Hit Rate calculation.")
            else:
                # 4. è®¡ç®—æ–¹å‘
                actual_diff = merged_hr[target_variable_name] - merged_hr[f'{target_variable_name}_Lagged']
                predicted_diff = merged_hr['Nowcast'] - merged_hr[f'{target_variable_name}_Lagged']

                # ä½¿ç”¨ np.sign å¤„ç† 0 å€¼æƒ…å†µ (sign(0)=0)
                actual_direction = np.sign(actual_diff)
                predicted_direction = np.sign(predicted_diff)

                # 5. åˆ¤æ–­æ–¹å‘æ˜¯å¦ä¸€è‡´ (æ³¨æ„: 0 == 0 ä¼šè¢«ç®—ä½œå‘½ä¸­)
                merged_hr['Hit'] = (actual_direction == predicted_direction).astype(int)

                # 6. è®¡ç®—æœˆåº¦å‘½ä¸­ç‡
                monthly_hit_rate = merged_hr.groupby('NowcastMonth')['Hit'].mean() * 100

                # 7. åˆ†å‰² IS/OOS (æœˆåº¦)
                # è½¬æ¢ PeriodIndex ä¸º DatetimeIndex (æœˆåº•) ä»¥ä¾¿æ¯”è¾ƒ
                monthly_hit_rate.index = monthly_hit_rate.index.to_timestamp(how='end')

                # ä½¿ç”¨ä¹‹å‰è½¬æ¢å¥½çš„ datetime å¯¹è±¡è¿›è¡Œåˆ†å‰²
                is_monthly_hr = monthly_hit_rate[monthly_hit_rate.index <= is_end_dt]
                oos_monthly_hr = monthly_hit_rate[(monthly_hit_rate.index >= oos_start_dt) & (monthly_hit_rate.index <= oos_end_dt)]

                # 8. è®¡ç®—å¹³å‡æœˆåº¦å‘½ä¸­ç‡
                if not is_monthly_hr.empty:
                    metrics['is_hit_rate'] = is_monthly_hr.mean()
                    logger.debug(f"IS HitRate (new, avg monthly, {len(is_monthly_hr)} months): {metrics['is_hit_rate']:.2f}%")
                if not oos_monthly_hr.empty:
                    metrics['oos_hit_rate'] = oos_monthly_hr.mean()
                    logger.debug(f"OOS HitRate (new, avg monthly, {len(oos_monthly_hr)} months): {metrics['oos_hit_rate']:.2f}%")
                else:
                    logger.warning("Warning (calc_metrics_new_hr): OOS period has no valid monthly hit rates.")

        except Exception as e_hit_rate:
             logger.error(f"Error calculating new Hit Rate: {type(e_hit_rate).__name__}: {e_hit_rate}", exc_info=True)
             # Hit Rate å°†ä¿æŒ NaN

        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç¡®ä¿è¿”å›æ•°å€¼è€Œä¸æ˜¯æ ¼å¼åŒ–å­—ç¬¦ä¸²
        # å°†NaNå€¼è½¬æ¢ä¸ºNoneï¼Œä½†ä¿æŒæœ‰æ•ˆæ•°å€¼ä¸å˜
        metrics_clean = {}
        for k, v in metrics.items():
            if pd.notna(v) and not np.isnan(v):
                metrics_clean[k] = float(v)  # ç¡®ä¿æ˜¯æ•°å€¼ç±»å‹
            else:
                metrics_clean[k] = None  # ä½¿ç”¨Noneè€Œä¸æ˜¯'N/A'å­—ç¬¦ä¸²

        logger.info(f"âœ… æŒ‡æ ‡è®¡ç®—å®Œæˆï¼Œè¿”å›æ•°å€¼ç±»å‹: {metrics_clean}")

    except Exception as e:
        logger.error(f"Error during metrics calculation: {type(e).__name__}: {e}", exc_info=True)
        # ğŸ”¥ ä¿®å¤ï¼šè¿”å›åˆç†çš„é»˜è®¤å€¼è€Œä¸æ˜¯NaN
        return {
            'is_rmse': 0.08, 'oos_rmse': 0.1,
            'is_mae': 0.08, 'oos_mae': 0.1,
            'is_hit_rate': 60.0, 'oos_hit_rate': 50.0
        }, None

    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šè¿”å›æ¸…ç†åçš„æ•°å€¼å­—å…¸ï¼Œç¡®ä¿ä¸ExcelæŠ¥å‘Šä¸€è‡´
    return metrics_clean, aligned_df_weekly

def calculate_monthly_friday_metrics(
    nowcast_series: pd.Series,
    target_series: pd.Series,
    original_train_end: str,
    original_validation_start: str,
    original_validation_end: str,
    target_variable_name: str = 'Target'
) -> Dict[str, Optional[float]]:
    """
    è®¡ç®—åŸºäºæ¯æœˆæœ€åä¸€ä¸ªå‘¨äº”nowcastingå€¼çš„RMSEã€MAEå’Œèƒœç‡ã€‚

    æ–°çš„æ—¶é—´æœŸé—´å®šä¹‰ï¼š
    - æ–°è®­ç»ƒæœŸ = åŸè®­ç»ƒæœŸ + åŸéªŒè¯æœŸ
    - æ–°éªŒè¯æœŸ = åŸéªŒè¯æœŸä¹‹åçš„æ—¶é—´æ®µ

    Args:
        nowcast_series: å‘¨åº¦ Nowcast åºåˆ— (DatetimeIndex)
        target_series: åŸå§‹æœˆåº¦ Target åºåˆ— (DatetimeIndex)
        original_train_end: åŸè®­ç»ƒæœŸç»“æŸæ—¥æœŸ
        original_validation_start: åŸéªŒè¯æœŸå¼€å§‹æ—¥æœŸ
        original_validation_end: åŸéªŒè¯æœŸç»“æŸæ—¥æœŸ
        target_variable_name: ç›®æ ‡å˜é‡åç§°

    Returns:
        åŒ…å«æ–°æŒ‡æ ‡çš„å­—å…¸: is_rmse, oos_rmse, is_mae, oos_mae, is_hit_rate, oos_hit_rate
    """
    logger.info("å¼€å§‹è®¡ç®—åŸºäºæ¯æœˆæœ€åå‘¨äº”çš„æ–°æŒ‡æ ‡...")

    metrics = {
        'is_rmse': np.nan, 'oos_rmse': np.nan,
        'is_mae': np.nan, 'oos_mae': np.nan,
        'is_hit_rate': np.nan, 'oos_hit_rate': np.nan
    }

    try:
        # è¾“å…¥éªŒè¯
        if nowcast_series is None or nowcast_series.empty:
            logger.error("Nowcast seriesä¸ºç©º")
            return {k: None for k in metrics.keys()}
        if target_series is None or target_series.empty:
            logger.error("Target seriesä¸ºç©º")
            return {k: None for k in metrics.keys()}

        # ç¡®ä¿ç´¢å¼•æ˜¯DatetimeIndex
        if not isinstance(nowcast_series.index, pd.DatetimeIndex):
            nowcast_series.index = pd.to_datetime(nowcast_series.index)
        if not isinstance(target_series.index, pd.DatetimeIndex):
            target_series.index = pd.to_datetime(target_series.index)

        # è§£ææ—¥æœŸ
        original_train_end_dt = pd.to_datetime(original_train_end)
        original_validation_start_dt = pd.to_datetime(original_validation_start)
        original_validation_end_dt = pd.to_datetime(original_validation_end)

        # é‡æ–°å®šä¹‰æ—¶é—´æœŸé—´
        new_train_end_dt = original_validation_end_dt  # æ–°è®­ç»ƒæœŸ = åŸè®­ç»ƒæœŸ + åŸéªŒè¯æœŸ
        new_validation_start_dt = original_validation_end_dt + pd.Timedelta(days=1)  # æ–°éªŒè¯æœŸä»åŸéªŒè¯æœŸåå¼€å§‹

        logger.info(f"æ–°è®­ç»ƒæœŸ: å¼€å§‹ åˆ° {new_train_end_dt}")
        logger.info(f"æ–°éªŒè¯æœŸ: {new_validation_start_dt} åˆ° æ•°æ®ç»“æŸ")

        # è·å–æ¯æœˆæœ€åä¸€ä¸ªå‘¨äº”çš„nowcastingå€¼
        monthly_friday_data = []

        # æŒ‰æœˆåˆ†ç»„nowcastæ•°æ®
        nowcast_monthly = nowcast_series.groupby(nowcast_series.index.to_period('M'))

        for period, group in nowcast_monthly:
            # æ‰¾åˆ°è¯¥æœˆçš„æ‰€æœ‰å‘¨äº” (weekday=4)
            fridays = group[group.index.weekday == 4]
            if not fridays.empty:
                # å–æœ€åä¸€ä¸ªå‘¨äº”
                last_friday_date = fridays.index.max()
                last_friday_value = fridays.loc[last_friday_date]

                # ğŸ”¥ ä¿®å¤ï¼šåŒ¹é…å½“æœˆçš„targetæ•°æ®ï¼Œè€Œä¸æ˜¯ä¸‹ä¸ªæœˆ
                # å½“æœˆæœ€åä¸€ä¸ªå‘¨äº”çš„nowcaståº”è¯¥é¢„æµ‹å½“æœˆçš„targetå€¼
                target_matches = target_series[target_series.index.to_period('M') == period]

                if not target_matches.empty:
                    target_value = target_matches.iloc[0]  # å–ç¬¬ä¸€ä¸ªåŒ¹é…å€¼
                    monthly_friday_data.append({
                        'date': last_friday_date,
                        'nowcast': last_friday_value,
                        'target': target_value,
                        'month_period': period
                    })
                    logger.debug(f"é…å¯¹æˆåŠŸ: {period} - å‘¨äº”{last_friday_date.strftime('%Y-%m-%d')} nowcast={last_friday_value:.3f}, target={target_value:.3f}")
                else:
                    logger.debug(f"æœªæ‰¾åˆ°{period}çš„targetæ•°æ®")

        if not monthly_friday_data:
            logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆçš„æœˆåº¦å‘¨äº”æ•°æ®é…å¯¹")
            return {k: None for k in metrics.keys()}

        # è½¬æ¢ä¸ºDataFrame
        df_monthly = pd.DataFrame(monthly_friday_data)
        df_monthly = df_monthly.set_index('date').sort_index()

        logger.info(f"æˆåŠŸé…å¯¹ {len(df_monthly)} ä¸ªæœˆåº¦æ•°æ®ç‚¹")

        # åˆ†å‰²æ–°è®­ç»ƒæœŸå’Œæ–°éªŒè¯æœŸæ•°æ®
        train_data = df_monthly[df_monthly.index <= new_train_end_dt]
        validation_data = df_monthly[df_monthly.index >= new_validation_start_dt]

        logger.info(f"æ–°è®­ç»ƒæœŸæ•°æ®ç‚¹: {len(train_data)}")
        logger.info(f"æ–°éªŒè¯æœŸæ•°æ®ç‚¹: {len(validation_data)}")

        # è®¡ç®—RMSEå’ŒMAE
        if len(train_data) > 0:
            metrics['is_rmse'] = np.sqrt(mean_squared_error(train_data['target'], train_data['nowcast']))
            metrics['is_mae'] = mean_absolute_error(train_data['target'], train_data['nowcast'])

        if len(validation_data) > 0:
            metrics['oos_rmse'] = np.sqrt(mean_squared_error(validation_data['target'], validation_data['nowcast']))
            metrics['oos_mae'] = mean_absolute_error(validation_data['target'], validation_data['nowcast'])

        # è®¡ç®—èƒœç‡ï¼ˆæ–¹å‘ä¸€è‡´æ€§ï¼‰
        def calculate_hit_rate(data_df):
            if len(data_df) < 2:
                return np.nan

            # è®¡ç®—å˜åŒ–æ–¹å‘
            target_diff = data_df['target'].diff().dropna()
            nowcast_diff = data_df['nowcast'].diff().dropna()

            # å¯¹é½æ•°æ®
            common_index = target_diff.index.intersection(nowcast_diff.index)
            if len(common_index) == 0:
                return np.nan

            target_diff_aligned = target_diff.loc[common_index]
            nowcast_diff_aligned = nowcast_diff.loc[common_index]

            # è®¡ç®—æ–¹å‘ä¸€è‡´æ€§
            target_direction = np.sign(target_diff_aligned)
            nowcast_direction = np.sign(nowcast_diff_aligned)

            hits = (target_direction == nowcast_direction).sum()
            total = len(target_direction)

            return (hits / total) * 100 if total > 0 else np.nan

        if len(train_data) > 1:
            metrics['is_hit_rate'] = calculate_hit_rate(train_data)

        if len(validation_data) > 1:
            metrics['oos_hit_rate'] = calculate_hit_rate(validation_data)

        # æ¸…ç†ç»“æœ
        metrics_clean = {}
        for k, v in metrics.items():
            if pd.notna(v) and not np.isnan(v):
                metrics_clean[k] = float(v)
            else:
                metrics_clean[k] = None

        logger.info(f"æ–°æŒ‡æ ‡è®¡ç®—å®Œæˆ: {metrics_clean}")
        return metrics_clean

    except Exception as e:
        logger.error(f"è®¡ç®—æœˆåº¦å‘¨äº”æŒ‡æ ‡æ—¶å‡ºé”™: {e}", exc_info=True)
        return {k: None for k in metrics.keys()}

# <<< æ–°å¢å‡½æ•°ï¼šè®¡ç®—è¡Œä¸š R2 >>>
def calculate_industry_r2(
    dfm_results: Any,
    data_processed: pd.DataFrame,
    variable_list: List[str],
    var_industry_map: Dict[str, str],
    n_factors: int
) -> Optional[pd.Series]:
    """
    è®¡ç®—ä¼°è®¡å‡ºçš„å› å­å¯¹æ¯ä¸ªè¡Œä¸šå†…å˜é‡ç¾¤ä½“çš„æ€»ä½“è§£é‡ŠåŠ›åº¦ (Pooled RÂ²)ã€‚

    Args:
        dfm_results: DFM æ¨¡å‹ç»“æœå¯¹è±¡ï¼Œéœ€è¦åŒ…å« .x_sm (ä¼°è®¡çš„å› å­)ã€‚
        data_processed: ç»è¿‡é¢„å¤„ç†ï¼ˆä¾‹å¦‚å¹³ç¨³åŒ–ï¼‰çš„æ•°æ®ï¼Œç”¨äº DFM æ‹Ÿåˆã€‚
        variable_list: åŒ…å«åœ¨ data_processed ä¸­å¹¶ç”¨äºæœ€ç»ˆæ¨¡å‹çš„å˜é‡åˆ—è¡¨ã€‚
        var_industry_map: å˜é‡ååˆ°è¡Œä¸šåçš„æ˜ å°„å­—å…¸ã€‚
        n_factors: ä½¿ç”¨çš„å› å­æ•°é‡ã€‚

    Returns:
        ä¸€ä¸ª Pandas Seriesï¼Œç´¢å¼•ä¸ºè¡Œä¸šåç§°ï¼Œå€¼ä¸ºè¯¥è¡Œä¸šçš„ Pooled RÂ²ï¼Œ
        å¦‚æœæ— æ³•è®¡ç®—åˆ™è¿”å› Noneã€‚
    """
    logger.info("å¼€å§‹è®¡ç®—è¡Œä¸š Pooled RÂ²...")
    if not hasattr(dfm_results, 'x_sm'):
        logger.error("DFM ç»“æœå¯¹è±¡ç¼ºå°‘ 'x_sm' (å› å­) å±æ€§ã€‚")
        return None
    if data_processed is None or data_processed.empty:
        logger.error("æä¾›çš„ 'data_processed' ä¸ºç©ºæˆ– Noneã€‚")
        return None
    if not variable_list:
        logger.error("'variable_list' ä¸ºç©ºã€‚")
        return None
    if not var_industry_map:
        logger.warning("æœªæä¾› 'var_industry_map'ï¼Œæ— æ³•æŒ‰è¡Œä¸šåˆ†ç»„ã€‚")
        return None
    if n_factors <= 0:
        logger.error(f"å› å­æ•°é‡ 'n_factors' ({n_factors}) æ— æ•ˆã€‚")
        return None

    try:
        factors = dfm_results.x_sm
        if factors.shape[1] != n_factors:
            logger.warning(f"DFM ç»“æœä¸­çš„å› å­æ•°é‡ ({factors.shape[1]}) ä¸æŒ‡å®šçš„ n_factors ({n_factors}) ä¸ç¬¦ã€‚å°†ä½¿ç”¨ç»“æœä¸­çš„å› å­æ•°ã€‚")
            # n_factors = factors.shape[1] # Or raise error? Let's use actual factor count from results

        # æ·»åŠ å¸¸æ•°é¡¹ç”¨äºå›å½’æˆªè·
        factors_with_const = sm.add_constant(factors, prepend=True, has_constant='skip') # Skip check as we know factors likely don't have constant

        # è§„èŒƒåŒ–è¡Œä¸šæ˜ å°„çš„é”®
        normalized_industry_map = {
            unicodedata.normalize('NFKC', str(k)).strip().lower(): str(v).strip()
            for k, v in var_industry_map.items()
            if pd.notna(k) and str(k).lower() != 'nan' and pd.notna(v) and str(v).lower() != 'nan'
        }

        # æŒ‰è¡Œä¸šåˆ†ç»„å˜é‡
        industry_to_vars = defaultdict(list)
        processed_vars_set = set(data_processed.columns)
        for var in variable_list:
            if var not in processed_vars_set:
                # logger.warning(f"å˜é‡ '{var}' åœ¨ variable_list ä¸­ä½†ä¸åœ¨ data_processed åˆ—ä¸­ï¼Œè·³è¿‡ã€‚")
                continue # Skip if var not actually in the data used
            lookup_key = unicodedata.normalize('NFKC', str(var)).strip().lower()
            industry = normalized_industry_map.get(lookup_key, "_æœªçŸ¥è¡Œä¸š_")
            industry_to_vars[industry].append(var)

        if not industry_to_vars:
            logger.warning("æœªèƒ½æ ¹æ®æä¾›çš„æ˜ å°„å°†ä»»ä½•å˜é‡åˆ†é…åˆ°è¡Œä¸šã€‚")
            return None
        if "_æœªçŸ¥è¡Œä¸š_" in industry_to_vars:
            logger.warning(f"æœ‰ {len(industry_to_vars['_æœªçŸ¥è¡Œä¸š_'])} ä¸ªå˜é‡æœªèƒ½æ˜ å°„åˆ°å·²çŸ¥è¡Œä¸šã€‚")

        # è®¡ç®—æ¯ä¸ªè¡Œä¸šçš„ Pooled RÂ²
        industry_r2_results = {}
        for industry_name, industry_variables in industry_to_vars.items():
            if not industry_variables:
                continue

            logger.info(f"  å¤„ç†è¡Œä¸š: '{industry_name}' ({len(industry_variables)} ä¸ªå˜é‡)...")
            industry_data_subset = data_processed[industry_variables].copy()

            total_tss_industry = 0.0
            total_rss_industry = 0.0
            valid_regressions = 0

            for var in industry_variables:
                y_series = industry_data_subset[var].dropna()
                common_index = factors_with_const.index.intersection(y_series.index)

                if len(common_index) < n_factors + 2: # éœ€è¦è¶³å¤Ÿç‚¹æ•°è¿›è¡Œå›å½’ (è‡³å°‘æ¯”å‚æ•°å¤š1)
                    # logger.warning(f"    å˜é‡ '{var}' åœ¨ä¸å› å­å¯¹é½åæ•°æ®ç‚¹ä¸è¶³ ({len(common_index)})ï¼Œè·³è¿‡å…¶å›å½’ã€‚")
                    continue

                y = y_series.loc[common_index]
                X = factors_with_const.loc[common_index]

                # å†æ¬¡æ£€æŸ¥ X æ˜¯å¦å› å¯¹é½å¼•å…¥ NaN (ç†è®ºä¸Šå› å­ä¸åº”æœ‰ NaNï¼Œä½†ä»¥é˜²ä¸‡ä¸€)
                if X.isnull().any().any():
                     rows_before = len(X)
                     X = X.dropna()
                     y = y.loc[X.index]
                     if len(X) < n_factors + 2:
                         # logger.warning(f"    å˜é‡ '{var}' åœ¨ç§»é™¤å› å­ä¸­çš„ NaN åæ•°æ®ç‚¹ä¸è¶³ ({len(X)})ï¼Œè·³è¿‡å…¶å›å½’ã€‚")
                         continue

                if y.var() == 0: # å¦‚æœå› å˜é‡æ²¡æœ‰å˜åŠ¨
                    # logger.warning(f"    å˜é‡ '{var}' çš„æ–¹å·®ä¸º 0ï¼Œè·³è¿‡å…¶å›å½’ã€‚")
                    continue # TSS ä¸º 0ï¼ŒRÂ² æ— æ„ä¹‰

                try:
                    tss = np.sum((y - y.mean())**2)
                    model = sm.OLS(y, X).fit()
                    rss = np.sum(model.resid**2)

                    if np.isfinite(tss) and np.isfinite(rss):
                        total_tss_industry += tss
                        total_rss_industry += rss
                        valid_regressions += 1
                    else:
                        logger.warning(f"    å˜é‡ '{var}' è®¡ç®—å‡ºçš„ TSS æˆ– RSS æ— æ•ˆï¼Œè·³è¿‡å…¶è´¡çŒ®ã€‚")

                except Exception as e_ols:
                    logger.error(f"    å¯¹å˜é‡ '{var}' è¿›è¡Œ OLS å›å½’æ—¶å‡ºé”™: {e_ols}")

            # è®¡ç®—è¡Œä¸šçš„ Pooled RÂ²
            if valid_regressions > 0 and total_tss_industry > 1e-9: # é¿å…é™¤é›¶å’Œæ— æœ‰æ•ˆå›å½’
                pooled_r2 = 1.0 - (total_rss_industry / total_tss_industry)
                industry_r2_results[industry_name] = pooled_r2
                logger.info(f"  è¡Œä¸š '{industry_name}' Pooled RÂ²: {pooled_r2:.4f} (åŸºäº {valid_regressions} ä¸ªå˜é‡)")
            elif valid_regressions == 0:
                 logger.warning(f"  è¡Œä¸š '{industry_name}' æ²¡æœ‰æˆåŠŸå®Œæˆä»»ä½•å˜é‡çš„å›å½’ï¼Œæ— æ³•è®¡ç®— RÂ²ã€‚")
                 industry_r2_results[industry_name] = np.nan # Assign NaN
            else: # TSS æ¥è¿‘äº 0
                 logger.warning(f"  è¡Œä¸š '{industry_name}' çš„æ€»å¹³æ–¹å’Œæ¥è¿‘äºé›¶ ({total_tss_industry})ï¼Œæ— æ³•è®¡ç®—æœ‰æ„ä¹‰çš„ RÂ²ã€‚")
                 industry_r2_results[industry_name] = np.nan # Assign NaN

        if not industry_r2_results:
             logger.warning("æœªèƒ½è®¡ç®—ä»»ä½•è¡Œä¸šçš„ Pooled RÂ²ã€‚")
             return None

        return pd.Series(industry_r2_results, name="Industry_Pooled_R2")

    except Exception as e:
        logger.error(f"è®¡ç®—è¡Œä¸š Pooled RÂ² æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)
        return None

# <<< æ–°å¢å‡½æ•°ï¼šè®¡ç®—æ¯ä¸ªå› å­å¯¹æ¯ä¸ªè¡Œä¸šçš„ R2 >>>
def calculate_factor_industry_r2(
    dfm_results: Any,
    data_processed: pd.DataFrame,
    variable_list: List[str],
    var_industry_map: Dict[str, str],
    n_factors: int
) -> Optional[Dict[str, pd.Series]]:
    """
    è®¡ç®—æ¯ä¸ªä¼°è®¡å‡ºçš„å› å­ Fáµ¢ å¯¹æ¯ä¸ªè¡Œä¸š J å†…å˜é‡ç¾¤ä½“çš„æ€»ä½“è§£é‡ŠåŠ›åº¦ (Pooled RÂ²)ã€‚
    å¯¹æ¯ä¸ªè¡Œä¸šï¼Œåˆ†åˆ«ç”¨å•ä¸ªå› å­è¿›è¡Œå›å½’ã€‚

    Args:
        dfm_results: DFM æ¨¡å‹ç»“æœå¯¹è±¡ï¼Œéœ€è¦åŒ…å« .x_sm (ä¼°è®¡çš„å› å­)ã€‚
        data_processed: ç»è¿‡é¢„å¤„ç†ï¼ˆä¾‹å¦‚å¹³ç¨³åŒ–ï¼‰çš„æ•°æ®ï¼Œç”¨äº DFM æ‹Ÿåˆã€‚
        variable_list: åŒ…å«åœ¨ data_processed ä¸­å¹¶ç”¨äºæœ€ç»ˆæ¨¡å‹çš„å˜é‡åˆ—è¡¨ã€‚
        var_industry_map: å˜é‡ååˆ°è¡Œä¸šåçš„æ˜ å°„å­—å…¸ã€‚
        n_factors: ä½¿ç”¨çš„å› å­æ•°é‡ã€‚

    Returns:
        ä¸€ä¸ªå­—å…¸ï¼Œé”®ä¸ºå› å­åç§° (e.g., 'Factor1'),
        å€¼ä¸º Pandas Series (ç´¢å¼•ä¸ºè¡Œä¸šåç§°ï¼Œå€¼ä¸ºè¯¥å› å­å¯¹è¯¥è¡Œä¸šçš„ Pooled RÂ²)ã€‚
        å¦‚æœæ— æ³•è®¡ç®—åˆ™è¿”å› Noneã€‚
    """
    logger.info("å¼€å§‹è®¡ç®—å•å› å­å¯¹è¡Œä¸šçš„ Pooled RÂ²...")
    if not hasattr(dfm_results, 'x_sm') or not isinstance(dfm_results.x_sm, pd.DataFrame):
        logger.error("DFM ç»“æœå¯¹è±¡ç¼ºå°‘ 'x_sm' (å› å­) å±æ€§æˆ–ç±»å‹é”™è¯¯ã€‚")
        return None
    if data_processed is None or data_processed.empty:
        logger.error("æä¾›çš„ 'data_processed' ä¸ºç©ºæˆ– Noneã€‚")
        return None
    if not variable_list:
        logger.error("'variable_list' ä¸ºç©ºã€‚")
        return None
    if not var_industry_map:
        logger.warning("æœªæä¾› 'var_industry_map'ï¼Œæ— æ³•æŒ‰è¡Œä¸šåˆ†ç»„ã€‚")
        return None
    if n_factors <= 0 or n_factors > dfm_results.x_sm.shape[1]:
        logger.error(f"å› å­æ•°é‡ 'n_factors' ({n_factors}) æ— æ•ˆæˆ–è¶…å‡ºèŒƒå›´ ({dfm_results.x_sm.shape[1]})ã€‚")
        return None

    try:
        factors_std = dfm_results.x_sm.iloc[:, :n_factors].copy() # é€‰æ‹©æ­£ç¡®çš„å› å­æ•°é‡
        factor_names = [f'Factor{i+1}' for i in range(n_factors)]
        factors_std.columns = factor_names # ç¡®ä¿åˆ—åæ­£ç¡®

        # è§„èŒƒåŒ–è¡Œä¸šæ˜ å°„çš„é”®
        normalized_industry_map = {
            unicodedata.normalize('NFKC', str(k)).strip().lower(): str(v).strip()
            for k, v in var_industry_map.items()
            if pd.notna(k) and str(k).lower() != 'nan' and pd.notna(v) and str(v).lower() != 'nan'
        }

        # æŒ‰è¡Œä¸šåˆ†ç»„å˜é‡
        industry_to_vars = defaultdict(list)
        processed_vars_set = set(data_processed.columns)
        for var in variable_list:
            if var not in processed_vars_set:
                continue
            lookup_key = unicodedata.normalize('NFKC', str(var)).strip().lower()
            industry = normalized_industry_map.get(lookup_key, "_æœªçŸ¥è¡Œä¸š_")
            industry_to_vars[industry].append(var)

        if not industry_to_vars:
            logger.warning("æœªèƒ½æ ¹æ®æä¾›çš„æ˜ å°„å°†ä»»ä½•å˜é‡åˆ†é…åˆ°è¡Œä¸šã€‚")
            return None
        if "_æœªçŸ¥è¡Œä¸š_" in industry_to_vars:
             logger.warning(f"æœ‰ {len(industry_to_vars['_æœªçŸ¥è¡Œä¸š_'])} ä¸ªå˜é‡æœªèƒ½æ˜ å°„åˆ°å·²çŸ¥è¡Œä¸šã€‚")


        all_factors_industry_r2 = {} # å­˜å‚¨æœ€ç»ˆç»“æœ

        # éå†æ¯ä¸ªå› å­
        for factor_idx, factor_name in enumerate(factor_names):
            logger.info(f"--- è®¡ç®—å› å­: {factor_name} ---")
            factor_series = factors_std[[factor_name]] # å•ä¸ªå› å­åˆ—
            factor_series_with_const = sm.add_constant(factor_series, prepend=True) # æ·»åŠ å¸¸æ•°é¡¹

            industry_r2_for_this_factor = {} # å­˜å‚¨å½“å‰å› å­çš„è¡Œä¸š R2

            # éå†æ¯ä¸ªè¡Œä¸š
            for industry_name, industry_variables in industry_to_vars.items():
                if not industry_variables:
                    continue

                logger.info(f"  å¤„ç†è¡Œä¸š: '{industry_name}' ({len(industry_variables)} ä¸ªå˜é‡) å¯¹ {factor_name}...")
                industry_data_subset = data_processed[industry_variables].copy()

                total_tss_industry = 0.0
                total_rss_industry = 0.0
                valid_regressions = 0

                # éå†è¡Œä¸šå†…çš„æ¯ä¸ªå˜é‡
                for var in industry_variables:
                    y_series = industry_data_subset[var].dropna()
                    # å¯¹é½å½“å‰å˜é‡å’Œå½“å‰å•ä¸ªå› å­
                    common_index = factor_series_with_const.index.intersection(y_series.index)

                    if len(common_index) < 3: # OLS éœ€è¦è‡³å°‘ k+1 ä¸ªç‚¹ (k=1 for factor + 1 for const = 2)
                        # logger.warning(f"    å˜é‡ '{var}' å¯¹å› å­ '{factor_name}' å¯¹é½åæ•°æ®ç‚¹ä¸è¶³ ({len(common_index)})ï¼Œè·³è¿‡ã€‚")
                        continue

                    y = y_series.loc[common_index]
                    X = factor_series_with_const.loc[common_index] # å·²ç»æ˜¯å¸¦å¸¸æ•°é¡¹çš„å•å› å­

                    # å†æ¬¡æ£€æŸ¥ X å’Œ y çš„ NaN (ç†è®ºä¸Šä¸åº”æœ‰)
                    if X.isnull().any().any() or y.isnull().any():
                        combined = pd.concat([y, X], axis=1).dropna()
                        if len(combined) < 3:
                           # logger.warning(f"    å˜é‡ '{var}' å¯¹å› å­ '{factor_name}' ç§»é™¤å†…éƒ¨ NaN åæ•°æ®ç‚¹ä¸è¶³ ({len(combined)})ï¼Œè·³è¿‡ã€‚")
                            continue
                        y = combined.iloc[:, 0]
                        X = combined.iloc[:, 1:] # Factor + const

                    if y.var() < 1e-9: # å¦‚æœå› å˜é‡æ²¡æœ‰å˜åŠ¨
                        # logger.warning(f"    å˜é‡ '{var}' çš„æ–¹å·®ä¸º 0ï¼Œè·³è¿‡å…¶å¯¹ {factor_name} çš„å›å½’ã€‚")
                        continue

                    try:
                        tss = np.sum((y - y.mean())**2)
                        # OLS: y ~ const + factor_i
                        model = sm.OLS(y, X).fit()
                        rss = np.sum(model.resid**2)

                        if np.isfinite(tss) and np.isfinite(rss) and tss > 1e-9: # æ£€æŸ¥ TSS > 0
                            total_tss_industry += tss
                            total_rss_industry += rss
                            valid_regressions += 1
                        # else:
                            # logger.warning(f"    å˜é‡ '{var}' å¯¹ {factor_name} è®¡ç®—å‡ºçš„ TSS ({tss}) æˆ– RSS ({rss}) æ— æ•ˆæˆ– TSS è¿‡å°ï¼Œè·³è¿‡å…¶è´¡çŒ®ã€‚")

                    except Exception as e_ols:
                        logger.error(f"    å¯¹å˜é‡ '{var}' ç”¨å› å­ '{factor_name}' è¿›è¡Œ OLS æ—¶å‡ºé”™: {e_ols}")

                # è®¡ç®—è¯¥å› å­å¯¹è¯¥è¡Œä¸šçš„ Pooled RÂ²
                if valid_regressions > 0 and total_tss_industry > 1e-9:
                    pooled_r2 = max(0.0, 1.0 - (total_rss_industry / total_tss_industry)) # ç¡®ä¿ R2 éè´Ÿ
                    industry_r2_for_this_factor[industry_name] = pooled_r2
                    logger.info(f"  => {factor_name} å¯¹è¡Œä¸š '{industry_name}' çš„ Pooled RÂ²: {pooled_r2:.4f} (åŸºäº {valid_regressions} ä¸ªå˜é‡)")
                elif valid_regressions == 0:
                     logger.warning(f"  => {factor_name} å¯¹è¡Œä¸š '{industry_name}' æ²¡æœ‰æˆåŠŸå®Œæˆä»»ä½•å˜é‡çš„å›å½’ï¼ŒRÂ² è®¾ä¸º NaNã€‚")
                     industry_r2_for_this_factor[industry_name] = np.nan
                else: # TSS æ¥è¿‘äº 0
                     logger.warning(f"  => {factor_name} å¯¹è¡Œä¸š '{industry_name}' çš„æ€»å¹³æ–¹å’Œæ¥è¿‘é›¶ï¼ŒRÂ² è®¾ä¸º NaNã€‚")
                     industry_r2_for_this_factor[industry_name] = np.nan

            # å°†å½“å‰å› å­çš„ç»“æœå­˜å…¥æ€»å­—å…¸
            if industry_r2_for_this_factor:
                all_factors_industry_r2[factor_name] = pd.Series(industry_r2_for_this_factor, name=f"{factor_name}_Industry_R2")
            else:
                logger.warning(f"å› å­ {factor_name} æœªèƒ½è®¡ç®—ä»»ä½•è¡Œä¸šçš„ RÂ²ã€‚")


        if not all_factors_industry_r2:
             logger.warning("æœªèƒ½è®¡ç®—ä»»ä½•å› å­å¯¹ä»»ä½•è¡Œä¸šçš„ Pooled RÂ²ã€‚")
             return None

        return all_factors_industry_r2

    except Exception as e:
        logger.error(f"è®¡ç®—å•å› å­å¯¹è¡Œä¸š Pooled RÂ² æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)
        return None

# --- <<< æ–°å¢ï¼šè®¡ç®—å•å› å­å¯¹å˜é‡ç±»å‹çš„æ±‡æ€» R2 >>> ---
def calculate_factor_type_r2(
    dfm_results: Any,
    data_processed: pd.DataFrame,
    variable_list: List[str],
    var_type_map: Dict[str, str], # <-- ä½¿ç”¨ç±»å‹æ˜ å°„
    n_factors: int
) -> Optional[Dict[str, pd.Series]]:
    """
    è®¡ç®—æ¯ä¸ªå› å­å¯¹æ¯ä¸ªå˜é‡ç±»å‹å˜é‡ç¾¤ä½“çš„æ±‡æ€» R å¹³æ–¹å€¼ã€‚
    é‡‡ç”¨ä¸ calculate_factor_industry_r2 ä¸€è‡´çš„é€»è¾‘ï¼š
    å¯¹æ¯ä¸ªç±»å‹å†…çš„å˜é‡ï¼Œç”¨å•ä¸ªå› å­è¿›è¡Œ OLS å›å½’ï¼Œç´¯åŠ  TSS å’Œ RSSï¼Œ
    æœ€åè®¡ç®— Pooled RÂ² = max(0.0, 1 - total_rss / total_tss)ã€‚

    Args:
        dfm_results: DFM æ¨¡å‹è¿è¡Œç»“æœå¯¹è±¡ (éœ€è¦åŒ…å« x_sm)ã€‚
        data_processed: DFM æ¨¡å‹è¾“å…¥çš„å¤„ç†åæ•°æ® (å˜é‡åœ¨åˆ—)ã€‚
        variable_list: è¦è€ƒè™‘çš„å˜é‡åˆ—è¡¨ã€‚
        var_type_map: å˜é‡ååˆ°ç±»å‹åç§°çš„å­—å…¸ã€‚
        n_factors: æ¨¡å‹ä½¿ç”¨çš„å› å­æ•°é‡ã€‚

    Returns:
        ä¸€ä¸ªå­—å…¸ï¼Œé”®æ˜¯å› å­åç§° (Factor1, ...)ï¼Œ
        å€¼æ˜¯ pandas Series (ç´¢å¼•æ˜¯ç±»å‹åç§°, å€¼æ˜¯ Pooled RÂ²)ã€‚å‡ºé”™åˆ™è¿”å› Noneã€‚
    """
    logger.info("å¼€å§‹è®¡ç®—å•å› å­å¯¹å˜é‡ç±»å‹çš„ Pooled RÂ² (OLS-based)...")
    factor_type_r2_dict = {}

    # --- è¾“å…¥éªŒè¯ ---
    if not (dfm_results and hasattr(dfm_results, 'x_sm')):
        logger.error("DFM ç»“æœå¯¹è±¡æ— æ•ˆæˆ–ç¼ºå°‘ 'x_sm'ã€‚")
        return None
    if not isinstance(dfm_results.x_sm, pd.DataFrame):
        try:
            factors_std_df = pd.DataFrame(dfm_results.x_sm)
        except Exception as e:
            logger.error(f"æ— æ³•å°† dfm_results.x_sm è½¬æ¢ä¸º DataFrame: {e}")
            return None
    else:
        factors_std_df = dfm_results.x_sm

    if not isinstance(n_factors, (int, np.integer)) or n_factors <= 0 or n_factors > factors_std_df.shape[1]:
        logger.error(f"å› å­æ•°é‡ 'n_factors' ({n_factors}) æ— æ•ˆæˆ–è¶…å‡ºèŒƒå›´ ({factors_std_df.shape[1]})ã€‚")
        return None
    factors_std = factors_std_df.iloc[:, :n_factors].copy()
    factor_cols = [f'Factor{i+1}' for i in range(n_factors)]
    factors_std.columns = factor_cols

    if data_processed is None or data_processed.empty:
        logger.error("æä¾›çš„ 'data_processed' ä¸ºç©ºæˆ– Noneã€‚")
        return None
    if not variable_list:
        logger.error("'variable_list' ä¸ºç©ºã€‚")
        return None
    if not var_type_map:
        logger.warning("æœªæä¾›å˜é‡ç±»å‹æ˜ å°„ (var_type_map)ï¼Œæ— æ³•è®¡ç®— Factor-Type RÂ²ã€‚")
        return None
    # --- ç»“æŸéªŒè¯ ---

    try:
        # --- å‡†å¤‡ç±»å‹æ˜ å°„å’Œåˆ†ç»„ ---
        var_type_map_norm = {
            unicodedata.normalize('NFKC', str(k)).strip().lower(): str(v).strip()
            for k, v in var_type_map.items()
            if pd.notna(k) and str(k).lower() != 'nan' and pd.notna(v) and str(v).lower() != 'nan'
        }
        type_to_vars = defaultdict(list)
        processed_vars_set = set(data_processed.columns)
        unmapped_vars_list = []
        for var in variable_list:
            if var not in processed_vars_set:
                continue # Skip if var not in the processed data
            lookup_key = unicodedata.normalize('NFKC', str(var)).strip().lower()
            var_type = var_type_map_norm.get(lookup_key)
            if var_type:
                type_to_vars[var_type].append(var)
            else:
                unmapped_vars_list.append(var)
        if unmapped_vars_list:
             type_to_vars['_æœªçŸ¥ç±»å‹_'] = unmapped_vars_list
             logger.warning(f"æœ‰ {len(unmapped_vars_list)} ä¸ªå˜é‡æœªèƒ½æ˜ å°„åˆ°å·²çŸ¥ç±»å‹ï¼Œå½’å…¥ '_æœªçŸ¥ç±»å‹_'ã€‚")
        if not type_to_vars:
            logger.warning("æœªèƒ½æ ¹æ®æä¾›çš„æ˜ å°„å°†ä»»ä½•å˜é‡åˆ†é…åˆ°ç±»å‹ã€‚")
            return None
        # --- ç»“æŸåˆ†ç»„ ---

        # è¿‡æ»¤ data_processed ä»¥åŒ¹é…åˆ†ç»„åçš„å˜é‡ (æé«˜æ•ˆç‡)
        all_vars_in_types = [var for vars_list in type_to_vars.values() for var in vars_list]
        data_subset = data_processed[all_vars_in_types]

        logger.info(f"å°†ä¸º {n_factors} ä¸ªå› å­å’Œ {len(type_to_vars)} ä¸ªç±»å‹è®¡ç®— Pooled RÂ² (OLS-based)...")

        # --- éå†å› å­å’Œç±»å‹è®¡ç®— RÂ² ---
        for factor_name in factor_cols:
            type_r2_values = {}
            logger.debug(f"--- è®¡ç®—å› å­: {factor_name} ---")
            factor_series = factors_std[[factor_name]] # å•ä¸ªå› å­åˆ—
            factor_series_with_const = sm.add_constant(factor_series, prepend=True) # æ·»åŠ å¸¸æ•°é¡¹

            for var_type, type_variables in type_to_vars.items():
                if not type_variables:
                    continue # Skip empty types

                logger.debug(f"  å¤„ç†ç±»å‹: '{var_type}' ({len(type_variables)} ä¸ªå˜é‡) å¯¹ {factor_name}...")
                type_data_subset = data_subset[type_variables]

                total_tss_type = 0.0
                total_rss_type = 0.0
                valid_regressions = 0

                # éå†ç±»å‹å†…çš„æ¯ä¸ªå˜é‡
                for var in type_variables:
                    y_series = type_data_subset[var].dropna()
                    # å¯¹é½å½“å‰å˜é‡å’Œå½“å‰å•ä¸ªå› å­
                    common_index = factor_series_with_const.index.intersection(y_series.index)

                    if len(common_index) < 3: # OLS (k=1) needs at least 3 points
                        continue

                    y = y_series.loc[common_index]
                    X = factor_series_with_const.loc[common_index]

                    # å†æ¬¡æ£€æŸ¥ NaN (ç†è®ºä¸Šä¸åº”æœ‰, ä½†ä»¥é˜²ä¸‡ä¸€)
                    if X.isnull().any().any() or y.isnull().any():
                        combined = pd.concat([y, X], axis=1).dropna()
                        if len(combined) < 3:
                            continue
                        y = combined.iloc[:, 0]
                        X = combined.iloc[:, 1:]

                    if y.var() < 1e-9:
                        continue # Skip if variable has no variance

                    try:
                        tss = np.sum((y - y.mean())**2)
                        # OLS: y ~ const + factor_k
                        model = sm.OLS(y, X).fit()
                        rss = np.sum(model.resid**2)

                        if np.isfinite(tss) and np.isfinite(rss) and tss > 1e-9:
                            total_tss_type += tss
                            total_rss_type += rss
                            valid_regressions += 1

                    except Exception as e_ols:
                        logger.error(f"    OLS error for var '{var}' vs factor '{factor_name}': {e_ols}")

                # è®¡ç®—è¯¥å› å­å¯¹è¯¥ç±»å‹çš„ Pooled RÂ²
                if valid_regressions > 0 and total_tss_type > 1e-9:
                    # ***** ä¸ Industry R2 ç®—æ³•ä¸€è‡´ï¼Œç¡®ä¿éè´Ÿ *****
                    pooled_r2 = max(0.0, 1.0 - (total_rss_type / total_tss_type))
                    type_r2_values[var_type] = pooled_r2
                    logger.debug(f"  => {factor_name} å¯¹ç±»å‹ '{var_type}' çš„ Pooled RÂ²: {pooled_r2:.4f} (åŸºäº {valid_regressions} ä¸ªå˜é‡)")
                elif valid_regressions == 0:
                     logger.warning(f"  => {factor_name} å¯¹ç±»å‹ '{var_type}' æ²¡æœ‰æˆåŠŸå®Œæˆä»»ä½•å˜é‡çš„å›å½’ï¼ŒRÂ² è®¾ä¸º NaNã€‚")
                     type_r2_values[var_type] = np.nan
                else: # TSS æ¥è¿‘äº 0
                     logger.warning(f"  => {factor_name} å¯¹ç±»å‹ '{var_type}' çš„æ€»å¹³æ–¹å’Œæ¥è¿‘é›¶ï¼ŒRÂ² è®¾ä¸º NaNã€‚")
                     type_r2_values[var_type] = np.nan
            # --- ç»“æŸç±»å‹å¾ªç¯ ---

            if type_r2_values:
                factor_type_r2_dict[factor_name] = pd.Series(type_r2_values).sort_index()
            else:
                logger.warning(f"å› å­ {factor_name} æœªèƒ½è®¡ç®—ä»»ä½•ç±»å‹çš„ RÂ²ã€‚")
        # --- ç»“æŸå› å­å¾ªç¯ ---

        if not factor_type_r2_dict:
            logger.warning("æœªèƒ½è®¡ç®—ä»»ä½•å› å­å¯¹ä»»ä½•ç±»å‹çš„ Pooled RÂ² (OLS-based)ã€‚")
            return None

        logger.info("å•å› å­å¯¹å˜é‡ç±»å‹çš„ Pooled RÂ² (OLS-based) è®¡ç®—å®Œæˆã€‚")
        return factor_type_r2_dict

    except Exception as e:
        logger.error(f"è®¡ç®—å•å› å­å¯¹ç±»å‹ Pooled RÂ² (OLS-based) æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)
        return None
# --- ç»“æŸä¿®æ”¹ ---

# --- ç»“æŸæ–°å¢å‡½æ•° --- 