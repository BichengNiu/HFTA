import streamlit as st

# --- æ–°å¢ï¼šå¯¼å…¥çŠ¶æ€ç®¡ç†å™¨ ---
try:
    from ...core.state_manager import StateManager
    from ...core.compat import CompatibilityAdapter
    from ...core.state_keys import StateKeys
    DFM_STATE_MANAGER_AVAILABLE = True
except ImportError:
    DFM_STATE_MANAGER_AVAILABLE = False
    print("[DFM UI] Warning: State manager not available, using legacy state management")


def get_dfm_state_manager_instance():
    """è·å–çŠ¶æ€ç®¡ç†å™¨å®ä¾‹ï¼ˆDFMæ¨¡å—ä¸“ç”¨ï¼‰"""
    if DFM_STATE_MANAGER_AVAILABLE and hasattr(st.session_state, 'state_manager_initialized'):
        try:
            state_manager = StateManager(st.session_state)
            compat_adapter = CompatibilityAdapter(st.session_state)
            return state_manager, compat_adapter
        except Exception as e:
            print(f"[DFM UI] Error getting state manager: {e}")
            return None, None
    return None, None


def get_dfm_state(key, default=None, session_state=None):
    """è·å–DFMçŠ¶æ€å€¼ï¼ˆå…¼å®¹æ–°æ—§çŠ¶æ€ç®¡ç†ï¼‰"""
    state_manager, compat_adapter = get_dfm_state_manager_instance()
    
    if compat_adapter is not None:
        return compat_adapter.get_value(key, default)
    else:
        # å›é€€åˆ°ä¼ ç»Ÿæ–¹å¼
        if session_state is not None:
            return getattr(session_state, key, default) if hasattr(session_state, key) else session_state.get(key, default)
        else:
            return st.session_state.get(key, default)


def set_dfm_state(key, value, session_state=None):
    """è®¾ç½®DFMçŠ¶æ€å€¼ï¼ˆå…¼å®¹æ–°æ—§çŠ¶æ€ç®¡ç†ï¼‰"""
    state_manager, compat_adapter = get_dfm_state_manager_instance()
    
    if compat_adapter is not None:
        compat_adapter.set_value(key, value, use_new_key=True)
    else:
        # å›é€€åˆ°ä¼ ç»Ÿæ–¹å¼
        if session_state is not None:
            if hasattr(session_state, key):
                setattr(session_state, key, value)
            else:
                session_state[key] = value
        else:
            st.session_state[key] = value
# --- ç»“æŸæ–°å¢ ---

from datetime import datetime

# å¯¼å…¥é…ç½®
try:
    from config import (
        DataDefaults, TrainDefaults, UIDefaults, VisualizationDefaults,
        FileDefaults, FormatDefaults, AnalysisDefaults
    )
    CONFIG_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ æ— æ³•å¯¼å…¥é…ç½®æ¨¡å—: {e}")
    CONFIG_AVAILABLE = False

def render_dfm_data_prep_tab(st, session_state):
    """Renders the DFM Model Data Preparation tab."""
    st.markdown("#### ä¸Šä¼ æ•°æ®")

    # åˆå§‹åŒ– session_state ä¸­çš„æ–‡ä»¶å­˜å‚¨
    if get_dfm_state('dfm_training_data_file', None, session_state) is None:
        set_dfm_state("dfm_training_data_file", None, session_state)
    
    # --- NEW: Initialize session_state for direct data passing ---
    if get_dfm_state('dfm_prepared_data_df', None, session_state) is None:
        set_dfm_state("dfm_prepared_data_df", None, session_state)
    if get_dfm_state('dfm_transform_log_obj', None, session_state) is None:
        set_dfm_state("dfm_transform_log_obj", None, session_state)
    if get_dfm_state('dfm_industry_map_obj', None, session_state) is None:
        set_dfm_state("dfm_industry_map_obj", None, session_state)
    if get_dfm_state('dfm_removed_vars_log_obj', None, session_state) is None:
        set_dfm_state("dfm_removed_vars_log_obj", None, session_state)
    if get_dfm_state('dfm_var_type_map_obj', None, session_state) is None:
        set_dfm_state("dfm_var_type_map_obj", None, session_state)
    # --- END NEW ---

    uploaded_file = st.file_uploader(
        "é€‰æ‹©è®­ç»ƒæ•°æ®é›† (ä¾‹å¦‚ï¼š.csv, .xlsx)", 
        type=["csv", "xlsx"], 
        key="dfm_training_data_uploader",
        help="è¯·ä¸Šä¼ åŒ…å«æ¨¡å‹è®­ç»ƒæ‰€éœ€æŒ‡æ ‡çš„è¡¨æ ¼æ•°æ®ã€‚"
    )

    if uploaded_file is not None:
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°æ–‡ä»¶ä¸Šä¼ ï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰
        file_changed = (
            session_state.dfm_training_data_file is None or
            session_state.dfm_training_data_file.name != uploaded_file.name or
            get_dfm_state('dfm_file_processed', False, session_state) == False
        )

        set_dfm_state("dfm_training_data_file", uploaded_file, session_state)
        # ğŸ”¥ æ–°å¢ï¼šä¿å­˜Excelæ–‡ä»¶è·¯å¾„ç”¨äºè®­ç»ƒæ¨¡å—
        set_dfm_state("dfm_uploaded_excel_file_path", uploaded_file.name, session_state)
        set_dfm_state("dfm_use_full_data_preparation", True, session_state)

        # åªæœ‰åœ¨æ–‡ä»¶å‘ç”Ÿå˜åŒ–æ—¶æ‰æ ‡è®°éœ€è¦é‡æ–°æ£€æµ‹
        if file_changed:
            print(f"[UI] æ£€æµ‹åˆ°æ–°æ–‡ä»¶ä¸Šä¼ : {uploaded_file.name}ï¼Œæ ‡è®°éœ€è¦é‡æ–°æ£€æµ‹...")
            set_dfm_state("dfm_file_processed", False, session_state)  # é‡ç½®å¤„ç†æ ‡è®°
            set_dfm_state("dfm_date_detection_needed", True, session_state)  # æ ‡è®°éœ€è¦æ—¥æœŸæ£€æµ‹

            # æ˜¾ç¤ºæ–‡ä»¶ä¸Šä¼ æˆåŠŸä¿¡æ¯
            st.success(f"æ–‡ä»¶ '{uploaded_file.name}' å·²ä¸Šä¼ å¹¶å‡†å¤‡å°±ç»ªã€‚")
            st.info("ğŸ“… æ–‡ä»¶å·²åŠ è½½ï¼Œå°†è‡ªåŠ¨æ£€æµ‹æ—¥æœŸèŒƒå›´ã€‚")

            # æ ‡è®°æ–‡ä»¶å·²å¤„ç†
            set_dfm_state("dfm_file_processed", True, session_state)
        else:
            # æ–‡ä»¶æ²¡æœ‰å˜åŒ–ï¼Œæ˜¾ç¤ºç®€å•çš„çŠ¶æ€ä¿¡æ¯
            st.success(f"æ–‡ä»¶ '{uploaded_file.name}' å·²åŠ è½½ã€‚")
            
    elif session_state.dfm_training_data_file is not None:
        st.info(f"å½“å‰å·²åŠ è½½è®­ç»ƒæ•°æ®: {session_state.dfm_training_data_file.name}. æ‚¨å¯ä»¥ä¸Šä¼ æ–°æ–‡ä»¶æ›¿æ¢å®ƒã€‚")
        
        # æ·»åŠ æ–‡ä»¶ç»“æ„æ£€æŸ¥å·¥å…·
        with st.expander("ğŸ” æ–‡ä»¶ç»“æ„è¯Šæ–­å·¥å…· (å¯é€‰)", expanded=False):
            if st.button("æ£€æŸ¥æ–‡ä»¶ç»“æ„", help="æŸ¥çœ‹å·²ä¸Šä¼ æ–‡ä»¶çš„å†…éƒ¨ç»“æ„ï¼Œå¸®åŠ©è¯Šæ–­æ ¼å¼é—®é¢˜"):
                try:
                    import io
                    import pandas as pd
                    
                    uploaded_file_bytes = session_state.dfm_training_data_file.getvalue()
                    excel_file_like_object = io.BytesIO(uploaded_file_bytes)
                    
                    if session_state.dfm_training_data_file.name.endswith('.xlsx'):
                        xl_file = pd.ExcelFile(excel_file_like_object)
                        sheet_names = xl_file.sheet_names
                        
                        st.write(f"**æ–‡ä»¶åŒ…å« {len(sheet_names)} ä¸ªå·¥ä½œè¡¨:**")
                        for i, sheet_name in enumerate(sheet_names):  # æ˜¾ç¤ºæ‰€æœ‰å·¥ä½œè¡¨
                            with st.expander(f"å·¥ä½œè¡¨ {i+1}: {sheet_name}", expanded=(i==0)):
                                try:
                                    # å°è¯•ä½¿ç”¨æ ¼å¼æ£€æµ‹
                                    try:
                                        from .data_preparation import detect_sheet_format
                                        format_info = detect_sheet_format(excel_file_like_object, sheet_name)
                                        st.write(f"**æ£€æµ‹åˆ°çš„æ ¼å¼:** {format_info['format']} (æ¥æº: {format_info['source']})")
                                        st.write(f"**å»ºè®®å‚æ•°:** header={format_info['header']}, skiprows={format_info['skiprows']}")
                                    except:
                                        st.write("**æ ¼å¼æ£€æµ‹:** ä½¿ç”¨é»˜è®¤å‚æ•°")
                                    
                                    # è¯»å–å‰å‡ è¡Œ
                                    df_preview = pd.read_excel(excel_file_like_object, sheet_name=sheet_name, nrows=5)
                                    st.write(f"**æ•°æ®å½¢çŠ¶:** {df_preview.shape}")
                                    st.write("**å‰5è¡Œé¢„è§ˆ:**")
                                    st.dataframe(df_preview)
                                    
                                    # æ£€æŸ¥ç¬¬ä¸€åˆ—æ˜¯å¦å¯èƒ½æ˜¯æ—¥æœŸ
                                    if len(df_preview.columns) > 0:
                                        first_col = df_preview.columns[0]
                                        first_col_values = df_preview[first_col].dropna().head(3)
                                        st.write(f"**ç¬¬ä¸€åˆ— '{first_col}' çš„æ ·æœ¬å€¼:** {list(first_col_values)}")
                                        
                                        # å°è¯•è½¬æ¢ä¸ºæ—¥æœŸ
                                        try:
                                            date_converted = pd.to_datetime(first_col_values, errors='coerce')
                                            if not date_converted.isna().all():
                                                st.success(f"âœ… ç¬¬ä¸€åˆ—å¯ä»¥è½¬æ¢ä¸ºæ—¥æœŸ: {date_converted.dropna().iloc[0]}")
                                            else:
                                                st.warning("âš ï¸ ç¬¬ä¸€åˆ—æ— æ³•è½¬æ¢ä¸ºæ—¥æœŸ")
                                        except:
                                            st.warning("âš ï¸ ç¬¬ä¸€åˆ—æ—¥æœŸè½¬æ¢å‡ºé”™")
                                    
                                except Exception as e:
                                    st.error(f"è¯»å–å·¥ä½œè¡¨å‡ºé”™: {e}")
                        
                        # ç§»é™¤é™åˆ¶æ˜¾ç¤ºçš„æç¤ºä¿¡æ¯
                            
                except Exception as e:
                    st.error(f"æ–‡ä»¶ç»“æ„æ£€æŸ¥å‡ºé”™: {e}")
    else:
        st.info("è¯·ä¸Šä¼ è®­ç»ƒæ•°æ®é›†ã€‚")

    st.markdown("**è¯´æ˜:**")
    st.markdown("- æ­¤å¤„ä¸Šä¼ çš„æ•°æ®å°†ç”¨äºDFMæ¨¡å‹çš„è®­ç»ƒæˆ–é‡æ–°è®­ç»ƒã€‚")
    st.markdown("- è¯·ç¡®ä¿æ•°æ®æ ¼å¼ç¬¦åˆæ¨¡å‹è¦æ±‚ã€‚")

    
    # æ·»åŠ é‡è¦æç¤º
    st.info("ğŸ“Œ **é‡è¦è¯´æ˜**: ä¸‹é¢è®¾ç½®çš„æ—¥æœŸèŒƒå›´å°†ä½œä¸ºç³»ç»Ÿå¤„ç†æ•°æ®çš„**æœ€å¤§è¾¹ç•Œ**ã€‚åç»­çš„è®­ç»ƒæœŸã€éªŒè¯æœŸè®¾ç½®å¿…é¡»åœ¨æ­¤èŒƒå›´å†…ã€‚ç»“æœå±•ç¤ºçš„Nowcastingé»˜è®¤è¦†ç›–æ­¤å®Œæ•´æ—¶é—´èŒƒå›´ã€‚")

    # ğŸ”¥ æ­£ç¡®ä¿®å¤ï¼šæ ¹æ®æ•°æ®æ–‡ä»¶çš„å®é™…æ—¥æœŸèŒƒå›´è¿›è¡Œæ£€æµ‹
    def detect_data_date_range(uploaded_file):
        """ä»ä¸Šä¼ çš„æ–‡ä»¶ä¸­æ£€æµ‹æ•°æ®çš„çœŸå®æ—¥æœŸèŒƒå›´"""
        try:
            if uploaded_file is None:
                return None, None

            import io
            import pandas as pd

            # è¯»å–æ–‡ä»¶
            file_bytes = uploaded_file.getvalue()
            excel_file = io.BytesIO(file_bytes)

            all_dates_found = []

            # è·å–æ‰€æœ‰å·¥ä½œè¡¨åç§°
            try:
                xl_file = pd.ExcelFile(excel_file)
                sheet_names = xl_file.sheet_names
                print(f"æ£€æµ‹åˆ°å·¥ä½œè¡¨: {sheet_names}")
            except:
                sheet_names = [0]  # å›é€€åˆ°ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨

            # æ£€æŸ¥æ¯ä¸ªå·¥ä½œè¡¨å¯»æ‰¾çœŸå®çš„æ—¥æœŸæ•°æ®
            for sheet_name in sheet_names:
                try:
                    excel_file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ

                    # è·³è¿‡æ˜æ˜¾çš„å…ƒæ•°æ®å·¥ä½œè¡¨
                    if any(keyword in str(sheet_name).lower() for keyword in ['æŒ‡æ ‡ä½“ç³»', 'mapping', 'meta', 'info']):
                        print(f"è·³è¿‡å…ƒæ•°æ®å·¥ä½œè¡¨: {sheet_name}")
                        continue

                    # è¯»å–å·¥ä½œè¡¨
                    df = pd.read_excel(excel_file, sheet_name=sheet_name, index_col=0)

                    if len(df) < 5:  # è·³è¿‡æ•°æ®å¤ªå°‘çš„å·¥ä½œè¡¨
                        continue

                    # æ£€æŸ¥ç´¢å¼•ä¸­çš„æ—¥æœŸ
                    datetime_indices = []
                    for idx in df.index:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯æ—¥æœŸæ—¶é—´ç±»å‹
                        if pd.api.types.is_datetime64_any_dtype(pd.Series([idx])):
                            datetime_indices.append(idx)
                        elif isinstance(idx, pd.Timestamp):
                            datetime_indices.append(idx)
                        elif hasattr(idx, 'year'):  # datetimeå¯¹è±¡
                            datetime_indices.append(idx)

                    if len(datetime_indices) > 10:  # è‡³å°‘è¦æœ‰10ä¸ªæ—¥æœŸæ‰è®¤ä¸ºæ˜¯æ—¶é—´åºåˆ—
                        dates_series = pd.to_datetime(datetime_indices)
                        all_dates_found.extend(dates_series.tolist())
                        print(f"âœ… å·¥ä½œè¡¨ '{sheet_name}': æ‰¾åˆ° {len(dates_series)} ä¸ªæ—¥æœŸ")
                        print(f"   èŒƒå›´: {dates_series.min().date()} åˆ° {dates_series.max().date()}")

                except Exception as e:
                    print(f"âš ï¸ å·¥ä½œè¡¨ '{sheet_name}': å¤„ç†å¤±è´¥ - {e}")
                    continue

            # æ±‡æ€»æ‰€æœ‰çœŸå®æ—¥æœŸï¼Œè¿”å›å®é™…çš„æ•°æ®èŒƒå›´
            if all_dates_found:
                all_dates = pd.to_datetime(all_dates_found)
                actual_start = all_dates.min().date()
                actual_end = all_dates.max().date()

                print(f"âœ… æ£€æµ‹åˆ°æ•°æ®çš„å®é™…æ—¥æœŸèŒƒå›´: {actual_start} åˆ° {actual_end}")
                print(f"   æ€»å…± {len(all_dates)} ä¸ªæ—¥æœŸç‚¹ï¼Œå¹´ä»½è·¨åº¦: {actual_start.year}-{actual_end.year}")

                return actual_start, actual_end
            else:
                print("âŒ æœªèƒ½æ£€æµ‹åˆ°ä»»ä½•æ—¥æœŸæ•°æ®")
                return None, None

        except Exception as e:
            print(f"âŒ æ£€æµ‹æ—¥æœŸèŒƒå›´å¤±è´¥: {e}")
            return None, None

    # ğŸ”¥ ä¼˜åŒ–ï¼šå¢å¼ºç¼“å­˜æœºåˆ¶ï¼Œé¿å…é‡å¤æ£€æµ‹
    # æ£€æµ‹ä¸Šä¼ æ–‡ä»¶çš„æ—¥æœŸèŒƒå›´ï¼ˆåªåœ¨æ–‡ä»¶å˜åŒ–æ—¶æ‰§è¡Œï¼‰
    if session_state.dfm_training_data_file:
        file_hash = hash(session_state.dfm_training_data_file.getvalue())
        cache_key = f"date_range_{session_state.dfm_training_data_file.name}_{file_hash}"
    else:
        cache_key = "date_range_none"

    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ
    cache_valid = (
        cache_key in session_state and
        not get_dfm_state('dfm_date_detection_needed', False, session_state)
    )

    if not cache_valid:
        # éœ€è¦é‡æ–°æ£€æµ‹
        if session_state.dfm_training_data_file:
            print(f"[UI] æ‰§è¡Œæ—¥æœŸæ£€æµ‹: {session_state.dfm_training_data_file.name}")
            with st.spinner("ğŸ” æ­£åœ¨æ£€æµ‹æ•°æ®æ—¥æœŸèŒƒå›´..."):
                detected_start, detected_end = detect_data_date_range(session_state.dfm_training_data_file)
            # ç¼“å­˜ç»“æœ
            session_state[cache_key] = (detected_start, detected_end)
            set_dfm_state("dfm_date_detection_needed", False, session_state)

            # æ¸…ç†æ—§çš„ç¼“å­˜
            old_keys = [k for k in session_state.keys() if k.startswith("date_range_") and k != cache_key]
            for old_key in old_keys:
                del session_state[old_key]
        else:
            detected_start, detected_end = None, None
            session_state[cache_key] = (None, None)
    else:
        # ä½¿ç”¨ç¼“å­˜çš„ç»“æœ
        detected_start, detected_end = session_state[cache_key]
        if detected_start and detected_end:
            print(f"[UI] ä½¿ç”¨ç¼“å­˜çš„æ—¥æœŸèŒƒå›´: {detected_start} åˆ° {detected_end}")

    # è®¾ç½®é»˜è®¤å€¼ï¼šä¼˜å…ˆä½¿ç”¨æ£€æµ‹åˆ°çš„æ—¥æœŸï¼Œå¦åˆ™ä½¿ç”¨ç¡¬ç¼–ç é»˜è®¤å€¼
    default_start_date = detected_start if detected_start else datetime(2020, 1, 1).date()
    default_end_date = detected_end if detected_end else datetime(2025, 4, 30).date()

    param_defaults = {
        'dfm_param_target_variable': 'è§„æ¨¡ä»¥ä¸Šå·¥ä¸šå¢åŠ å€¼:å½“æœˆåŒæ¯”',
        'dfm_param_target_sheet_name': 'å·¥ä¸šå¢åŠ å€¼åŒæ¯”å¢é€Ÿ_æœˆåº¦_åŒèŠ±é¡º',
        'dfm_param_target_freq': 'W-FRI',
        'dfm_param_remove_consecutive_nans': True,
        'dfm_param_consecutive_nan_threshold': 10,
        'dfm_param_type_mapping_sheet': DataDefaults.TYPE_MAPPING_SHEET if CONFIG_AVAILABLE else 'æŒ‡æ ‡ä½“ç³»',
        'dfm_param_data_start_date': default_start_date,
        'dfm_param_data_end_date': default_end_date
    }

    # ğŸ”¥ ä¿®å¤ï¼šåªåœ¨é¦–æ¬¡åˆå§‹åŒ–æˆ–æ–‡ä»¶æ›´æ–°æ—¶è®¾ç½®é»˜è®¤å€¼
    for key, default_value in param_defaults.items():
        if key not in session_state:
            session_state[key] = default_value
        elif key in ['dfm_param_data_start_date', 'dfm_param_data_end_date'] and detected_start and detected_end:
            # å¦‚æœæ£€æµ‹åˆ°æ–°çš„æ—¥æœŸèŒƒå›´ï¼Œæ›´æ–°æ—¥æœŸè®¾ç½®
            if key == 'dfm_param_data_start_date':
                session_state[key] = default_start_date
            elif key == 'dfm_param_data_end_date':
                session_state[key] = default_end_date

    # æ˜¾ç¤ºæ£€æµ‹ç»“æœ
    if detected_start and detected_end:
        st.success(f"âœ… å·²è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶æ—¥æœŸèŒƒå›´: {detected_start} åˆ° {detected_end}")
    elif session_state.dfm_training_data_file:
        st.warning("âš ï¸ æ— æ³•è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶æ—¥æœŸèŒƒå›´ï¼Œä½¿ç”¨é»˜è®¤å€¼ã€‚è¯·æ‰‹åŠ¨è°ƒæ•´æ—¥æœŸè®¾ç½®ã€‚")

    row1_col1, row1_col2 = st.columns(2)
    with row1_col1:
        set_dfm_state("dfm_param_data_start_date", st.date_input(
            "æ•°æ®å¼€å§‹æ—¥æœŸ (ç³»ç»Ÿè¾¹ç•Œ)",
            value=session_state.dfm_param_data_start_date,
            key="ss_dfm_data_start",
            help="è®¾ç½®ç³»ç»Ÿå¤„ç†æ•°æ®çš„æœ€æ—©æ—¥æœŸè¾¹ç•Œã€‚è®­ç»ƒæœŸã€éªŒè¯æœŸå¿…é¡»åœ¨æ­¤æ—¥æœŸä¹‹åã€‚"
        ), session_state)
    with row1_col2:
        set_dfm_state("dfm_param_data_end_date", st.date_input(
            "æ•°æ®ç»“æŸæ—¥æœŸ (ç³»ç»Ÿè¾¹ç•Œ)",
            value=session_state.dfm_param_data_end_date,
            key="ss_dfm_data_end",
            help="è®¾ç½®ç³»ç»Ÿå¤„ç†æ•°æ®çš„æœ€æ™šæ—¥æœŸè¾¹ç•Œã€‚è®­ç»ƒæœŸã€éªŒè¯æœŸå¿…é¡»åœ¨æ­¤æ—¥æœŸä¹‹å‰ã€‚"
        ), session_state)

    row2_col1, row2_col2 = st.columns(2)
    with row2_col1:
        set_dfm_state("dfm_param_target_sheet_name", st.text_input(
            "ç›®æ ‡å·¥ä½œè¡¨åç§° (Target Sheet Name)",
            value=session_state.dfm_param_target_sheet_name,
            key="ss_dfm_target_sheet"
        ), session_state)
    with row2_col2:
        set_dfm_state("dfm_param_target_variable", st.text_input(
            "ç›®æ ‡å˜é‡ (Target Variable)",
            value=session_state.dfm_param_target_variable,
            key="ss_dfm_target_var"
        ), session_state)

    row3_col1, row3_col2 = st.columns(2)
    with row3_col1:
        set_dfm_state("dfm_param_consecutive_nan_threshold", st.number_input(
            "è¿ç»­ NaN é˜ˆå€¼ (Consecutive NaN Threshold)",
            min_value=0,
            value=session_state.dfm_param_consecutive_nan_threshold,
            step=1,
            key="ss_dfm_nan_thresh"
        ), session_state)
    with row3_col2:
        set_dfm_state("dfm_param_remove_consecutive_nans", st.checkbox(
            "ç§»é™¤è¿‡å¤šè¿ç»­ NaN çš„å˜é‡",
            value=session_state.dfm_param_remove_consecutive_nans,
            key="ss_dfm_remove_nans",
            help="ç§»é™¤åˆ—ä¸­è¿ç»­ç¼ºå¤±å€¼æ•°é‡è¶…è¿‡é˜ˆå€¼çš„å˜é‡"
        ), session_state)

    row4_col1, row4_col2 = st.columns(2)
    with row4_col1:
        set_dfm_state("dfm_param_target_freq", st.text_input(
            "ç›®æ ‡é¢‘ç‡ (Target Frequency)",
            value=session_state.dfm_param_target_freq,
            help="ä¾‹å¦‚: W-FRI, D, M, Q",
            key="ss_dfm_target_freq"
        ), session_state)
    with row4_col2:
        set_dfm_state("dfm_param_type_mapping_sheet", st.text_input(
            "æŒ‡æ ‡æ˜ å°„è¡¨åç§° (Type Mapping Sheet)",
            value=session_state.dfm_param_type_mapping_sheet,
            key="ss_dfm_type_map_sheet"
        ), session_state)

    st.markdown("--- ") # Separator before the new section
    st.markdown("#### æ•°æ®é¢„å¤„ç†ä¸å¯¼å‡º")

    # Initialize session_state for new UI elements if not already present
    if get_dfm_state('dfm_export_base_name', None, session_state) is None:
        set_dfm_state("dfm_export_base_name", "dfm_prepared_output", session_state)
    if get_dfm_state('dfm_processed_outputs', None, session_state) is None: # For storing results to persist downloads
        set_dfm_state("dfm_processed_outputs", None, session_state)

    left_col, right_col = st.columns([1, 2]) # Left col for inputs, Right col for outputs/messages

    with left_col:
        set_dfm_state("dfm_export_base_name", st.text_input(
            "å¯¼å‡ºæ–‡ä»¶åŸºç¡€åç§° (Export Base Filename)",
            value=session_state.dfm_export_base_name,
            key="ss_dfm_export_basename"
        ), session_state)

        run_button_clicked = st.button("è¿è¡Œæ•°æ®é¢„å¤„ç†å¹¶å¯¼å‡º", key="ss_dfm_run_preprocessing")

    with right_col:
        if run_button_clicked:
            set_dfm_state("dfm_processed_outputs", None, session_state)  # Clear previous downloadable results
            # --- NEW: Clear previous direct data objects --- 
            set_dfm_state("dfm_prepared_data_df", None, session_state)
            set_dfm_state("dfm_transform_log_obj", None, session_state)
            set_dfm_state("dfm_industry_map_obj", None, session_state)
            set_dfm_state("dfm_removed_vars_log_obj", None, session_state)
            set_dfm_state("dfm_var_type_map_obj", None, session_state)
            # --- END NEW ---

            if session_state.dfm_training_data_file is None:
                st.error("é”™è¯¯ï¼šè¯·å…ˆä¸Šä¼ è®­ç»ƒæ•°æ®é›†ï¼")
            elif not session_state.dfm_export_base_name:
                st.error("é”™è¯¯ï¼šè¯·æŒ‡å®šæœ‰æ•ˆçš„æ–‡ä»¶åŸºç¡€åç§°ï¼")
            else:
                try:
                    import io 
                    import json
                    import pandas as pd
                    from .data_preparation import prepare_data, load_mappings

                    # ğŸ”¥ ä¼˜åŒ–ï¼šæ·»åŠ è¯¦ç»†çš„è¿›åº¦æŒ‡ç¤ºå™¨
                    progress_bar = st.progress(0)
                    status_text = st.empty()

                    try:
                        status_text.text("ğŸ”„ æ­£åœ¨å‡†å¤‡æ•°æ®...")
                        progress_bar.progress(10)

                        uploaded_file_bytes = session_state.dfm_training_data_file.getvalue()
                        excel_file_like_object = io.BytesIO(uploaded_file_bytes)

                        start_date_str = session_state.dfm_param_data_start_date.strftime('%Y-%m-%d') \
                            if session_state.dfm_param_data_start_date else None
                        end_date_str = session_state.dfm_param_data_end_date.strftime('%Y-%m-%d') \
                            if session_state.dfm_param_data_end_date else None

                        status_text.text("ğŸ“Š æ­£åœ¨è¯»å–æ•°æ®æ–‡ä»¶...")
                        progress_bar.progress(20)
                        
                        # ğŸ”§ ä¿®å¤ï¼šåªæœ‰åœ¨å¯ç”¨ç§»é™¤è¿ç»­NaNåŠŸèƒ½æ—¶æ‰ä¼ é€’é˜ˆå€¼
                        nan_threshold_int = None
                        if session_state.dfm_param_remove_consecutive_nans:
                            nan_threshold = session_state.dfm_param_consecutive_nan_threshold
                            if not pd.isna(nan_threshold):
                                try:
                                    nan_threshold_int = int(nan_threshold)
                                except ValueError:
                                    st.warning(f"è¿ç»­NaNé˜ˆå€¼ '{nan_threshold}' ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ•´æ•°ã€‚å°†å¿½ç•¥æ­¤é˜ˆå€¼ã€‚")
                                    nan_threshold_int = None

                        # ğŸ”¥ğŸ”¥ğŸ”¥ æ–°å¢ï¼šæ•°æ®é¢„å¤„ç†å‚æ•°è°ƒè¯•
                        print(f"\n" + "="*80)
                        print(f"ğŸ”ğŸ”ğŸ” [æ•°æ®é¢„å¤„ç†å‚æ•°è°ƒè¯•] æ£€æŸ¥å˜é‡è¿‡æ»¤è®¾ç½®")
                        print(f"="*80)
                        print(f"ğŸ“Š é¢„å¤„ç†å‚æ•°:")
                        print(f"   ç›®æ ‡å˜é‡: {session_state.dfm_param_target_variable}")
                        print(f"   ç›®æ ‡é¢‘ç‡: {session_state.dfm_param_target_freq}")
                        print(f"   æ˜¯å¦ç§»é™¤è¿ç»­NaNå˜é‡: {session_state.dfm_param_remove_consecutive_nans}")
                        print(f"   è¿ç»­NaNé˜ˆå€¼: {session_state.dfm_param_consecutive_nan_threshold}")
                        print(f"   å®é™…ä¼ é€’çš„é˜ˆå€¼: {nan_threshold_int}")
                        print(f"   æ•°æ®æ—¥æœŸèŒƒå›´: {start_date_str} åˆ° {end_date_str}")

                        if nan_threshold_int is not None:
                            print(f"âš ï¸ è­¦å‘Š: è¿ç»­NaNé˜ˆå€¼è®¾ç½®ä¸º {nan_threshold_int}ï¼Œå¯èƒ½ä¼šç§»é™¤å¤§é‡å˜é‡ï¼")
                            print(f"   ä»»ä½•è¿ç»­ç¼ºå¤±å€¼ â‰¥ {nan_threshold_int} çš„å˜é‡éƒ½ä¼šè¢«ç§»é™¤")
                        else:
                            print(f"âœ… è¿ç»­NaNè¿‡æ»¤å·²ç¦ç”¨ï¼Œä¸ä¼šç§»é™¤å˜é‡")
                        print(f"="*80)

                        status_text.text("ğŸ”§ æ­£åœ¨æ‰§è¡Œæ•°æ®é¢„å¤„ç†...")
                        progress_bar.progress(30)

                        # ä¿®å¤å‚æ•°é¡ºåºä»¥åŒ¹é…data_preparation.pyä¸­çš„å‡½æ•°ç­¾å
                        results = prepare_data(
                            excel_path=excel_file_like_object,
                            target_freq=session_state.dfm_param_target_freq,
                            target_sheet_name=session_state.dfm_param_target_sheet_name,
                            target_variable_name=session_state.dfm_param_target_variable,
                            consecutive_nan_threshold=nan_threshold_int,
                            data_start_date=start_date_str,
                            data_end_date=end_date_str,
                            reference_sheet_name=session_state.dfm_param_type_mapping_sheet,
                            # reference_column_name ä½¿ç”¨é…ç½®çš„é»˜è®¤å€¼
                        )

                        status_text.text("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œæ­£åœ¨ç”Ÿæˆç»“æœ...")
                        progress_bar.progress(70)

                        if results:
                            status_text.text("ğŸ“‹ æ­£åœ¨å¤„ç†ç»“æœæ•°æ®...")
                            progress_bar.progress(80)

                            # ä¿®å¤è§£åŒ…é¡ºåºï¼šprepare_dataè¿”å› (data, industry_map, transform_log, removed_vars_log)
                            prepared_data, industry_map, transform_log, removed_variables_detailed_log = results

                            # ğŸ”¥ğŸ”¥ğŸ”¥ æ–°å¢ï¼šè¯¦ç»†çš„å˜é‡ç§»é™¤åˆ†æ
                            print(f"\n" + "="*80)
                            print(f"ğŸ”ğŸ”ğŸ” [æ•°æ®é¢„å¤„ç†ç»“æœåˆ†æ] å˜é‡ç§»é™¤è¯¦æƒ…")
                            print(f"="*80)
                            print(f"ğŸ“Š é¢„å¤„ç†ç»“æœ:")
                            print(f"   æœ€ç»ˆæ•°æ®å½¢çŠ¶: {prepared_data.shape}")
                            print(f"   æœ€ç»ˆå˜é‡æ•°é‡: {len(prepared_data.columns)}")
                            print(f"   ç§»é™¤çš„å˜é‡æ•°é‡: {len(removed_variables_detailed_log) if removed_variables_detailed_log else 0}")

                            if removed_variables_detailed_log:
                                print(f"\nğŸ“‹ ç§»é™¤å˜é‡è¯¦ç»†åˆ†æ:")
                                removal_reasons = {}
                                for item in removed_variables_detailed_log:
                                    reason = item.get('Reason', 'unknown')
                                    if reason not in removal_reasons:
                                        removal_reasons[reason] = []
                                    removal_reasons[reason].append(item.get('Variable', 'unknown'))

                                for reason, vars_list in removal_reasons.items():
                                    print(f"   ğŸ”¸ {reason}: {len(vars_list)} ä¸ªå˜é‡")
                                    if 'consecutive_nan' in reason.lower():
                                        print(f"      âŒ å› è¿ç»­ç¼ºå¤±å€¼è¿‡å¤šè¢«ç§»é™¤: {vars_list[:5]}{'...' if len(vars_list) > 5 else ''}")
                                    else:
                                        print(f"      - å˜é‡: {vars_list[:3]}{'...' if len(vars_list) > 3 else ''}")

                                # ç»Ÿè®¡è¿ç»­NaNç§»é™¤çš„å˜é‡
                                nan_removed = [item for item in removed_variables_detailed_log if 'consecutive_nan' in item.get('Reason', '').lower()]
                                if nan_removed:
                                    print(f"\nâš ï¸ é‡è¦å‘ç°: {len(nan_removed)} ä¸ªå˜é‡å› è¿ç»­ç¼ºå¤±å€¼ â‰¥ {nan_threshold_int} è¢«ç§»é™¤ï¼")
                                    print(f"   è¿™å°±æ˜¯ä¸ºä»€ä¹ˆä»76ä¸ªå˜é‡å˜æˆ{len(prepared_data.columns)}ä¸ªçš„åŸå› ï¼")
                                    print(f"   ğŸ’¡ è§£å†³æ–¹æ¡ˆ: å¢åŠ è¿ç»­NaNé˜ˆå€¼æˆ–ç¦ç”¨æ­¤åŠŸèƒ½")
                            else:
                                print(f"âœ… æ²¡æœ‰å˜é‡è¢«ç§»é™¤")

                            print(f"="*80)

                            # --- NEW: Also load var_type_map separately using load_mappings ---
                            try:
                                var_type_map, var_industry_map_loaded = load_mappings(
                                    excel_path=excel_file_like_object,
                                    sheet_name=session_state.dfm_param_type_mapping_sheet,
                                    indicator_col=DataDefaults.INDICATOR_COLUMN if CONFIG_AVAILABLE else 'é«˜é¢‘æŒ‡æ ‡',
                                    type_col=DataDefaults.TYPE_COLUMN if CONFIG_AVAILABLE else 'ç±»å‹',
                                    industry_col=DataDefaults.INDUSTRY_COLUMN if CONFIG_AVAILABLE else 'è¡Œä¸š'
                                )
                                # Store var_type_map separately in session_state
                                set_dfm_state("dfm_var_type_map_obj", var_type_map, session_state)
                                st.info(f"âœ… å·²æˆåŠŸåŠ è½½å˜é‡ç±»å‹æ˜ å°„ï¼š{len(var_type_map)} ä¸ªæ˜ å°„")
                            except Exception as e_load_maps:
                                st.warning(f"åŠ è½½å˜é‡ç±»å‹æ˜ å°„å¤±è´¥: {e_load_maps}")
                                set_dfm_state("dfm_var_type_map_obj", {}, session_state)
                            # --- END NEW ---
                            
                            # --- NEW: Store Python objects in session_state for direct use ---
                            set_dfm_state("dfm_prepared_data_df", prepared_data, session_state)
                            set_dfm_state("dfm_transform_log_obj", transform_log, session_state)
                            set_dfm_state("dfm_industry_map_obj", industry_map, session_state)
                            set_dfm_state("dfm_removed_vars_log_obj", removed_variables_detailed_log, session_state)
                            
                            st.success("æ•°æ®é¢„å¤„ç†å®Œæˆï¼ç»“æœå·²å‡†å¤‡å°±ç»ªï¼Œå¯ç”¨äºæ¨¡å‹è®­ç»ƒæ¨¡å—ã€‚")
                            st.info(f"ğŸ“Š é¢„å¤„ç†åæ•°æ®å½¢çŠ¶: {prepared_data.shape}")

                            # ğŸ”¥ æ–°å¢ï¼šåœ¨UIä¸­æ˜¾ç¤ºå˜é‡ç§»é™¤è­¦å‘Š
                            if removed_variables_detailed_log:
                                nan_removed_count = len([item for item in removed_variables_detailed_log if 'consecutive_nan' in item.get('Reason', '').lower()])
                                if nan_removed_count > 0:
                                    st.warning(f"âš ï¸ æ³¨æ„: {nan_removed_count} ä¸ªå˜é‡å› è¿ç»­ç¼ºå¤±å€¼ â‰¥ {nan_threshold_int} è¢«ç§»é™¤ï¼")
                                    st.info(f"ğŸ’¡ å¦‚éœ€ä¿ç•™æ›´å¤šå˜é‡ï¼Œå¯ä»¥å¢åŠ è¿ç»­NaNé˜ˆå€¼æˆ–ç¦ç”¨æ­¤åŠŸèƒ½")

                                    with st.expander("ğŸ” æŸ¥çœ‹è¢«ç§»é™¤çš„å˜é‡è¯¦æƒ…", expanded=False):
                                        removal_reasons = {}
                                        for item in removed_variables_detailed_log:
                                            reason = item.get('Reason', 'unknown')
                                            if reason not in removal_reasons:
                                                removal_reasons[reason] = []
                                            removal_reasons[reason].append(item.get('Variable', 'unknown'))

                                        for reason, vars_list in removal_reasons.items():
                                            st.write(f"**{reason}**: {len(vars_list)} ä¸ªå˜é‡")
                                            if 'consecutive_nan' in reason.lower():
                                                st.error(f"å› è¿ç»­ç¼ºå¤±å€¼è¿‡å¤šè¢«ç§»é™¤: {vars_list[:10]}")
                                            else:
                                                st.write(f"å˜é‡: {vars_list[:5]}")
                            # --- END NEW ---

                            # Prepare for download (existing logic)
                            set_dfm_state("dfm_processed_outputs", {
                                'base_name': session_state.dfm_export_base_name,
                                'data': None, 'industry_map': None, 'transform_log': None, 'removed_vars_log': None
                            }, session_state)
                            if prepared_data is not None:
                                session_state.dfm_processed_outputs['data'] = prepared_data.to_csv(index=True, index_label='Date', encoding='utf-8-sig').encode('utf-8-sig')
                            
                            if industry_map:
                                try:
                                    df_industry_map = pd.DataFrame(list(industry_map.items()), columns=['Indicator', 'Industry'])
                                    session_state.dfm_processed_outputs['industry_map'] = df_industry_map.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                                except Exception as e_im:
                                    st.warning(f"è¡Œä¸šæ˜ å°„è½¬æ¢åˆ°CSVæ—¶å‡ºé”™: {e_im}")
                                    session_state.dfm_processed_outputs['industry_map'] = None
                            
                            if removed_variables_detailed_log:
                                try:
                                    df_removed_log = pd.DataFrame(removed_variables_detailed_log)
                                    session_state.dfm_processed_outputs['removed_vars_log'] = df_removed_log.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                                except Exception as e_rl:
                                    st.warning(f"ç§»é™¤å˜é‡æ—¥å¿—è½¬æ¢åˆ°CSVæ—¶å‡ºé”™: {e_rl}")
                                    session_state.dfm_processed_outputs['removed_vars_log'] = None

                            # Handling transform_log (it's a dict, potentially nested)
                            if transform_log:
                                formatted_log_data = []
                                # Attempt to flatten or nicely format the transform_log for CSV
                                # This is a simplified example; actual flattening might be more complex
                                for category, entries in transform_log.items():
                                    if isinstance(entries, dict):
                                        for var, details in entries.items():
                                            if isinstance(details, dict):
                                                log_entry = {'Category': category, 'Variable': var}
                                                log_entry.update(details) # Add all sub-details
                                                formatted_log_data.append(log_entry)
                                    elif isinstance(entries, list): # e.g. for 'removed_highly_correlated_vars'
                                         for item_pair in entries:
                                            if isinstance(item_pair, (list, tuple)) and len(item_pair) == 2:
                                                formatted_log_data.append({'Category': category, 'Variable1': item_pair[0], 'Variable2': item_pair[1]})
                                            else:
                                                formatted_log_data.append({'Category': category, 'Detail': str(item_pair)})
                                
                                if formatted_log_data:
                                    try:
                                        df_transformed_log_nice = pd.DataFrame(formatted_log_data)
                                        session_state.dfm_processed_outputs['transform_log'] = df_transformed_log_nice.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                                    except Exception as e_tl:
                                        st.warning(f"è½¬æ¢æ—¥å¿—åˆ°CSVæ—¶å‡ºé”™: {e_tl}. å°†å°è¯•ä¿å­˜ä¸ºJSONå­—ç¬¦ä¸²ã€‚")
                                        try:
                                            session_state.dfm_processed_outputs['transform_log'] = json.dumps(transform_log, ensure_ascii=False, indent=4).encode('utf-8-sig')
                                        except Exception as e_json:
                                            st.warning(f"è½¬æ¢æ—¥å¿—åˆ°JSONæ—¶ä¹Ÿå‡ºé”™: {e_json}")
                                            session_state.dfm_processed_outputs['transform_log'] = None
                                else:
                                    session_state.dfm_processed_outputs['transform_log'] = None 
                                    st.info("è½¬æ¢æ—¥å¿—ä¸ºç©ºæˆ–æ ¼å¼æ— æ³•ç›´æ¥è½¬æ¢ä¸ºç®€å•CSVã€‚")
                            else:
                                session_state.dfm_processed_outputs['transform_log'] = None
                        
                        else:
                            progress_bar.progress(100)
                            status_text.text("âŒ å¤„ç†å¤±è´¥")
                            st.error("æ•°æ®é¢„å¤„ç†å¤±è´¥æˆ–æœªè¿”å›æ•°æ®ã€‚è¯·æ£€æŸ¥æ§åˆ¶å°æ—¥å¿—è·å–æ›´å¤šä¿¡æ¯ã€‚")
                            set_dfm_state("dfm_processed_outputs", None, session_state)
                            # Ensure direct data objects are also None on failure
                            set_dfm_state("dfm_prepared_data_df", None, session_state)
                            set_dfm_state("dfm_transform_log_obj", None, session_state)
                            set_dfm_state("dfm_industry_map_obj", None, session_state)
                            set_dfm_state("dfm_removed_vars_log_obj", None, session_state)
                            set_dfm_state("dfm_var_type_map_obj", None, session_state)

                        # ğŸ”¥ ä¼˜åŒ–ï¼šå®Œæˆè¿›åº¦æŒ‡ç¤ºå™¨
                        if 'progress_bar' in locals():
                            progress_bar.progress(100)
                            status_text.text("ğŸ‰ å¤„ç†å®Œæˆï¼")
                            import time
                            time.sleep(1)
                            progress_bar.empty()
                            status_text.empty()

                    except ImportError as ie:
                        st.error(f"å¯¼å…¥é”™è¯¯: {ie}. è¯·ç¡®ä¿ 'data_preparation.py' æ–‡ä»¶ä¸UIè„šæœ¬åœ¨åŒä¸€ç›®å½•ä¸‹æˆ–æ­£ç¡®å®‰è£…ã€‚")
                    except FileNotFoundError as fnfe:
                        st.error(f"æ–‡ä»¶æœªæ‰¾åˆ°é”™è¯¯: {fnfe}. è¿™å¯èƒ½ä¸ 'data_preparation.py' å†…éƒ¨çš„æ–‡ä»¶è¯»å–æœ‰å…³ã€‚")
                    except Exception as e:
                        st.error(f"è¿è¡Œæ•°æ®é¢„å¤„ç†æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                        import traceback
                        st.text_area("è¯¦ç»†é”™è¯¯ä¿¡æ¯:", traceback.format_exc(), height=200)
                        set_dfm_state("dfm_processed_outputs", None, session_state)

                except Exception as outer_e:
                    st.error(f"æ•°æ®é¢„å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {outer_e}")
                    import traceback
                    st.text_area("è¯¦ç»†é”™è¯¯ä¿¡æ¯:", traceback.format_exc(), height=200)

        # Render download buttons if data is available in session_state
        if session_state.dfm_processed_outputs:
            outputs = session_state.dfm_processed_outputs
            base_name = outputs['base_name']

            st.download_button(
                label=f"ä¸‹è½½å¤„ç†åçš„æ•°æ® ({base_name}_data_v3.csv)",
                data=outputs['data'],
                file_name=f"{base_name}_data_v3.csv",
                mime='text/csv',
                key='download_data_csv'
            )

            if outputs['industry_map']:
                st.download_button(
                    label=f"ä¸‹è½½è¡Œä¸šæ˜ å°„ ({base_name}_industry_map_v3.csv)",
                    data=outputs['industry_map'],
                    file_name=f"{base_name}_industry_map_v3.csv",
                    mime='text/csv',
                    key='download_industry_map_csv'
                )
            
            if outputs['transform_log']:
                st.download_button(
                    label=f"ä¸‹è½½è½¬æ¢æ—¥å¿— ({base_name}_transform_log_v3.csv)",
                    data=outputs['transform_log'],
                    file_name=f"{base_name}_transform_log_v3.csv",
                    mime='text/csv',
                    key='download_transform_log_csv'
                )

            if outputs['removed_vars_log']:
                st.download_button(
                    label=f"ä¸‹è½½ç§»é™¤å˜é‡æ—¥å¿— ({base_name}_removed_log_v3.csv)",
                    data=outputs['removed_vars_log'],
                    file_name=f"{base_name}_removed_log_v3.csv",
                    mime='text/csv',
                    key='download_removed_log_csv'
                )
