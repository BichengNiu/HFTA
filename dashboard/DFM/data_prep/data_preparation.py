# data_preparation.py
import pandas as pd
# import suppress_prints  # æŠ‘åˆ¶å­è¿›ç¨‹ä¸­çš„é‡å¤æ‰“å° - æš‚æ—¶æ³¨é‡Šæ‰
import numpy as np
import sys
import os
import json
from collections import Counter, defaultdict
from typing import Tuple, Dict, Optional, List, Any
import unicodedata
from statsmodels.tsa.stattools import adfuller
import io

# --- NEW: Flag for testing with reduced variables ---
USE_REDUCED_VARIABLES_FOR_TESTING = False # <<--- å…³é—­æµ‹è¯•æ¨¡å¼ï¼Œä½¿ç”¨æ‰€æœ‰å˜é‡

# --- æ§åˆ¶å¼€å…³ ---
CREATE_REDUCED_TEST_SET = False # æ”¹å› False ä»¥ä½¿ç”¨å®Œæ•´æ•°æ®é›†
REDUCED_SET_SUFFIX = "_REDUCED" if CREATE_REDUCED_TEST_SET else "" # æ ¹æ®å¼€å…³è°ƒæ•´åç¼€

# --- NEW: Function to detect target sheet format ---
def detect_sheet_format(excel_file, sheet_name: str) -> Dict[str, Any]:
    """
    Detects the format of any sheet to handle different database versions and data sources.
    Returns a dictionary with format information and reading parameters.
    
    Supports:
    - åŒèŠ±é¡ºæ ¼å¼: Row0(åºŸå¼ƒ), Row1(æŒ‡æ ‡åç§°), Row2(é¢‘ç‡), Row3(å•ä½), Row4(æŒ‡æ ‡ID), Row5+(æ•°æ®)
    - Windæ ¼å¼: Row0(åºŸå¼ƒ), Row1(æŒ‡æ ‡åç§°), Row2+(æ•°æ®)  
    - Mysteelæ ¼å¼: Row0(åºŸå¼ƒ), Row1(æŒ‡æ ‡åç§°), Row2(é¢‘åº¦), Row3(æŒ‡æ ‡æè¿°), Row4+(æ•°æ®)
    - æ—§æ ¼å¼: Row0(æŒ‡æ ‡åç§°), Row1+(æ•°æ®æˆ–å…ƒæ•°æ®)
    """
    try:
        # Read first few rows to analyze format
        df_sample = pd.read_excel(excel_file, sheet_name=sheet_name, nrows=8, header=None)
        
        if df_sample.shape[0] < 2:
            print(f"      [æ ¼å¼æ£€æµ‹] Sheet '{sheet_name}' è¡Œæ•°ä¸è¶³ï¼Œä½¿ç”¨é»˜è®¤æ ¼å¼")
            return {
                'format': 'default',
                'skiprows': None,
                'header': 0,
                'data_start_row': 1,
                'source': 'unknown'
            }
        
        # Check for new format patterns
        row_0 = df_sample.iloc[0, 0] if df_sample.shape[0] > 0 else None
        row_1 = df_sample.iloc[1, 0] if df_sample.shape[1] > 0 else None
        row_2 = df_sample.iloc[2, 0] if df_sample.shape[0] > 2 else None
        row_3 = df_sample.iloc[3, 0] if df_sample.shape[0] > 3 else None
        row_4 = df_sample.iloc[4, 0] if df_sample.shape[0] > 4 else None
        row_4_col1 = df_sample.iloc[4, 1] if df_sample.shape[0] > 4 and df_sample.shape[1] > 1 else None
        
        # åŒèŠ±é¡ºæ–°æ ¼å¼æ£€æµ‹ (æ›´ç²¾ç¡®çš„æ£€æµ‹æ¡ä»¶)
        # æ£€æŸ¥å®Œæ•´çš„5è¡Œæ ¼å¼ï¼šæŒ‡æ ‡åç§°+é¢‘ç‡+å•ä½+æŒ‡æ ‡ID+æ•°æ®
        if (row_1 == 'æŒ‡æ ‡åç§°' and row_2 == 'é¢‘ç‡' and row_3 == 'å•ä½' and row_4 == 'æŒ‡æ ‡ID' and
            str(row_4_col1) not in [None, '', 'nan'] and (str(row_4_col1).startswith('M') or str(row_4_col1).startswith('S'))):  # æ”¯æŒMå’ŒSå¼€å¤´çš„æŒ‡æ ‡ID
            print(f"      [æ ¼å¼æ£€æµ‹] æ£€æµ‹åˆ°åŒèŠ±é¡ºå®Œæ•´æ–°æ ¼å¼")
            return {
                'format': 'tonghuashun_new',
                'skiprows': [0, 2, 3, 4],  # Skip: åºŸå¼ƒè¡Œ, é¢‘ç‡, å•ä½, æŒ‡æ ‡ID
                'header': 0,               # Row 1 becomes header
                'data_start_row': 5,
                'source': 'tonghuashun'
            }
        
        # Windæ–°æ ¼å¼æ£€æµ‹ (æ£€æŸ¥row_0æ˜¯å¦ä¸ºæ•°å­—0æˆ–NaN)
        elif (row_1 == 'æŒ‡æ ‡åç§°' and (row_0 == 0 or pd.isna(row_0)) and 
              row_2 != 'é¢‘ç‡' and row_2 != 'é¢‘åº¦' and 
              not pd.isna(row_2)):  # row_2æ˜¯å®é™…æ•°æ®ï¼Œä¸æ˜¯å…ƒæ•°æ®
            print(f"      [æ ¼å¼æ£€æµ‹] æ£€æµ‹åˆ°Windæ–°æ ¼å¼")
            return {
                'format': 'wind_new',
                'skiprows': [0],           # Skip: åºŸå¼ƒè¡Œ
                'header': 0,               # Row 1 becomes header
                'data_start_row': 2,
                'source': 'wind'
            }
        
        # Mysteelæ–°æ ¼å¼æ£€æµ‹ (æ£€æŸ¥æ˜¯å¦ä»¥"é’¢è”æ•°æ®"å¼€å¤´)
        elif (row_0 == 'é’¢è”æ•°æ®' and row_1 == 'æŒ‡æ ‡åç§°' and row_2 == 'é¢‘åº¦'):
            print(f"      [æ ¼å¼æ£€æµ‹] æ£€æµ‹åˆ°Mysteelæ–°æ ¼å¼")
            return {
                'format': 'mysteel_new',
                'skiprows': [0, 2, 3],     # Skip: åºŸå¼ƒè¡Œ, é¢‘åº¦, æŒ‡æ ‡æè¿°
                'header': 0,               # Row 1 becomes header
                'data_start_row': 4,
                'source': 'mysteel'
            }
        
        # æ—§æ ¼å¼æ£€æµ‹
        elif row_0 == 'æŒ‡æ ‡åç§°':
            print(f"      [æ ¼å¼æ£€æµ‹] æ£€æµ‹åˆ°æ—§æ ¼å¼")
            # æ£€æµ‹æ˜¯å¦æœ‰æŒ‡æ ‡IDè¡Œéœ€è¦è·³è¿‡
            if (df_sample.shape[0] > 1 and 
                str(df_sample.iloc[1, 0]).startswith('S') and len(str(df_sample.iloc[1, 0])) > 5):
                return {
                    'format': 'old_with_id',
                    'skiprows': [1],       # Skip: æŒ‡æ ‡IDè¡Œ
                    'header': 0,
                    'data_start_row': 2,
                    'source': 'old'
                }
            else:
                return {
                    'format': 'old_direct',
                    'skiprows': None,
                    'header': 0,
                    'data_start_row': 1,
                    'source': 'old'
                }
        
        # å¦‚æœéƒ½ä¸åŒ¹é…ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥
        else:
            print(f"      [æ ¼å¼æ£€æµ‹] æœªè¯†åˆ«çš„æ ¼å¼ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥")
            print(f"      [Debug] Row 0: {row_0}, Row 1: {row_1}, Row 2: {row_2}")
            return {
                'format': 'default',
                'skiprows': None,
                'header': 0,
                'data_start_row': 1,
                'source': 'unknown'
            }
            
    except Exception as e:
        print(f"      [æ ¼å¼æ£€æµ‹] æ£€æµ‹æ ¼å¼æ—¶å‡ºé”™: {e}ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
        return {
            'format': 'error',
            'skiprows': None,
            'header': 0,
            'data_start_row': 1,
            'source': 'error'
        }

# --- Backward compatibility function ---
def detect_target_sheet_format(excel_file, sheet_name: str) -> Dict[str, Any]:
    """
    Backward compatibility wrapper for detect_sheet_format.
    """
    return detect_sheet_format(excel_file, sheet_name)

# --- NEW: Helper function to parse frequency and industry from sheet name ---
def parse_sheet_info(sheet_name: str, target_sheet_name: str) -> Dict[str, Optional[str]]:
    """
    Parses sheet names like 'è¡Œä¸š_é¢‘ç‡_æ•°æ®æ¥æº' or handles target sheet.
    Returns a dictionary with 'industry', 'freq_type', 'source'.
    Frequency types: 'daily', 'weekly', 'monthly', None.
    Special freq_type: 'monthly_target' if sheet_name matches target_sheet_name.
    """
    # +++ è°ƒè¯•æ‰“å° +++
    # print(f"    [Debug parse_sheet_info] sheet_name='{sheet_name}', target_sheet_name='{target_sheet_name}'")
    # +++ ç»“æŸè°ƒè¯• +++
    info: Dict[str, Optional[str]] = {'industry': None, 'freq_type': None, 'source': None}
    if not isinstance(sheet_name, str):
        return info

    # --- ä¿®æ”¹ï¼šè¿›è¡Œå¤§å°å†™ä¸æ•æ„Ÿçš„æ¯”è¾ƒï¼Œå¹¶ç¡®ä¿ target_sheet_name ä¹Ÿæ˜¯å­—ç¬¦ä¸² ---\
    is_target_sheet = False
    if isinstance(target_sheet_name, str):
        if sheet_name.lower() == target_sheet_name.lower():
            is_target_sheet = True

    if is_target_sheet:
    # --- ç»“æŸä¿®æ”¹ ---\
         # +++ è°ƒè¯•æ‰“å° +++\n         # print(f"      [Debug parse_sheet_info] Matched target sheet!")\n         # +++ ç»“æŸè°ƒè¯• +++\n         info['freq_type'] = 'monthly_target'
         # å°è¯•ä»ç›®æ ‡åç§°æå–è¡Œä¸š (å¯é€‰)
         parts_target = sheet_name.split('_')
         if len(parts_target) > 0:
             # å‡è®¾ç¬¬ä¸€ä¸ªéƒ¨åˆ†æ˜¯è¡Œä¸šç›¸å…³
             industry_part = parts_target[0].replace('-æœˆåº¦', '').replace('_æœˆåº¦','').strip() # åŒæ—¶æ›¿æ¢ - å’Œ _
             info['industry'] = industry_part if industry_part else 'Macro' # é»˜è®¤ä¸º Macro
         else:
             info['industry'] = 'Macro' # é»˜è®¤
         return info
    # --- ç»“æŸç›®æ ‡ Sheet æ£€æŸ¥ ---

    # --- é€šç”¨æ ¼å¼è§£æ ---
    parts = sheet_name.split('_')
    if len(parts) >= 2: # è‡³å°‘éœ€è¦ è¡Œä¸š_é¢‘ç‡
        info['industry'] = parts[0].strip()
        freq_part = parts[1].strip()
        if freq_part == 'æ—¥åº¦':
            info['freq_type'] = 'daily'
        elif freq_part == 'å‘¨åº¦':
            info['freq_type'] = 'weekly'
        elif freq_part == 'æœˆåº¦':
            # We handle monthly predictors separately if they come from the target sheet
            # This type is for *other* monthly predictor sheets, if any exist.
            info['freq_type'] = 'monthly_predictor' # <--- ç¡®ä¿è¯†åˆ«å…¶ä»–å¯èƒ½çš„æœˆåº¦Sheet
        # Add other potential frequencies if needed

        if len(parts) >= 3:
            info['source'] = '_'.join(parts[2:]).strip() # å…è®¸æ¥æºåŒ…å«ä¸‹åˆ’çº¿

    # å¦‚æœæœªèƒ½è§£æå‡ºè¡Œä¸šï¼Œç»™ä¸ªé»˜è®¤å€¼
    if info['industry'] is None and info['freq_type'] is not None:
         info['industry'] = "Uncategorized"

    return info

# --- NEW: Function to load mappings ---
def load_mappings(
    excel_path: str,
    sheet_name: str,
    indicator_col: str = 'é«˜é¢‘æŒ‡æ ‡',
    type_col: str = 'ç±»å‹',
    industry_col: Optional[str] = 'è¡Œä¸š' # Industry column is optional
) -> Tuple[Dict[str, str], Dict[str, str]]:
    """
    Loads variable type and industry mappings from a specified sheet in an Excel file.
    Normalizes indicator names (keys) to lowercase NFKC.
    (Implementation remains the same as before)
    """
    var_type_map = {}
    var_industry_map = {}
    print(f"\n--- [Mappings] Loading type/industry maps from: ")
    print(f"    Excel: {excel_path}")
    print(f"    Sheet: {sheet_name}")
    print(f"    Indicator Col: '{indicator_col}', Type Col: '{type_col}', Industry Col: '{industry_col}'")

    try:
        excel_file_obj = pd.ExcelFile(excel_path)
        if sheet_name not in excel_file_obj.sheet_names:
             raise FileNotFoundError(f"Sheet '{sheet_name}' not found in '{excel_path}'")

        indicator_sheet = pd.read_excel(excel_file_obj, sheet_name=sheet_name)

        # Normalize column names
        indicator_sheet.columns = indicator_sheet.columns.str.strip()
        indicator_col = indicator_col.strip()
        type_col = type_col.strip()
        if industry_col:
            industry_col = industry_col.strip()

        # Check required columns exist
        if indicator_col not in indicator_sheet.columns or type_col not in indicator_sheet.columns:
            raise ValueError(f"æœªæ‰¾åˆ°å¿…éœ€çš„åˆ— '{indicator_col}' æˆ– '{type_col}' åœ¨ sheet '{sheet_name}'")

        # Create Type Map
        var_type_map_temp = pd.Series(
            indicator_sheet[type_col].astype(str).str.strip().values,
            index=indicator_sheet[indicator_col].astype(str).str.strip()
        ).to_dict()
        # Normalize keys and filter NaNs/empty strings
        var_type_map = {unicodedata.normalize('NFKC', str(k)).strip().lower(): str(v).strip()
                        for k, v in var_type_map_temp.items()
                        if pd.notna(k) and str(k).strip().lower() not in ['', 'nan']
                        and pd.notna(v) and str(v).strip().lower() not in ['', 'nan']}
        print(f"  [Mappings] Successfully created type map with {len(var_type_map)} entries.")

        # Create Industry Map (optional)
        if industry_col and industry_col in indicator_sheet.columns:
            industry_map_temp = pd.Series(
                indicator_sheet[industry_col].astype(str).str.strip().values,
                index=indicator_sheet[indicator_col].astype(str).str.strip() # Use indicator_col for index
            ).to_dict()
            # Normalize keys and filter NaNs/empty strings
            var_industry_map = {unicodedata.normalize('NFKC', str(k)).strip().lower(): str(v).strip()
                                for k, v in industry_map_temp.items()
                                if pd.notna(k) and str(k).strip().lower() not in ['', 'nan']
                                and pd.notna(v) and str(v).strip().lower() not in ['', 'nan']}
            print(f"  [Mappings] Successfully created industry map with {len(var_industry_map)} entries.")
        elif industry_col:
             print(f"  [Mappings] Warning: Industry column '{industry_col}' not found in sheet '{sheet_name}'. Industry map will be empty.")
        else:
             print(f"  [Mappings] Industry column not specified. Industry map will be empty.")

    except FileNotFoundError as e:
        print(f"Error loading mappings: {e}")
        # Return empty maps on file/sheet not found
    except ValueError as e:
        print(f"Error processing mapping sheet: {e}")
        # Return empty maps on column errors
    except Exception as e:
        print(f"An unexpected error occurred while loading mappings: {e}")
        # Return empty maps on other errors

    print(f"--- [Mappings] Loading finished. Type map size: {len(var_type_map)}, Industry map size: {len(var_industry_map)} ---")
    return var_type_map, var_industry_map

# --- REVISED: _ensure_stationarity function ---
def _ensure_stationarity(
    df: pd.DataFrame,
    skip_cols: Optional[set] = None, # <-- Added: skip columns
    adf_p_threshold: float = 0.05
) -> Tuple[pd.DataFrame, Dict, Dict]:
    """
    å†…éƒ¨å‡½æ•°ï¼šæ£€æŸ¥å¹¶è½¬æ¢ DataFrame ä¸­çš„å˜é‡ä»¥è¾¾åˆ°å¹³ç¨³æ€§ã€‚
    æ–°å¢: å¯ä»¥é€šè¿‡ skip_cols å‚æ•°æŒ‡å®šä¸è¿›è¡Œæ£€æŸ¥çš„åˆ—ã€‚
    ğŸ”¥ ä¼˜åŒ–ï¼šæ·»åŠ æ‰¹é‡å¤„ç†å’Œç¼“å­˜æœºåˆ¶ä»¥æé«˜æ€§èƒ½ã€‚

    å¤„ç†é€»è¾‘:
    1. å¯¹ df ä¸­çš„æ¯ä¸€åˆ—è¿›è¡Œå¤„ç† (é™¤éåœ¨ skip_cols ä¸­æŒ‡å®š)ã€‚
    2. è¿›è¡Œ ADF æ£€éªŒ (Level)ã€‚
    3. å¦‚æœå¹³ç¨³ (p < adf_p_threshold)ï¼Œä¿ç•™ levelã€‚
    4. å¦‚æœä¸å¹³ç¨³ï¼Œè®¡ç®—ä¸€é˜¶å·®åˆ†ï¼Œå†æ¬¡æ£€éªŒã€‚
    5. å¦‚æœå·®åˆ†åå¹³ç¨³ï¼Œä½¿ç”¨å·®åˆ†åºåˆ—ã€‚
    6. å¦‚æœå·®åˆ†åä»ä¸å¹³ç¨³ï¼Œä»ä½¿ç”¨å·®åˆ†åºåˆ—ï¼Œä½†æ ‡è®°çŠ¶æ€ã€‚
    7. ç§»é™¤åŸå§‹ä¸ºç©º/å¸¸é‡çš„åˆ—ï¼Œæˆ–å·®åˆ†åä¸ºç©º/å¸¸é‡çš„åˆ—ã€‚
    8. è¢«è·³è¿‡çš„åˆ—ç›´æ¥ä¿ç•™åŸå§‹å€¼ã€‚
    """
    print(f"\n--- [Stationarity Check] å¼€å§‹æ£€æŸ¥å’Œè½¬æ¢å¹³ç¨³æ€§ (ADF p<{adf_p_threshold}) --- ")
    print(f"  ğŸ”§ ä¼˜åŒ–æ¨¡å¼ï¼šæ‰¹é‡å¤„ç† {len(df.columns)} ä¸ªå˜é‡")

    transformed_data = pd.DataFrame(index=df.index)
    transform_log = {}
    removed_cols_info = defaultdict(list)

    # --- MODIFICATION: Normalize skip_cols for reliable matching ---
    skip_cols_normalized = set()
    if skip_cols:
        skip_cols_normalized = {unicodedata.normalize('NFKC', str(c)).strip().lower() for c in skip_cols}
        print(f"    [Stationarity Check] æ ‡å‡†åŒ–åçš„è·³è¿‡åˆ—è¡¨ (é¦–5é¡¹): {list(skip_cols_normalized)[:5]}")
    # --- END MODIFICATION ---

    for col in df.columns:
        # --- MODIFICATION: Check against normalized skip_cols ---
        col_normalized = unicodedata.normalize('NFKC', str(col)).strip().lower()
        # print(f"    [Debug Stationarity] Checking col: '{col}' (Normalized: '{col_normalized}') against skip list...") # Debug print
        if col_normalized in skip_cols_normalized:
            transformed_data[col] = df[col].copy() # Ensure we copy the data
            transform_log[col] = {'status': 'skipped_by_request'}
            print(f"    - {col}: æ ¹æ®è¯·æ±‚è·³è¿‡å¹³ç¨³æ€§æ£€æŸ¥ (åŒ¹é…åˆ°è§„èŒƒåŒ–åç§° '{col_normalized}').")
            continue
        # --- END MODIFICATION ---

        series = df[col]
        series_dropna = series.dropna()

        # --- NEW DEBUG for specific columns ---
        # debug_cols = ['ä¸­å›½ï¼šå¯å†ç”Ÿèƒ½æºï¼šå‘ç”µé‡ï¼ˆæœˆï¼‰', 'ä¸­å›½ï¼šç«åŠ›å‘ç”µï¼šå‘ç”µé‡ï¼ˆæœˆï¼‰']
        # if col in debug_cols:
        #     print(f"      [DEBUG _ensure_stationarity] Processing column: {col}")
        #     print("        Original series (head):")
        #     print(series.head())
        #     print("        Original series (tail):")
        #     print(series.tail())
        #     print(f"        Original series length: {len(series)}")
        #     print("        Series after dropna() (head):")
        #     print(series_dropna.head())
        #     print("        Series after dropna() (tail):")
        #     print(series_dropna.tail())
        #     print(f"        Series after dropna() length: {len(series_dropna)}")
        # --- END NEW DEBUG ---

        if series_dropna.empty:
            transform_log[col] = {'status': 'skipped_empty'}
            removed_cols_info['skipped_empty'].append(col)
            print(f"    - {col}: æ•°æ®ä¸ºç©ºæˆ–å…¨ä¸º NaNï¼Œå·²ç§»é™¤.")
            continue

        if series_dropna.nunique() == 1:
            transform_log[col] = {'status': 'skipped_constant'}
            removed_cols_info['skipped_constant'].append(col)
            print(f"    - {col}: åˆ—ä¸ºå¸¸é‡ï¼Œå·²ç§»é™¤.")
            continue

        original_pval = np.nan
        diff_pval = np.nan
        try:
            adf_result_level = adfuller(series_dropna)
            original_pval = adf_result_level[1]

            if original_pval < adf_p_threshold:
                transformed_data[col] = series
                transform_log[col] = {'status': 'level', 'original_pval': original_pval}
                # ğŸ”¥ ä¼˜åŒ–ï¼šå‡å°‘è¯¦ç»†è¾“å‡ºï¼Œåªåœ¨å¿…è¦æ—¶æ‰“å°
                # print(f"    - {col}: Level å¹³ç¨³ (p={original_pval:.3f}), ä¿ç•™ Level.")
            else:
                # --- MODIFIED: Try Log Difference First ---
                series_orig = series # Keep original series
                series_transformed = None
                transform_type = 'diff' # Default to simple diff

                # Check if log difference is possible (all positive values)
                if (series_dropna > 0).all():
                    try:
                        series_transformed = np.log(series_orig).diff(1)
                        transform_type = 'log_diff'
                        # ğŸ”¥ ä¼˜åŒ–ï¼šå‡å°‘è¯¦ç»†è¾“å‡º
                        # print(f"    - {col}: Level ä¸å¹³ç¨³ (p={original_pval:.3f}), å°è¯•å¯¹æ•°å·®åˆ†...")
                    except Exception as e_log:
                         print(f"    - {col}: å¯¹æ•°å·®åˆ†å‡ºé”™: {e_log}. å›é€€åˆ°æ™®é€šå·®åˆ†ã€‚")
                         series_transformed = series_orig.diff(1) # Fallback
                         transform_type = 'diff'
                else:
                    # print(f"    - {col}: Level ä¸å¹³ç¨³ (p={original_pval:.3f}), åŒ…å«éæ­£å€¼ï¼Œä½¿ç”¨æ™®é€šä¸€é˜¶å·®åˆ†ã€‚")
                    series_transformed = series_orig.diff(1)
                    transform_type = 'diff'
                # --- END MODIFICATION ---

                series_transformed_dropna = series_transformed.dropna()

                # Check transformed series for empty or constant
                if series_transformed_dropna.empty:
                     transform_log[col] = {'status': f'skipped_{transform_type}_empty', 'original_pval': original_pval} # Use dynamic status
                     removed_cols_info[f'skipped_{transform_type}_empty'].append(col)
                     print(f"    - {col}: {transform_type.capitalize()} åä¸ºç©ºï¼Œå·²ç§»é™¤.") # Use dynamic message
                     continue
                if series_transformed_dropna.nunique() == 1:
                     transform_log[col] = {'status': f'skipped_{transform_type}_constant', 'original_pval': original_pval} # Use dynamic status
                     removed_cols_info[f'skipped_{transform_type}_constant'].append(col)
                     print(f"    - {col}: {transform_type.capitalize()} åä¸ºå¸¸é‡ï¼Œå·²ç§»é™¤.") # Use dynamic message
                     continue

                # Perform ADF test on the transformed series
                try:
                    adf_result_transformed = adfuller(series_transformed_dropna)
                    diff_pval = adf_result_transformed[1] # Store p-value from transformed series

                    transformed_data[col] = series_transformed # Assign the transformed series

                    if diff_pval < adf_p_threshold:
                        # Use dynamic status based on transform_type
                        transform_log[col] = {'status': transform_type, 'original_pval': original_pval, 'diff_pval': diff_pval}
                        print(f"    - {col}: {transform_type.capitalize()} åå¹³ç¨³ (p={diff_pval:.3f}), ä½¿ç”¨ {transform_type.capitalize()}.")
                    else:
                        # Use dynamic status based on transform_type
                        transform_log[col] = {'status': f'{transform_type}_still_nonstat', 'original_pval': original_pval, 'diff_pval': diff_pval}
                        print(f"    - {col}: {transform_type.capitalize()} åä»ä¸å¹³ç¨³ (p={diff_pval:.3f}), ä½¿ç”¨ {transform_type.capitalize()}.")

                except Exception as e_diff:
                    print(f"    - {col}: å¯¹ {transform_type.capitalize()} åºåˆ— ADF æ£€éªŒå‡ºé”™: {e_diff}. ä¿ç•™ {transform_type.capitalize()} åºåˆ—.")
                    transformed_data[col] = series_transformed # Keep transformed series even if test fails
                    transform_log[col] = {'status': f'{transform_type}_test_error', 'original_pval': original_pval}

        except Exception as e_level:
            print(f"    - {col}: Level ADF æ£€éªŒæˆ–å¤„ç†æ—¶å‡ºé”™: {e_level}. ä¿ç•™ Level (ä¸æ¨è). ")
            transformed_data[col] = series
            transform_log[col] = {'status': 'level_test_error'}

    print(f"--- [Stationarity Check] æ£€æŸ¥å’Œè½¬æ¢å®Œæˆ. è¾“å‡º Shape: {transformed_data.shape} ---")
    total_removed = sum(len(v) for v in removed_cols_info.values())
    if total_removed > 0:
        print(f"  [!] å…±ç§»é™¤äº† {total_removed} ä¸ªå˜é‡:")
        for reason, cols in removed_cols_info.items():
             if cols:
                 print(f"      - å›  '{reason}' ç§»é™¤ ({len(cols)} ä¸ª): {', '.join(cols[:5])}{'...' if len(cols)>5 else ''}")

    return transformed_data, transform_log, removed_cols_info

# --- NEW: Function to apply predefined stationarity transforms --- 
def apply_stationarity_transforms(
    data: pd.DataFrame,
    transform_rules: Dict[str, Dict[str, Any]] # é¢„æœŸæ˜¯ {var_name: {'status': 'level'/'diff'/'log_diff'/...}}
) -> pd.DataFrame:
    """
    æ ¹æ®æä¾›çš„è§„åˆ™å­—å…¸å¯¹ DataFrame ä¸­çš„å˜é‡åº”ç”¨å¹³ç¨³æ€§è½¬æ¢ã€‚
    å¦‚æœæŸä¸ªå˜é‡åœ¨è§„åˆ™å­—å…¸ä¸­æ‰¾ä¸åˆ°ï¼Œåˆ™ä¿ç•™å…¶åŸå§‹å€¼ã€‚

    Args:
        data: åŒ…å«åŸå§‹ï¼ˆæˆ–é¢„å¤„ç†ï¼‰æ•°æ®çš„ DataFrameã€‚
        transform_rules: ä¸€ä¸ªå­—å…¸ï¼Œé”®æ˜¯å˜é‡åï¼Œå€¼æ˜¯åŒ…å«è½¬æ¢çŠ¶æ€çš„å­—å…¸ï¼Œ
                         ä¾‹å¦‚ {'status': 'level'}, {'status': 'diff'}, {'status': 'log_diff'}ã€‚
                         è¿™ä¸ªå­—å…¸é€šå¸¸æ¥è‡ª _ensure_stationarity çš„è¾“å‡ºæˆ–æ¨¡å‹å…ƒæ•°æ®ã€‚

    Returns:
        ä¸€ä¸ªåŒ…å«åº”ç”¨è½¬æ¢åæ•°æ®çš„æ–°çš„ DataFrameï¼ŒåŒ…å«æ‰€æœ‰åŸå§‹åˆ—ã€‚
    """
    print(f"\n--- [Apply Stationarity V2] å¼€å§‹æ ¹æ®æä¾›çš„è§„åˆ™åº”ç”¨å¹³ç¨³æ€§è½¬æ¢ ---")
    transformed_data = pd.DataFrame(index=data.index) # åˆå§‹åŒ–ç©ºçš„ DataFrame
    applied_count = 0
    level_kept_count = 0 # è®¡æ•°å™¨ï¼šä¿ç•™ Level (åŒ…æ‹¬æ— è§„åˆ™æˆ–è§„åˆ™ä¸º level)
    error_count = 0

    # --- éå†è¾“å…¥æ•°æ®çš„æ¯ä¸€åˆ— ---
    for col in data.columns:
        rule_info = transform_rules.get(col, None)
        status = 'level' # é»˜è®¤ä¿ç•™ Level

        if rule_info and isinstance(rule_info, dict) and 'status' in rule_info:
            # å¦‚æœæ‰¾åˆ°æœ‰æ•ˆè§„åˆ™ï¼Œåˆ™ä½¿ç”¨è§„åˆ™ä¸­çš„ status
            status = rule_info['status'].lower()
            # print(f"    - {col}: æ‰¾åˆ°è§„åˆ™ '{status}'.") # Debugging
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è§„åˆ™æˆ–è§„åˆ™æ— æ•ˆï¼ŒçŠ¶æ€ä¿æŒä¸º 'level'
            # print(f"    - {col}: æœªæ‰¾åˆ°è§„åˆ™æˆ–è§„åˆ™æ— æ•ˆï¼Œä¿ç•™ Level.") # Debugging
            pass

        try:
            series = data[col]
            if status == 'diff':
                transformed_data[col] = series.diff(1)
                applied_count += 1
            elif status == 'log_diff':
                # æ£€æŸ¥æ˜¯å¦æœ‰éæ­£å€¼
                if (series <= 0).any():
                    print(f"    è­¦å‘Š: å˜é‡ '{col}' åŒ…å«éæ­£å€¼ï¼Œæ— æ³•åº”ç”¨ 'log_diff'ã€‚å°†å°è¯•æ™®é€š 'diff'ã€‚")
                    transformed_data[col] = series.diff(1)
                    status = 'diff_fallback' # æ ‡è®°å®é™…æ“ä½œ
                    error_count += 1 # ç®—ä½œä¸€ä¸ªéœ€è¦æ³¨æ„çš„æƒ…å†µ
                else:
                    transformed_data[col] = np.log(series).diff(1)
                    applied_count += 1
            else: # status == 'level' æˆ– å…¶ä»–æœªçŸ¥/è·³è¿‡çŠ¶æ€
                transformed_data[col] = series.copy() # ä¿ç•™åŸå§‹åºåˆ—
                level_kept_count += 1

        except Exception as e:
            print(f"    é”™è¯¯: åº”ç”¨è§„åˆ™ '{status}' åˆ°å˜é‡ '{col}' æ—¶å‡ºé”™: {e}. å°†ä¿ç•™åŸåºåˆ—ã€‚")
            transformed_data[col] = data[col].copy() # å‡ºé”™æ—¶ä¿ç•™åŸåºåˆ—
            error_count += 1
            level_kept_count += 1 # å‡ºé”™ä¹Ÿç®—ä¿ç•™äº† Level

    print(f"--- [Apply Stationarity V2] è½¬æ¢åº”ç”¨å®Œæˆ. ---")
    print(f"    æˆåŠŸåº”ç”¨ 'diff'/'log_diff': {applied_count} ä¸ªå˜é‡")
    print(f"    ä¿ç•™ Level (æ— è§„åˆ™æˆ–è§„åˆ™æŒ‡ç¤º): {level_kept_count} ä¸ªå˜é‡")
    print(f"    è½¬æ¢æ—¶å‡ºé”™/å›é€€ (ä¿ç•™ Level æˆ–åº”ç”¨ Diff): {error_count} ä¸ªå˜é‡")
    print(f"    è¾“å…¥ Shape: {data.shape}, è¾“å‡º Shape: {transformed_data.shape}")

    # ç§»é™¤è½¬æ¢åå…¨ä¸º NaN çš„åˆ— (è¿™é€šå¸¸åªå‘ç”Ÿåœ¨å·®åˆ†åçš„ç¬¬ä¸€è¡Œï¼Œç†è®ºä¸Šä¸åº”ç§»é™¤æ•´ä¸ªåˆ—)
    # ä½†ä¸ºäº†å®‰å…¨èµ·è§ï¼Œä¿ç•™æ­¤æ£€æŸ¥
    all_nan_cols = transformed_data.columns[transformed_data.isnull().all()].tolist()
    if all_nan_cols:
        print(f"    è­¦å‘Šï¼šä»¥ä¸‹åˆ—åœ¨è½¬æ¢åå…¨ä¸º NaNï¼Œå°†è¢«ç§»é™¤: {all_nan_cols}")
        transformed_data = transformed_data.drop(columns=all_nan_cols)
        print(f"    ç§»é™¤å…¨ NaN åˆ—å Shape: {transformed_data.shape}")

    # ç¡®ä¿è¾“å‡ºåŒ…å«æ‰€æœ‰åŸå§‹åˆ—ï¼ˆå³ä½¿è½¬æ¢å¤±è´¥ä¹Ÿä¿ç•™åŸåˆ—ï¼‰
    if set(transformed_data.columns) != set(data.columns):
         print("    è­¦å‘Šï¼šè¾“å‡ºåˆ—ä¸è¾“å…¥åˆ—ä¸å®Œå…¨åŒ¹é…ï¼æ­£åœ¨å°è¯•é‡æ–°å¯¹é½...")
         transformed_data = transformed_data.reindex(columns=data.columns, fill_value=np.nan) # å¯èƒ½éœ€è¦æ›´å¤æ‚çš„å¡«å……

    return transformed_data

# --- REVISED: prepare_data function (V3 Logic) ---
def prepare_data(
    excel_path: str,
    target_freq: str,
    target_sheet_name: str,
    target_variable_name: str, # Keep this as input for initial identification
    consecutive_nan_threshold: Optional[int] = None,
    data_start_date: Optional[str] = None,
    data_end_date: Optional[str] = None,
    reference_sheet_name: str = 'æŒ‡æ ‡ä½“ç³»',
    reference_column_name: str = 'é«˜é¢‘æŒ‡æ ‡'
) -> Tuple[Optional[pd.DataFrame], Optional[Dict], Optional[Dict], Optional[List[Dict]]]:
    """
    Loads data, performs stationarity checks at appropriate frequencies (monthly vars
    at monthly freq), aligns all data to target_freq (weekly), performs NaN checks
    (skipping monthly vars), and weekly stationarity checks (skipping monthly vars).

    Args:
        (Parameters remain the same)

    Returns:
        Tuple[Optional[pd.DataFrame], Optional[Dict], Optional[Dict], Optional[List[Dict]]]:
            - Final aligned weekly data (DataFrame).
            - Variable-to-industry mapping (Dict).
            - Combined transformation log (Dict).
            - Detailed removal log (List[Dict]).
    """
    print(f"\n--- [Data Prep V3] å¼€å§‹åŠ è½½å’Œå¤„ç†æ•°æ® (ç›®æ ‡é¢‘ç‡: {target_freq}) ---")
    if CREATE_REDUCED_TEST_SET:
        print("  [!] å·²å¯ç”¨ç¼©å°ç‰ˆæµ‹è¯•é›†ç”Ÿæˆæ¨¡å¼ã€‚")

    if not target_freq.upper().endswith('-FRI'):
        print(f"é”™è¯¯: [Data Prep] å½“å‰ç›®æ ‡å¯¹é½é€»è¾‘ä»…æ”¯æŒå‘¨äº” (W-FRI)ã€‚æä¾›çš„ç›®æ ‡é¢‘ç‡ '{target_freq}' æ— æ•ˆã€‚")
        return None, None, None, None

    # Use original target name for clarity in logs, but read actual B col name later
    print(f"  [Data Prep] ç›®æ ‡ Sheet: '{target_sheet_name}', ç›®æ ‡å˜é‡å(é¢„æœŸBåˆ—): '{target_variable_name}'")

    var_industry_map = {}
    raw_columns_across_all_sheets = set() # Track normalized names of ALL predictors loaded
    reference_predictor_variables = set()
    target_sheet_cols = set() # Track ORIGINAL names from target sheet (B col + C+ cols)

    try:
        excel_file = pd.ExcelFile(excel_path)
        available_sheets = excel_file.sheet_names
        print(f"  [Data Prep] Excel æ–‡ä»¶ä¸­å¯ç”¨çš„ Sheets: {available_sheets}")

        # --- Load Reference Variables (same as before) ---
        if reference_sheet_name in available_sheets:
            try:
                ref_df = pd.read_excel(excel_file, sheet_name=reference_sheet_name)
                ref_df.columns = ref_df.columns.str.strip()
                clean_reference_column_name = reference_column_name.strip()
                if clean_reference_column_name in ref_df.columns:
                    raw_reference_vars = (
                        ref_df[clean_reference_column_name]
                        .astype(str).str.strip().replace('nan', np.nan).dropna().unique()
                    )
                    raw_reference_vars = [v for v in raw_reference_vars if v]
                    reference_predictor_variables = set(
                        unicodedata.normalize('NFKC', var).strip().lower()
                        for var in raw_reference_vars
                    )
                    print(f"  [Data Prep] ä» '{reference_sheet_name}' åŠ è½½å¹¶è§„èŒƒåŒ–äº† {len(reference_predictor_variables)} ä¸ªå‚è€ƒå˜é‡åã€‚")
                else:
                    print(f"  [Data Prep] è­¦å‘Š: åœ¨ '{reference_sheet_name}' æœªæ‰¾åˆ°å‚è€ƒåˆ— '{clean_reference_column_name}'ã€‚")
            except Exception as e_ref:
                print(f"  [Data Prep] è­¦å‘Š: è¯»å–å‚è€ƒ Sheet '{reference_sheet_name}' å‡ºé”™: {e_ref}ã€‚")
        else:
             print(f"  [Data Prep] è­¦å‘Š: æœªæ‰¾åˆ°å‚è€ƒ Sheet '{reference_sheet_name}'ã€‚")

        data_parts = defaultdict(list)
        # --- Variables for monthly data processing --- 
        publication_dates_from_target = None
        raw_target_values = None # <-- Separate target Series
        # df_all_monthly_predictors_pubdate = pd.DataFrame() # OLD: Combine ALL non-target monthly predictors here
        df_other_monthly_predictors_pubdate = pd.DataFrame() # <-- NEW: Combine only predictors from OTHER monthly sheets
        df_target_sheet_predictors_pubdate = pd.DataFrame() # <-- NEW: Store predictors from target sheet separately
        target_sheet_industry_name = "Macro"
        actual_target_variable_name = None # Will store the actual name from Col B
        # monthly_predictor_cols_original = set() # <-- Track original names from predictor monthlies - Handled differently now
        target_sheet_cols = set() # Keep this to track target sheet columns for skipping later

        # --- Step 1: Load Data by Frequency ---
        print("\n--- [Data Prep V3] æ­¥éª¤ 1: åŠ è½½æ•°æ® ---")
        for sheet_name in available_sheets:
            print(f"    [Data Prep] æ­£åœ¨æ£€æŸ¥ Sheet: {sheet_name}...")
            is_target_sheet = (sheet_name == target_sheet_name)
            sheet_info = parse_sheet_info(sheet_name, target_sheet_name)
            freq_type = sheet_info['freq_type']
            industry_name = sheet_info['industry'] if sheet_info['industry'] else "Uncategorized"

            # 1a: Handle Target Sheet (Extract Monthly Data)
            if is_target_sheet:
                print(f"      æ£€æµ‹åˆ°ç›®æ ‡ Sheet ('{freq_type}', è¡Œä¸š: '{industry_name}')...")
                target_sheet_industry_name = industry_name
                try:
                    # --- NEW: Detect target sheet format and use appropriate parameters ---
                    format_info = detect_target_sheet_format(excel_file, sheet_name)
                    read_header = format_info['header']
                    skip_rows = format_info['skiprows']
                    # ç›®æ ‡Sheeté»˜è®¤ä¸è®¾index_colï¼Œæ—¥æœŸåœ¨åç»­å¤„ç†
                    print(f"    [ç›®æ ‡Sheetè¯»å–å‚æ•°] æ ¼å¼: {format_info['format']}, header={read_header}, skiprows={skip_rows}")
                    # --- Read with detected format parameters ---
                    df_raw = pd.read_excel(excel_file, sheet_name=sheet_name, header=read_header, skiprows=skip_rows)
                    # --- END NEW ---
                    # --- <<< æ–°å¢ï¼šåœ¨æ­£ç¡®ä½ç½®æ‰“å°æ›´å¤šåŸå§‹è¯»å–ä¿¡æ¯ >>> ---
                    print("      [Debug Target Raw Read - FULL] åŸå§‹è¯»å–çš„ç›®æ ‡ Sheet (å‰ 5 è¡Œ):")
                    print(df_raw.head())
                    print("      [Debug Target Raw Read - FULL] åŸå§‹è¯»å–çš„ç›®æ ‡ Sheet (å 5 è¡Œ):")
                    print(df_raw.tail())
                    print(f"      [Debug Target Raw Read - FULL] åŸå§‹è¯»å–çš„ Shape: {df_raw.shape}")
                    # --- <<< ç»“æŸæ–°å¢ >>> ---

                    # --- <<< æ–°å¢ï¼šå°† 0 å€¼æ›¿æ¢ä¸º NaN >>> ---
                    df_raw = df_raw.replace(0, np.nan)
                    print(f"      [å¤„ç†] å°†ç›®æ ‡ Sheet '{sheet_name}' ä¸­çš„ 0 å€¼æ›¿æ¢ä¸º NaNã€‚")
                    # --- <<< ç»“æŸæ–°å¢ >>> ---

                    if df_raw.shape[1] < 2:
                        print(f"      é”™è¯¯: ç›®æ ‡ Sheet '{sheet_name}' åˆ—æ•° < 2ã€‚è·³è¿‡ã€‚")
                        continue

                    date_col_name = df_raw.columns[0]
                    actual_target_variable_name = df_raw.columns[1] # Use actual name from sheet B
                    target_sheet_cols.add(actual_target_variable_name) # <-- Corrected variable name

                    print(f"      ç¡®è®¤ç›®æ ‡å˜é‡ (Båˆ—): '{actual_target_variable_name}'")
                    print(f"      è§£æå‘å¸ƒæ—¥æœŸ (Aåˆ—: '{date_col_name}')...")
                    publication_dates_from_target = pd.to_datetime(df_raw[date_col_name], errors='coerce')
                    valid_date_mask = publication_dates_from_target.notna()
                    if not valid_date_mask.any():
                        print(f"      é”™è¯¯: æ— æ³•ä»åˆ— '{date_col_name}' è§£æä»»ä½•æœ‰æ•ˆæ—¥æœŸã€‚è·³è¿‡ç›®æ ‡Sheetã€‚")
                        continue
                    publication_dates_from_target = publication_dates_from_target[valid_date_mask] # Keep only valid dates

                    # --- <<< æ–°å¢ï¼šæ‰“å°åŸå§‹è¯»å–çš„æœ€åå‡ è¡Œå’Œæ—¥æœŸè§£ææƒ…å†µ >>> ---
                    print("      [Debug Target Raw Read] åŸå§‹è¯»å–çš„ç›®æ ‡ Sheet æœ€å 5 è¡Œ:")
                    print(df_raw.tail()[[date_col_name, actual_target_variable_name]])
                    print("      [Debug Target Raw Read] å¯¹åº”æ—¥æœŸåˆ—è§£æç»“æœ (valid_date_mask):")
                    print(valid_date_mask.tail())
                    # --- <<< ç»“æŸæ–°å¢ >>> ---

                    print(f"      æå–ç›®æ ‡å˜é‡åŸå§‹å€¼...")
                    raw_target_values = pd.to_numeric(df_raw.loc[valid_date_mask, actual_target_variable_name], errors='coerce')
                    raw_target_values.index = publication_dates_from_target # Index by Pub Date

                    # Update maps for target var
                    norm_target_name = unicodedata.normalize('NFKC', actual_target_variable_name).strip().lower()
                    var_industry_map[norm_target_name] = target_sheet_industry_name

                    # Extract Monthly Predictors (Cols C+) indexed by PUBLICATION DATE
                    if df_raw.shape[1] > 2:
                        print(f"      æå–ç›®æ ‡ Sheet çš„æœˆåº¦é¢„æµ‹å˜é‡ (Cåˆ—åŠä»¥å)...")
                        temp_monthly_predictors = {}
                        monthly_preds_from_target_sheet = pd.DataFrame() # <-- Temp DF for this sheet's preds
                        # --- <<< æ·»åŠ ï¼šåœ¨å¾ªç¯å‰ç§»é™¤åŸå§‹ DataFrame ä¸­çš„ Unnamed åˆ— (ä»ç¬¬3åˆ—å¼€å§‹æ£€æŸ¥) >>> ---
                        unnamed_cols_target_pred = [col for col in df_raw.columns[2:] if isinstance(col, str) and col.startswith('Unnamed:')]
                        if unnamed_cols_target_pred:
                            print(f"      [æ¸…ç† Target Sheet Predictors] åœ¨ \'{sheet_name}\' ä¸­å‘ç°å¹¶ç§»é™¤ Unnamed åˆ—: {unnamed_cols_target_pred}")
                            df_raw = df_raw.drop(columns=unnamed_cols_target_pred)
                        # --- <<< ç»“æŸæ·»åŠ  >>> ---
                        for col_idx in range(2, df_raw.shape[1]): # é‡æ–°æ£€æŸ¥ shape[1] å› ä¸ºåˆ—å¯èƒ½å·²è¢«ç§»é™¤
                            col_name = df_raw.columns[col_idx]
                            target_sheet_cols.add(col_name) # Track original name
                            # Clean values
                            cleaned_series = df_raw[col_name].astype(str).str.replace('%', '', regex=False).str.replace(',', '', regex=False).str.strip()
                            predictor_values = pd.to_numeric(cleaned_series, errors='coerce')
                            # Create Series indexed by PUBLICATION DATE (aligned to valid dates)
                            temp_monthly_predictors[col_name] = pd.Series(
                                predictor_values[valid_date_mask].values,
                                index=publication_dates_from_target
                            )
                            # Update maps and tracking
                            norm_pred_col = unicodedata.normalize('NFKC', str(col_name)).strip().lower()
                            if norm_pred_col:
                                var_industry_map[norm_pred_col] = target_sheet_industry_name
                                raw_columns_across_all_sheets.add(norm_pred_col)

                        monthly_preds_from_target_sheet = pd.DataFrame(temp_monthly_predictors).sort_index()
                        monthly_preds_from_target_sheet = monthly_preds_from_target_sheet.dropna(axis=1, how='all')
                        # --- NEW: Add these predictors to the combined monthly predictor DF ---
                        if not monthly_preds_from_target_sheet.empty:
                            # df_all_monthly_predictors_pubdate = pd.concat([df_all_monthly_predictors_pubdate, monthly_preds_from_target_sheet], axis=1, join='outer') # OLD
                            df_target_sheet_predictors_pubdate = pd.concat([df_target_sheet_predictors_pubdate, monthly_preds_from_target_sheet], axis=1, join='outer')
                        # --- END NEW ---
                        print(f"      æå–äº† {monthly_preds_from_target_sheet.shape[1]} ä¸ªæœ‰æ•ˆçš„æœˆåº¦é¢„æµ‹å˜é‡ (æŒ‰å‘å¸ƒæ—¥æœŸç´¢å¼•)ã€‚")
                    else:
                        print(f"      ç›®æ ‡ Sheet ä»…å« A, B åˆ—ã€‚")

                except Exception as e:
                    print(f"      åŠ è½½æˆ–å¤„ç†ç›®æ ‡ Sheet '{sheet_name}' å‡ºé”™: {e}. è·³è¿‡ã€‚")
                    publication_dates_from_target = None
                    raw_target_values = None
                    # df_all_monthly_predictors_pubdate = None
                    continue

            # 1b, 1c, 1d: Load Daily, Weekly, Other Monthly Predictors (logic mostly unchanged)
            elif freq_type in ['daily', 'weekly']:
                print(f"      æ£€æµ‹åˆ°é¢„æµ‹å˜é‡ Sheet ('{freq_type}', è¡Œä¸š: '{industry_name}')...")
                try:
                    # --- NEW: ä½¿ç”¨é€šç”¨æ ¼å¼æ£€æµ‹ ---
                    format_info = detect_sheet_format(excel_file, sheet_name)
                    read_header = format_info['header']
                    skip_rows = format_info['skiprows']
                    read_index_col = 0  # Always assume index is the first column
                    
                    print(f"        [æ ¼å¼æ£€æµ‹ç»“æœ] {format_info['format']} (æ¥æº: {format_info['source']})")
                    print(f"        [è¯»å–å‚æ•°] header={read_header}, skiprows={skip_rows}, index_col={read_index_col}")
                    # --- End NEW Logic ---

                    df = pd.read_excel(excel_file, sheet_name=sheet_name, header=read_header, skiprows=skip_rows, index_col=read_index_col, parse_dates=True)

                    # --- <<< æ–°å¢ï¼šå°† 0 å€¼æ›¿æ¢ä¸º NaN (æ—¥åº¦/å‘¨åº¦) >>> ---
                    df = df.replace(0, np.nan)
                    print(f"      [å¤„ç†] å°† Sheet '{sheet_name}' ä¸­çš„ 0 å€¼æ›¿æ¢ä¸º NaNã€‚")
                    # --- <<< ç»“æŸæ–°å¢ >>> ---

                    # --- <<< æ·»åŠ ï¼šåœ¨å¤„ç†å‰ç§»é™¤ Unnamed åˆ— >>> ---
                    unnamed_cols_dw = [col for col in df.columns if isinstance(col, str) and col.startswith('Unnamed:')]
                    if unnamed_cols_dw:
                        print(f"      [æ¸…ç† Daily/Weekly] åœ¨ \'{sheet_name}\' ä¸­å‘ç°å¹¶ç§»é™¤ Unnamed åˆ—: {unnamed_cols_dw}")
                        df = df.drop(columns=unnamed_cols_dw)
                    # --- <<< ç»“æŸæ·»åŠ  >>> ---

                    # --- <<< æ–°å¢ï¼šæ‰“å°åŠ è½½åçš„åˆ—å >>> ---
                    print(f"      [Debug Columns Loaded] Sheet: '{sheet_name}', Loaded Columns: {df.columns.tolist()}")
                    # --- <<< ç»“æŸæ–°å¢ >>> ---

                    # --- NEW: Force index to datetime and handle errors ---
                    print(f"      Attempting to convert index of '{sheet_name}' to datetime...")
                    original_index_len = len(df.index)
                    df.index = pd.to_datetime(df.index, errors='coerce')
                    # --- END NEW ---

                    if df is None or df.empty: continue
                    df = df.loc[df.index.notna()] # Filter rows where index conversion failed (became NaT)
                    filtered_index_len = len(df.index)
                    if filtered_index_len < original_index_len:
                        print(f"      è­¦å‘Š: åœ¨ '{sheet_name}' ä¸­ç§»é™¤äº† {original_index_len - filtered_index_len} è¡Œï¼Œå› ä¸ºå®ƒä»¬çš„ç´¢å¼•æ— æ³•è§£æä¸ºæœ‰æ•ˆæ—¥æœŸã€‚")

                    df = df.dropna(axis=1, how='all')
                    if df.empty: continue
                    df_numeric = df.apply(pd.to_numeric, errors='coerce')
                    if df_numeric.empty or df_numeric.isnull().all().all(): continue
                    print(f"      Sheet '{sheet_name}' ({industry_name}, {freq_type}) åŠ è½½å®Œæˆã€‚ Shape: {df_numeric.shape}")
                    data_parts[freq_type].append(df_numeric)
                    for col in df_numeric.columns:
                        norm_col = unicodedata.normalize('NFKC', str(col)).strip().lower()
                        if norm_col:
                            var_industry_map[norm_col] = industry_name
                            raw_columns_across_all_sheets.add(norm_col)
                except Exception as e:
                    print(f"      åŠ è½½æˆ–å¤„ç† {freq_type} Sheet '{sheet_name}' æ—¶å‡ºé”™: {e}. è·³è¿‡ã€‚")
                    continue
            elif freq_type == 'monthly_predictor':
                print(f"      æ£€æµ‹åˆ°éç›®æ ‡æœˆåº¦é¢„æµ‹ Sheet ('{freq_type}', è¡Œä¸š: '{industry_name}')...")
                try:
                    # --- NEW: ä½¿ç”¨é€šç”¨æ ¼å¼æ£€æµ‹ ---
                    format_info = detect_sheet_format(excel_file, sheet_name)
                    read_header = format_info['header']
                    skip_rows = format_info['skiprows']
                    
                    print(f"        [æ ¼å¼æ£€æµ‹ç»“æœ] {format_info['format']} (æ¥æº: {format_info['source']})")
                    print(f"        [è¯»å–å‚æ•°] header={read_header}, skiprows={skip_rows}")
                    # --- End NEW Logic ---

                    df_raw_pred = pd.read_excel(excel_file, sheet_name=sheet_name, header=read_header, skiprows=skip_rows)

                    # --- <<< æ·»åŠ ï¼šåœ¨å¤„ç†å‰ç§»é™¤ Unnamed åˆ— >>> ---
                    unnamed_cols_m_pred = [col for col in df_raw_pred.columns if isinstance(col, str) and col.startswith('Unnamed:')]
                    if unnamed_cols_m_pred:
                        print(f"      [æ¸…ç† Monthly Predictor] åœ¨ \'{sheet_name}\' ä¸­å‘ç°å¹¶ç§»é™¤ Unnamed åˆ—: {unnamed_cols_m_pred}")
                        df_raw_pred = df_raw_pred.drop(columns=unnamed_cols_m_pred)
                    # --- <<< ç»“æŸæ·»åŠ  >>> ---

                    # --- <<< æ–°å¢ï¼šå°† 0 å€¼æ›¿æ¢ä¸º NaN >>> ---
                    df_raw_pred = df_raw_pred.replace(0, np.nan)
                    print(f"      [å¤„ç†] å°†å…¶ä»–æœˆåº¦é¢„æµ‹ Sheet '{sheet_name}' ä¸­çš„ 0 å€¼æ›¿æ¢ä¸º NaNã€‚")
                    # --- <<< ç»“æŸæ–°å¢ >>> ---

                    if df_raw_pred.shape[1] < 2:
                        print(f"      é”™è¯¯: æœˆåº¦é¢„æµ‹ Sheet '{sheet_name}' åˆ—æ•° < 2ã€‚è·³è¿‡ã€‚")
                        continue

                    date_col_name_pred = df_raw_pred.columns[0]
                    print(f"      è§£æå‘å¸ƒæ—¥æœŸ (Aåˆ—: '{date_col_name_pred}')...")
                    publication_dates_predictor = pd.to_datetime(df_raw_pred[date_col_name_pred], errors='coerce')
                    valid_date_mask_pred = publication_dates_predictor.notna()
                    if not valid_date_mask_pred.any():
                        print(f"      é”™è¯¯: æ— æ³•ä»åˆ— '{date_col_name_pred}' è§£æä»»ä½•æœ‰æ•ˆæ—¥æœŸã€‚è·³è¿‡æ­¤Sheetã€‚")
                        continue
                    publication_dates_predictor = publication_dates_predictor[valid_date_mask_pred]

                    print(f"      æå–æœˆåº¦é¢„æµ‹å˜é‡ (Båˆ—åŠä»¥å)...")
                    temp_monthly_predictors_sheet = {}
                    for col_idx_pred in range(1, df_raw_pred.shape[1]): # Start from Col B (index 1)
                        col_name_pred = df_raw_pred.columns[col_idx_pred]
                        # Clean values
                        cleaned_series_pred = df_raw_pred[col_name_pred].astype(str).str.replace('%', '', regex=False).str.replace(',', '', regex=False).str.strip()
                        predictor_values_pred = pd.to_numeric(cleaned_series_pred, errors='coerce')
                        # Create Series indexed by PUBLICATION DATE (aligned to valid dates)
                        temp_monthly_predictors_sheet[col_name_pred] = pd.Series(
                            predictor_values_pred[valid_date_mask_pred].values,
                            index=publication_dates_predictor
                        )
                        # Update maps and tracking
                        norm_pred_col_p = unicodedata.normalize('NFKC', str(col_name_pred)).strip().lower()
                        if norm_pred_col_p:
                            var_industry_map[norm_pred_col_p] = industry_name # Use industry from this sheet
                            raw_columns_across_all_sheets.add(norm_pred_col_p)

                    df_monthly_pred_sheet = pd.DataFrame(temp_monthly_predictors_sheet).sort_index()
                    df_monthly_pred_sheet = df_monthly_pred_sheet.dropna(axis=1, how='all')
                    if not df_monthly_pred_sheet.empty:
                        # --- NEW: Add these predictors to the combined monthly predictor DF ---
                        # --- NEW: Add predictors from OTHER monthly sheets to their dedicated DF ---
                        df_other_monthly_predictors_pubdate = pd.concat([df_other_monthly_predictors_pubdate, df_monthly_pred_sheet], axis=1, join='outer')
                        # --- END NEW ---
                        print(f"      æå–äº† {df_monthly_pred_sheet.shape[1]} ä¸ªæœ‰æ•ˆçš„æœˆåº¦é¢„æµ‹å˜é‡ (æŒ‰å‘å¸ƒæ—¥æœŸç´¢å¼•)ã€‚")
                    else:
                        print("      æ­¤ Sheet æœªåŒ…å«æœ‰æ•ˆçš„æœˆåº¦é¢„æµ‹å˜é‡æ•°æ®ã€‚")

                except Exception as e_pred:
                    print(f"      åŠ è½½æˆ–å¤„ç†æœˆåº¦é¢„æµ‹ Sheet '{sheet_name}' å‡ºé”™: {e_pred}. è·³è¿‡ã€‚")
                    continue
            # 1e: Ignore other sheets
            else:
                 if sheet_name != reference_sheet_name:
                     print(f"      Sheet '{sheet_name}' ä¸ç¬¦åˆè¦æ±‚æˆ–éç›®æ ‡ Sheetï¼Œå·²è·³è¿‡ã€‚")
                 continue

        # --- Check if essential data was loaded ---
        if raw_target_values is None or raw_target_values.empty or publication_dates_from_target is None:
             print(f"é”™è¯¯ï¼š[Data Prep] æœªèƒ½æˆåŠŸåŠ è½½ç›®æ ‡å˜é‡ '{target_variable_name}' æˆ–å…¶å‘å¸ƒæ—¥æœŸã€‚")
             return None, None, None, None
        # Only warn if predictors are missing, target is essential  
        if (not data_parts['daily'] and not data_parts['weekly'] and 
            (df_other_monthly_predictors_pubdate is None or df_other_monthly_predictors_pubdate.empty) and
            (df_target_sheet_predictors_pubdate is None or df_target_sheet_predictors_pubdate.empty)):
             print("è­¦å‘Šï¼š[Data Prep] æœªèƒ½åŠ è½½ä»»ä½•æœ‰æ•ˆçš„æ—¥åº¦ã€å‘¨åº¦æˆ–æœˆåº¦é¢„æµ‹å˜é‡ã€‚")

        # +++++++++++++ DEBUG: Find missing variable +++++++++++++
        if reference_predictor_variables and raw_columns_across_all_sheets:
            missing_from_data_sheets = reference_predictor_variables - raw_columns_across_all_sheets
            if missing_from_data_sheets:
                print(f"\n+++ DEBUG: å˜é‡å­˜åœ¨äºæŒ‡æ ‡ä½“ç³»ä½†æœªåœ¨æ•°æ®Sheetä¸­æ‰¾åˆ°: {missing_from_data_sheets} +++\n")
            else:
                print("\n+++ DEBUG: æ‰€æœ‰æŒ‡æ ‡ä½“ç³»å˜é‡éƒ½åœ¨åŠ è½½çš„æ•°æ®ä¸­æ‰¾åˆ°äº†ã€‚ +++\n")
        # +++++++++++++ END DEBUG +++++++++++++

        # Initialize lists/sets for tracking
        removed_variables_detailed_log = [] # List of dicts {'Variable': name, 'Reason': reason_code}
        all_indices_for_range = [] # Collect all datetime indices to determine full range

        # --- Step 2: Target Variable Alignment (Nearest Friday) ---
        print("\n--- [Data Prep V3] æ­¥éª¤ 2: ç›®æ ‡å˜é‡å¤„ç†ä¸å¯¹é½ (æœ€è¿‘å‘¨äº”) ---")
        target_series_aligned_nearest_friday = pd.Series(dtype=float)
        if raw_target_values is not None and not raw_target_values.empty:
            temp_target_df = pd.DataFrame({'value': raw_target_values})
            # Calculate the nearest Friday for each publication date
            # If weekday is Mon, Tue, Wed -> go to upcoming Fri (4 - weekday)
            # If weekday is Thu, Fri, Sat, Sun -> go to previous Fri (4 - weekday)
            # Note: Python's weekday() is Mon=0, Tue=1, ..., Fri=4, Sat=5, Sun=6
            temp_target_df['nearest_friday'] = temp_target_df.index.map(lambda dt: dt + pd.Timedelta(days=4 - dt.weekday()))
            # Handle duplicates for the same target Friday: keep the one with the LATEST publication date
            # We sort by the original publication date index FIRST, then group and take the last.
            target_series_aligned_nearest_friday = temp_target_df.sort_index(ascending=True).groupby('nearest_friday')['value'].last()
            target_series_aligned_nearest_friday.index.name = 'Date'
            target_series_aligned_nearest_friday.name = actual_target_variable_name # Ensure Series has the correct name
            print(f"  ç›®æ ‡å˜é‡å¯¹é½åˆ°æœ€è¿‘å‘¨äº”å®Œæˆã€‚Shape: {target_series_aligned_nearest_friday.shape}")
            # --- <<< æ–°å¢ï¼šæ‰“å°å¯¹é½åçš„æœ€åå‡ è¡Œ >>> ---
            print("    [Debug Target Align] å¯¹é½åç›®æ ‡å˜é‡æœ€å 5 è¡Œ:")
            print(target_series_aligned_nearest_friday.tail())
            # --- <<< ç»“æŸæ–°å¢ >>> ---
            if not target_series_aligned_nearest_friday.empty:
                all_indices_for_range.append(target_series_aligned_nearest_friday.index)
        else:
             print("  æœªåŠ è½½ç›®æ ‡å˜é‡ï¼Œæ— æ³•è¿›è¡Œå¯¹é½ã€‚")
             # If target is essential, we should probably return error here, but we already checked above.

        # --- NEW Step 2.5: Target Sheet Predictors Alignment (Nearest Friday) ---
        print("\\n--- [Data Prep V3] æ­¥éª¤ 2.5: ç›®æ ‡ Sheet é¢„æµ‹å˜é‡å¤„ç†ä¸å¯¹é½ (æœ€è¿‘å‘¨äº”) ---")
        target_sheet_predictors_aligned_nearest_friday = pd.DataFrame()
        target_sheet_predictor_cols = set() # Track original names from target sheet C+
        if df_target_sheet_predictors_pubdate is not None and not df_target_sheet_predictors_pubdate.empty:
            # Ensure no duplicate columns before processing
            cols_before_dedup_tsp = set(df_target_sheet_predictors_pubdate.columns)
            df_target_sheet_predictors_pubdate = df_target_sheet_predictors_pubdate.loc[:, ~df_target_sheet_predictors_pubdate.columns.duplicated(keep='first')]
            cols_after_dedup_tsp = set(df_target_sheet_predictors_pubdate.columns)
            removed_cols_dedup_tsp = cols_before_dedup_tsp - cols_after_dedup_tsp
            if removed_cols_dedup_tsp:
                 print(f"    è­¦å‘Š: åœ¨å¤„ç†ç›®æ ‡ Sheet é¢„æµ‹å˜é‡å‰å› é‡å¤ç§»é™¤äº† {len(removed_cols_dedup_tsp)} åˆ—: {list(removed_cols_dedup_tsp)[:5]}...")
                 for col in removed_cols_dedup_tsp:
                     if not any(d['Variable'] == col for d in removed_variables_detailed_log):
                         removed_variables_detailed_log.append({'Variable': col, 'Reason': 'target_sheet_predictor_duplicate'})

            # Apply the same nearest Friday logic as the target variable
            temp_tsp_df = df_target_sheet_predictors_pubdate.copy()
            temp_tsp_df['nearest_friday'] = temp_tsp_df.index.map(lambda dt: dt + pd.Timedelta(days=4 - dt.weekday()))
            # Handle duplicates for the same target Friday: keep the one with the LATEST publication date
            target_sheet_predictors_aligned_nearest_friday = temp_tsp_df.sort_index(ascending=True).groupby('nearest_friday').last()
            target_sheet_predictors_aligned_nearest_friday.index.name = 'Date'
            # No need to rename columns, they keep their original names
            print(f"  ç›®æ ‡ Sheet é¢„æµ‹å˜é‡ ({len(target_sheet_predictors_aligned_nearest_friday.columns)} ä¸ª) å¯¹é½åˆ°æœ€è¿‘å‘¨äº”å®Œæˆã€‚Shape: {target_sheet_predictors_aligned_nearest_friday.shape}")
            if not target_sheet_predictors_aligned_nearest_friday.empty:
                all_indices_for_range.append(target_sheet_predictors_aligned_nearest_friday.index)
                target_sheet_predictor_cols = set(target_sheet_predictors_aligned_nearest_friday.columns)
        else:
            print("  ç›®æ ‡ Sheet ä¸­æœªåŒ…å«å…¶ä»–é¢„æµ‹å˜é‡ (Cåˆ—åŠä»¥å)ï¼Œæˆ–åŠ è½½å¤±è´¥ã€‚")

        # --- Step 3: Other Monthly Predictors Processing (Last Friday of Month) ---
        print("\\n--- [Data Prep V3] æ­¥éª¤ 3: å…¶ä»–æœˆåº¦é¢„æµ‹å˜é‡å¤„ç†ä¸å¯¹é½ (æœˆæœ«æœ€åå‘¨äº”) ---")
        monthly_predictors_aligned_last_friday = pd.DataFrame()
        monthly_transform_log = {}
        other_monthly_predictors_to_skip_weekly_stationarity = set() # Track columns from THIS step only
        # if df_all_monthly_predictors_pubdate is not None and not df_all_monthly_predictors_pubdate.empty: # OLD
        if df_other_monthly_predictors_pubdate is not None and not df_other_monthly_predictors_pubdate.empty:
            # 3a: Aggregate to Month End using last()
            print("  èšåˆå…¶ä»–æ¥æºçš„æœˆåº¦é¢„æµ‹å˜é‡åˆ°æœˆæœ« (å–å½“æœˆæœ€åæœ‰æ•ˆå€¼)...")
            # Ensure no duplicate columns before processing
            # cols_before_dedup_monthly = set(df_all_monthly_predictors_pubdate.columns) # OLD
            cols_before_dedup_monthly = set(df_other_monthly_predictors_pubdate.columns)
            # df_all_monthly_predictors_pubdate = df_all_monthly_predictors_pubdate.loc[:, ~df_all_monthly_predictors_pubdate.columns.duplicated(keep='first')] # OLD
            df_other_monthly_predictors_pubdate = df_other_monthly_predictors_pubdate.loc[:, ~df_other_monthly_predictors_pubdate.columns.duplicated(keep='first')]
            # cols_after_dedup_monthly = set(df_all_monthly_predictors_pubdate.columns) # OLD
            cols_after_dedup_monthly = set(df_other_monthly_predictors_pubdate.columns)
            removed_cols_dedup_monthly = cols_before_dedup_monthly - cols_after_dedup_monthly
            if removed_cols_dedup_monthly:
                 print(f"    è­¦å‘Š: åœ¨èšåˆå‰å› é‡å¤ç§»é™¤äº† {len(removed_cols_dedup_monthly)} ä¸ªå…¶ä»–æœˆåº¦é¢„æµ‹å˜é‡åˆ—: {list(removed_cols_dedup_monthly)[:5]}...")
                 for col in removed_cols_dedup_monthly:
                     if not any(d['Variable'] == col for d in removed_variables_detailed_log):
                         removed_variables_detailed_log.append({'Variable': col, 'Reason': 'other_monthly_predictor_duplicate'})

            # df_monthly_predictors_for_stat = df_all_monthly_predictors_pubdate.copy() # OLD
            df_monthly_predictors_for_stat = df_other_monthly_predictors_pubdate.copy()
            df_monthly_predictors_for_stat['MonthIndex'] = df_monthly_predictors_for_stat.index.to_period('M').to_timestamp('M')
            df_monthly_predictors_for_stat = df_monthly_predictors_for_stat.groupby('MonthIndex').last()
            df_monthly_predictors_for_stat = df_monthly_predictors_for_stat.sort_index()
            print(f"    èšåˆåˆ°æœˆæœ«å®Œæˆ. Shape: {df_monthly_predictors_for_stat.shape}")

            # 3b: Monthly NaN Check (Applied to OTHER monthly predictors)
            if consecutive_nan_threshold is not None and consecutive_nan_threshold > 0:
                print(f"  [æœˆåº¦æ£€æŸ¥] å¼€å§‹æ£€æŸ¥å…¶ä»–æ¥æºæœˆåº¦é¢„æµ‹å˜é‡çš„è¿ç»­ç¼ºå¤±å€¼ (é˜ˆå€¼ >= {consecutive_nan_threshold})...")
                # ... (NaN checking logic remains the same, applied to df_monthly_predictors_for_stat) ...
                initial_cols_monthly_nan = set(df_monthly_predictors_for_stat.columns)
                cols_to_remove_monthly_nan_pred = []
                for col in df_monthly_predictors_for_stat.columns:
                    series = df_monthly_predictors_for_stat[col]
                    first_valid_idx = series.first_valid_index()
                    if first_valid_idx is None: continue # Skip if column is all NaN already
                    series_after_first_valid = series.loc[first_valid_idx:]
                    is_na = series_after_first_valid.isna()
                    # Calculate consecutive NaNs
                    na_blocks = is_na.ne(is_na.shift()).cumsum()[is_na]
                    max_consecutive_nan = 0
                    if not na_blocks.empty:
                         try:
                             # Calculate counts of consecutive blocks
                             block_counts = na_blocks.value_counts()
                             if not block_counts.empty:
                                  max_consecutive_nan = block_counts.max()
                             else: # Handle edge case where na_blocks is not empty but value_counts is
                                 max_consecutive_nan = 0
                         except Exception as e_nan_count: # Catch potential errors in value_counts
                             print(f"    [æœˆåº¦æ£€æŸ¥] è­¦å‘Š: è®¡ç®— '{col}' çš„ NaN å—æ—¶å‡ºé”™: {e_nan_count}. è·³è¿‡æ­¤åˆ—æ£€æŸ¥.")
                             continue # Skip this column if counting fails

                    if max_consecutive_nan >= consecutive_nan_threshold:
                        cols_to_remove_monthly_nan_pred.append(col)
                        print(f"    [æœˆåº¦æ£€æŸ¥] æ ‡è®°ç§»é™¤å˜é‡: '{col}' (æœ€å¤§è¿ç»­ NaN: {max_consecutive_nan} >= {consecutive_nan_threshold})", end='')
                        if not any(d['Variable'] == col for d in removed_variables_detailed_log):
                             removed_variables_detailed_log.append({'Variable': col, 'Reason': 'other_monthly_predictor_consecutive_nan'})
                             print(" - å·²è®°å½•ç§»é™¤")
                        else: print(" - å·²åœ¨å…¶ä»–æ­¥éª¤è®°å½•")

                if cols_to_remove_monthly_nan_pred:
                    print(f"\n    [æœˆåº¦æ£€æŸ¥] æ­£åœ¨ç§»é™¤ {len(cols_to_remove_monthly_nan_pred)} ä¸ªæœˆåº¦é¢„æµ‹å˜é‡...")
                    df_monthly_predictors_for_stat = df_monthly_predictors_for_stat.drop(columns=cols_to_remove_monthly_nan_pred)
                    print(f"      ç§»é™¤å Shape: {df_monthly_predictors_for_stat.shape}")
                else:
                     print(f"    [æœˆåº¦æ£€æŸ¥] æ‰€æœ‰å…¶ä»–æœˆåº¦é¢„æµ‹å˜é‡çš„è¿ç»­ç¼ºå¤±å€¼å‡ä½äºé˜ˆå€¼ã€‚")
            else:
                print("  (è·³è¿‡/ç¦ç”¨) æœˆåº¦é¢„æµ‹å˜é‡è¿ç»­ç¼ºå¤±å€¼æ£€æŸ¥ã€‚")

            # 3c: Monthly Stationarity Check
            if not df_monthly_predictors_for_stat.empty:
                print("\n--- [æœˆåº¦é¢„æµ‹å˜é‡å¹³ç¨³æ€§æ£€æŸ¥] ---")
                df_monthly_predictors_stationary, monthly_transform_log, removed_cols_info_monthly_pred = _ensure_stationarity(
                    df_monthly_predictors_for_stat,
                    skip_cols=None, # Check all monthly predictors
                    adf_p_threshold=0.05
                )
                print(f"    æœˆåº¦é¢„æµ‹å˜é‡å¹³ç¨³æ€§å¤„ç†å®Œæˆã€‚å¤„ç†å Shape: {df_monthly_predictors_stationary.shape}")
                # --- FIX: Record columns to skip BEFORE adding temporary columns ---
                if not df_monthly_predictors_stationary.empty:
                     other_monthly_predictors_to_skip_weekly_stationarity = set(df_monthly_predictors_stationary.columns)
                else:
                     other_monthly_predictors_to_skip_weekly_stationarity = set()
                # --- END FIX ---

                # Log stationarity removals
                for reason, cols in removed_cols_info_monthly_pred.items():
                    for col in cols:
                        if not any(d['Variable'] == col for d in removed_variables_detailed_log):
                            # Log reason as 'other_monthly_predictor_stationarity...'
                            removed_variables_detailed_log.append({'Variable': col, 'Reason': f'other_monthly_predictor_stationarity_{reason}'})

                # 3d: Align to Last Friday of the Month
                if not df_monthly_predictors_stationary.empty:
                    print("  å¯¹é½å¤„ç†åçš„æœˆåº¦é¢„æµ‹å˜é‡åˆ°å½“æœˆæœ€åå‘¨äº”...")
                    # Calculate the last Friday of the month for each index (which is month end)
                    df_monthly_predictors_stationary['last_friday'] = df_monthly_predictors_stationary.index.map(
                        lambda dt: dt - pd.Timedelta(days=(dt.weekday() - 4 + 7) % 7) # Go back to the last Friday
                    )
                    monthly_predictors_aligned_last_friday = df_monthly_predictors_stationary.set_index('last_friday', drop=True)
                    monthly_predictors_aligned_last_friday.index.name = 'Date'
                    # Handle potential duplicates for the same target Friday (if multiple month-ends map to the same last Friday)
                    # Keep the LATEST month's data in case of overlap (though unlikely with month-end index)
                    monthly_predictors_aligned_last_friday = monthly_predictors_aligned_last_friday[
                        ~monthly_predictors_aligned_last_friday.index.duplicated(keep='last')
                    ]
                    monthly_predictors_aligned_last_friday = monthly_predictors_aligned_last_friday.sort_index()
                    print(f"    å¯¹é½åˆ°æœ€åå‘¨äº”å®Œæˆã€‚ Shape: {monthly_predictors_aligned_last_friday.shape}")
                    if not monthly_predictors_aligned_last_friday.empty:
                        all_indices_for_range.append(monthly_predictors_aligned_last_friday.index)
                    # Record columns that originated from OTHER monthly predictors to skip weekly stationarity later
                    # other_monthly_predictors_to_skip_weekly_stationarity = set(df_monthly_predictors_stationary.columns) # OLD BUGGY LINE
                    # --- Corrected log message using the set populated earlier ---
                    print(f"    å°†è®°å½• {len(other_monthly_predictors_to_skip_weekly_stationarity)} ä¸ªæ¥è‡ªå…¶ä»–æœˆåº¦æºçš„åˆ—ç”¨äºè·³è¿‡åç»­å‘¨åº¦å¹³ç¨³æ€§æ£€æŸ¥ã€‚")
                else:
                    print("  æ²¡æœ‰æ¥è‡ªå…¶ä»–æœˆåº¦æºçš„å¹³ç¨³åŒ–é¢„æµ‹å˜é‡å¯ä¾›å¯¹é½ã€‚")
            else: # If df_monthly_predictors_for_stat was empty after NaN check
                print("  æ²¡æœ‰æœˆåº¦é¢„æµ‹å˜é‡è¿›è¡Œå¹³ç¨³æ€§æ£€æŸ¥ (å¯èƒ½å› è¿ç»­ NaN è¢«ç§»é™¤)ã€‚")
        else: # If df_other_monthly_predictors_pubdate was None or empty initially
            print("  æ²¡æœ‰å…¶ä»–æœˆåº¦é¢„æµ‹å˜é‡éœ€è¦å¤„ç†ã€‚")

        # --- Step 4: Daily/Weekly Data Processing ---
        print(f"\n--- [Data Prep V3] æ­¥éª¤ 4: æ—¥åº¦å’Œå‘¨åº¦æ•°æ®å¤„ç† ({target_freq}) ---")
        df_daily_weekly_mean = pd.DataFrame()
        df_weekly_aligned = pd.DataFrame()
        df_combined_dw_weekly = pd.DataFrame() # Initialize

        # 4a: Daily -> Weekly (Mean)
        if data_parts['daily']:
            print("  å¤„ç†æ—¥åº¦æ•°æ® -> å‘¨åº¦ (å‡å€¼)...")
            df_daily_full = pd.concat(data_parts['daily'], axis=1)
            # Handle duplicate columns from different daily sheets before resampling
            cols_before_dedup_daily = set(df_daily_full.columns)
            df_daily_full = df_daily_full.loc[:, ~df_daily_full.columns.duplicated(keep='first')]
            cols_after_dedup_daily = set(df_daily_full.columns)
            removed_cols_dedup_daily = cols_before_dedup_daily - cols_after_dedup_daily
            if removed_cols_dedup_daily:
                 print(f"    è­¦å‘Š: åœ¨åˆå¹¶æ—¥åº¦æ•°æ®æ—¶å› é‡å¤ç§»é™¤äº† {len(removed_cols_dedup_daily)} åˆ—: {list(removed_cols_dedup_daily)[:5]}...")
                 for col in removed_cols_dedup_daily:
                     if not any(d['Variable'] == col for d in removed_variables_detailed_log):
                         removed_variables_detailed_log.append({'Variable': col, 'Reason': 'daily_duplicate_column'})

            if not df_daily_full.empty:
                 # Add original daily index to range calculation *before* resampling
                 all_indices_for_range.append(df_daily_full.index)
                 df_daily_weekly_mean = df_daily_full.resample(target_freq).mean()
                 print(f"    æ—¥åº¦->å‘¨åº¦(å‡å€¼) å®Œæˆ. Shape: {df_daily_weekly_mean.shape}")
            else:
                print("    åˆå¹¶åçš„æ—¥åº¦æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œå‘¨åº¦è½¬æ¢ã€‚")
        else:
            print("  æ— æ—¥åº¦æ•°æ®ã€‚")

        # 4b: Weekly -> Weekly (Last value alignment)
        if data_parts['weekly']:
            print("  å¤„ç†å‘¨åº¦æ•°æ® -> å‘¨åº¦ (æœ€åå€¼)...")
            df_weekly_full = pd.concat(data_parts['weekly'], axis=1)
             # Handle duplicate columns from different weekly sheets before resampling
            cols_before_dedup_weekly = set(df_weekly_full.columns)
            df_weekly_full = df_weekly_full.loc[:, ~df_weekly_full.columns.duplicated(keep='first')]
            cols_after_dedup_weekly = set(df_weekly_full.columns)
            removed_cols_dedup_weekly = cols_before_dedup_weekly - cols_after_dedup_weekly
            if removed_cols_dedup_weekly:
                 print(f"    è­¦å‘Š: åœ¨åˆå¹¶å‘¨åº¦æ•°æ®æ—¶å› é‡å¤ç§»é™¤äº† {len(removed_cols_dedup_weekly)} åˆ—: {list(removed_cols_dedup_weekly)[:5]}...")
                 for col in removed_cols_dedup_weekly:
                     if not any(d['Variable'] == col for d in removed_variables_detailed_log):
                         removed_variables_detailed_log.append({'Variable': col, 'Reason': 'weekly_duplicate_column'})

            if not df_weekly_full.empty:
                 # Add original weekly index to range calculation *before* resampling
                 all_indices_for_range.append(df_weekly_full.index)
                 df_weekly_aligned = df_weekly_full.resample(target_freq).last()
                 print(f"    å‘¨åº¦->å‘¨åº¦(å¯¹é½) å®Œæˆ. Shape: {df_weekly_aligned.shape}")
            else:
                print("    åˆå¹¶åçš„å‘¨åº¦æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œå‘¨åº¦è½¬æ¢ã€‚")
        else:
            print("  æ— å‘¨åº¦æ•°æ®ã€‚")

        # 4c: Combine Daily(W) and Weekly(W) and perform NaN check
        print("\n  åˆå¹¶è½¬æ¢åçš„æ—¥åº¦å’Œå‘¨åº¦æ•°æ®...")
        parts_to_combine_dw = []
        if not df_daily_weekly_mean.empty: parts_to_combine_dw.append(df_daily_weekly_mean)
        if not df_weekly_aligned.empty: parts_to_combine_dw.append(df_weekly_aligned)

        if parts_to_combine_dw:
            df_combined_dw_weekly = pd.concat(parts_to_combine_dw, axis=1)
            # Handle duplicates arising from combining daily/weekly sources
            cols_before_dedup_dw = set(df_combined_dw_weekly.columns)
            df_combined_dw_weekly = df_combined_dw_weekly.loc[:, ~df_combined_dw_weekly.columns.duplicated(keep='first')]
            cols_after_dedup_dw = set(df_combined_dw_weekly.columns)
            removed_cols_dedup_dw = cols_before_dedup_dw - cols_after_dedup_dw
            if removed_cols_dedup_dw:
                 print(f"    è­¦å‘Š: åœ¨åˆå¹¶æ—¥åº¦(å‘¨)å’Œå‘¨åº¦(å‘¨)æ•°æ®æ—¶å› é‡å¤ç§»é™¤äº† {len(removed_cols_dedup_dw)} åˆ—: {list(removed_cols_dedup_dw)[:5]}...")
                 for col in removed_cols_dedup_dw:
                     if not any(d['Variable'] == col for d in removed_variables_detailed_log):
                         removed_variables_detailed_log.append({'Variable': col, 'Reason': 'daily_weekly_combined_duplicate'})
            print(f"    åˆå¹¶å Shape: {df_combined_dw_weekly.shape}")

            # Perform NaN check on the combined daily/weekly data
            if consecutive_nan_threshold is not None and consecutive_nan_threshold > 0:
                print(f"  [å‘¨åº¦æ£€æŸ¥] å¼€å§‹æ£€æŸ¥åˆå¹¶çš„æ—¥/å‘¨æ•°æ®çš„è¿ç»­ç¼ºå¤±å€¼ (é˜ˆå€¼ >= {consecutive_nan_threshold})...")
                cols_to_remove_dw_nan = []
                for col in df_combined_dw_weekly.columns:
                    series = df_combined_dw_weekly[col]
                    first_valid_idx = series.first_valid_index()
                    if first_valid_idx is None: continue # Skip if column is all NaN
                    series_after_first_valid = series.loc[first_valid_idx:]
                    is_na = series_after_first_valid.isna()
                    na_blocks = is_na.ne(is_na.shift()).cumsum()[is_na]
                    max_consecutive_nan = 0
                    if not na_blocks.empty:
                        try:
                            block_counts = na_blocks.value_counts()
                            if not block_counts.empty: max_consecutive_nan = block_counts.max()
                        except Exception as e_nan_count_dw:
                             print(f"    [å‘¨åº¦æ£€æŸ¥] è­¦å‘Š: è®¡ç®— '{col}' çš„ NaN å—æ—¶å‡ºé”™: {e_nan_count_dw}. è·³è¿‡æ­¤åˆ—æ£€æŸ¥.")
                             continue

                    if max_consecutive_nan >= consecutive_nan_threshold:
                        cols_to_remove_dw_nan.append(col)
                        print(f"    [å‘¨åº¦æ£€æŸ¥] æ ‡è®°ç§»é™¤å˜é‡: '{col}' (æœ€å¤§è¿ç»­ NaN: {max_consecutive_nan} >= {consecutive_nan_threshold})", end='')
                        if not any(d['Variable'] == col for d in removed_variables_detailed_log):
                             removed_variables_detailed_log.append({'Variable': col, 'Reason': 'dw_consecutive_nan'})
                             print(" - å·²è®°å½•ç§»é™¤")
                        else: print(" - å·²åœ¨å…¶ä»–æ­¥éª¤è®°å½•")

                if cols_to_remove_dw_nan:
                    print("\n    [å‘¨åº¦æ£€æŸ¥] æ­£åœ¨ç§»é™¤ {len(cols_to_remove_dw_nan)} ä¸ªè¿ç»­ç¼ºå¤±å€¼è¶…æ ‡çš„æ—¥/å‘¨å˜é‡...")
                    df_combined_dw_weekly = df_combined_dw_weekly.drop(columns=cols_to_remove_dw_nan)
                    print(f"      ç§»é™¤åæ—¥/å‘¨æ•°æ® Shape: {df_combined_dw_weekly.shape}")
                else:
                    print(f"    [å‘¨åº¦æ£€æŸ¥] æ‰€æœ‰åˆå¹¶çš„æ—¥/å‘¨å˜é‡çš„è¿ç»­ç¼ºå¤±å€¼å‡ä½äºé˜ˆå€¼ã€‚")
            else:
                print("  (è·³è¿‡/ç¦ç”¨) åˆå¹¶æ—¥/å‘¨æ•°æ®çš„è¿ç»­ç¼ºå¤±å€¼æ£€æŸ¥ã€‚")
        else:
            print("  æ²¡æœ‰æœ‰æ•ˆçš„æ—¥åº¦æˆ–å‘¨åº¦æ•°æ®å¯åˆå¹¶ã€‚")


        # --- Step 5: Combine All Aligned Weekly Data ---
        print("\n--- [Data Prep V3] æ­¥éª¤ 5: åˆå¹¶æ‰€æœ‰å¯¹é½åçš„å‘¨åº¦æ•°æ® --- ")
        # List of parts to combine: Target (nearest Fri), Target Sheet Predictors (nearest Fri), DW (cleaned), Other Monthly Preds (last Fri)
        all_final_weekly_parts = []

        # ğŸ”¥ ä¿®å¤ï¼šæ£€æŸ¥ç›®æ ‡å˜é‡é‡å¤é—®é¢˜
        target_variable_added = False

        if target_series_aligned_nearest_friday is not None and not target_series_aligned_nearest_friday.empty:
            # Ensure the target series name is set correctly before appending
            target_series_aligned_nearest_friday.name = actual_target_variable_name
            all_final_weekly_parts.append(target_series_aligned_nearest_friday)
            target_variable_added = True
            print(f"  æ·»åŠ ç›®æ ‡å˜é‡ '{actual_target_variable_name}' (æœ€è¿‘å‘¨äº”å¯¹é½)...")

        # --- NEW: Add target sheet predictors ---
        if target_sheet_predictors_aligned_nearest_friday is not None and not target_sheet_predictors_aligned_nearest_friday.empty:
            # ğŸ”¥ ä¿®å¤ï¼šæ£€æŸ¥ç›®æ ‡Sheeté¢„æµ‹å˜é‡ä¸­æ˜¯å¦åŒ…å«ç›®æ ‡å˜é‡
            target_sheet_cols = list(target_sheet_predictors_aligned_nearest_friday.columns)
            if actual_target_variable_name in target_sheet_cols:
                print(f"  âš ï¸ è­¦å‘Šï¼šç›®æ ‡Sheeté¢„æµ‹å˜é‡ä¸­åŒ…å«ç›®æ ‡å˜é‡ '{actual_target_variable_name}'")
                if target_variable_added:
                    print(f"  ğŸ”§ ç§»é™¤ç›®æ ‡Sheetä¸­çš„é‡å¤ç›®æ ‡å˜é‡")
                    target_sheet_predictors_aligned_nearest_friday = target_sheet_predictors_aligned_nearest_friday.drop(columns=[actual_target_variable_name])
                    target_sheet_cols = list(target_sheet_predictors_aligned_nearest_friday.columns)
                else:
                    print(f"  âœ… ç›®æ ‡å˜é‡å°†ä»ç›®æ ‡Sheetä¸­è·å–")
                    target_variable_added = True

            if not target_sheet_predictors_aligned_nearest_friday.empty:
                all_final_weekly_parts.append(target_sheet_predictors_aligned_nearest_friday)
                print(f"  æ·»åŠ ç›®æ ‡ Sheet é¢„æµ‹å˜é‡ ({len(target_sheet_cols)} ä¸ª, æœ€è¿‘å‘¨äº”å¯¹é½)...")
        # --- END NEW ---

        if df_combined_dw_weekly is not None and not df_combined_dw_weekly.empty:
            # ğŸ”¥ ä¿®å¤ï¼šæ£€æŸ¥æ—¥åº¦/å‘¨åº¦æ•°æ®ä¸­æ˜¯å¦åŒ…å«ç›®æ ‡å˜é‡
            dw_cols = list(df_combined_dw_weekly.columns)
            if actual_target_variable_name in dw_cols:
                print(f"  âš ï¸ è­¦å‘Šï¼šæ—¥åº¦/å‘¨åº¦æ•°æ®ä¸­åŒ…å«ç›®æ ‡å˜é‡ '{actual_target_variable_name}'")
                if target_variable_added:
                    print(f"  ğŸ”§ ç§»é™¤æ—¥åº¦/å‘¨åº¦æ•°æ®ä¸­çš„é‡å¤ç›®æ ‡å˜é‡")
                    df_combined_dw_weekly = df_combined_dw_weekly.drop(columns=[actual_target_variable_name])
                else:
                    print(f"  âœ… ç›®æ ‡å˜é‡å°†ä»æ—¥åº¦/å‘¨åº¦æ•°æ®ä¸­è·å–")
                    target_variable_added = True

            if not df_combined_dw_weekly.empty:
                all_final_weekly_parts.append(df_combined_dw_weekly)
                print(f"  æ·»åŠ æ—¥åº¦/å‘¨åº¦é¢„æµ‹å˜é‡ (Shape: {df_combined_dw_weekly.shape})...")

        if monthly_predictors_aligned_last_friday is not None and not monthly_predictors_aligned_last_friday.empty:
            # ğŸ”¥ ä¿®å¤ï¼šæ£€æŸ¥æœˆåº¦é¢„æµ‹å˜é‡ä¸­æ˜¯å¦åŒ…å«ç›®æ ‡å˜é‡
            monthly_cols = list(monthly_predictors_aligned_last_friday.columns)
            if actual_target_variable_name in monthly_cols:
                print(f"  âš ï¸ è­¦å‘Šï¼šæœˆåº¦é¢„æµ‹å˜é‡ä¸­åŒ…å«ç›®æ ‡å˜é‡ '{actual_target_variable_name}'")
                if target_variable_added:
                    print(f"  ğŸ”§ ç§»é™¤æœˆåº¦é¢„æµ‹å˜é‡ä¸­çš„é‡å¤ç›®æ ‡å˜é‡")
                    monthly_predictors_aligned_last_friday = monthly_predictors_aligned_last_friday.drop(columns=[actual_target_variable_name])
                else:
                    print(f"  âœ… ç›®æ ‡å˜é‡å°†ä»æœˆåº¦é¢„æµ‹å˜é‡ä¸­è·å–")
                    target_variable_added = True

            if not monthly_predictors_aligned_last_friday.empty:
                all_final_weekly_parts.append(monthly_predictors_aligned_last_friday)
                print(f"  æ·»åŠ å…¶ä»–æœˆåº¦é¢„æµ‹å˜é‡ (æœˆæœ«æœ€åå‘¨äº”å¯¹é½, Shape: {monthly_predictors_aligned_last_friday.shape})...")

        print(f"  ğŸ” ç›®æ ‡å˜é‡æ·»åŠ çŠ¶æ€: {'å·²æ·»åŠ ' if target_variable_added else 'æœªæ·»åŠ '}")

        if not all_final_weekly_parts:
            print("é”™è¯¯ï¼š[Data Prep] æ²¡æœ‰æˆåŠŸå¤„ç†çš„æ•°æ®éƒ¨åˆ†å¯ä»¥åˆå¹¶ã€‚æ— æ³•ç»§ç»­ã€‚")
            return None, None, None, None

        # Determine full date range using collected indices
        if not all_indices_for_range or all(idx.empty for idx in all_indices_for_range):
            print("é”™è¯¯: æ— æ³•ç¡®å®šæ—¥æœŸèŒƒå›´ï¼Œå› ä¸ºæ²¡æœ‰åŠ è½½ä»»ä½•æœ‰æ•ˆæ•°æ®ç´¢å¼•ã€‚")
            return None, None, None, None

        # Filter out empty indices before finding min/max
        valid_indices = [idx for idx in all_indices_for_range if idx is not None and not idx.empty]
        if not valid_indices:
             print("é”™è¯¯: æ‰€æœ‰æ”¶é›†åˆ°çš„ç´¢å¼•éƒ½ä¸ºç©ºï¼Œæ— æ³•ç¡®å®šæ—¥æœŸèŒƒå›´ã€‚")
             return None, None, None, None

        min_date_orig = min(idx.min() for idx in valid_indices)
        max_date_orig = max(idx.max() for idx in valid_indices)
        print(f"  æ‰€æœ‰åŸå§‹æ•°æ®ä¸­çš„æœ€å°/æœ€å¤§æ—¥æœŸ: {min_date_orig.date()} / {max_date_orig.date()}")

        # Determine the final start/end dates considering config and data
        final_start_date = pd.to_datetime(data_start_date) if data_start_date else min_date_orig
        final_end_date = pd.to_datetime(data_end_date) if data_end_date else max_date_orig
        # Adjust if config dates are outside data range
        if data_start_date and pd.to_datetime(data_start_date) < min_date_orig:
            final_start_date = pd.to_datetime(data_start_date)
        if data_end_date and pd.to_datetime(data_end_date) > max_date_orig:
            final_end_date = pd.to_datetime(data_end_date)

        # Align final start/end dates to Friday frequency
        # Ensure start date is not pushed beyond the first available data point's Friday
        # Ensure end date is not pulled before the last available data point's Friday
        min_date_fri = min_date_orig - pd.Timedelta(days=(min_date_orig.weekday() - 4 + 7) % 7)
        max_date_fri = max_date_orig - pd.Timedelta(days=(max_date_orig.weekday() - 4 + 7) % 7) + pd.Timedelta(weeks=0 if max_date_orig.weekday()==4 else 1) # Go to next Friday if not Friday

        final_start_date_aligned = final_start_date - pd.Timedelta(days=(final_start_date.weekday() - 4 + 7) % 7)
        final_end_date_aligned = final_end_date - pd.Timedelta(days=(final_end_date.weekday() - 4 + 7) % 7)

        # Respect the actual data boundaries when creating the range
        actual_range_start = max(min_date_fri, final_start_date_aligned)
        actual_range_end = min(max_date_fri, final_end_date_aligned)

        if actual_range_start > actual_range_end:
            print(f"é”™è¯¯: è®¡ç®—å‡ºçš„å®é™…å¼€å§‹æ—¥æœŸ ({actual_range_start.date()}) æ™šäºç»“æŸæ—¥æœŸ ({actual_range_end.date()})ã€‚è¯·æ£€æŸ¥æ•°æ®èŒƒå›´å’Œé…ç½®ã€‚")
            return None, None, None, None

        full_date_range = pd.date_range(start=actual_range_start, end=actual_range_end, freq=target_freq)
        print(f"  æœ€ç»ˆç¡®å®šçš„å®Œæ•´å‘¨åº¦æ—¥æœŸèŒƒå›´ (å¯¹é½åˆ° {target_freq}): {full_date_range.min().date()} åˆ° {full_date_range.max().date()}")

        # Combine and reindex
        # Use outer join initially to capture all data points on their respective Fridays
        combined_data_weekly_final = pd.concat(all_final_weekly_parts, axis=1, join='outer')
        print(f"  åˆå¹¶æ‰€æœ‰ {len(all_final_weekly_parts)} ä¸ªæœ€ç»ˆå‘¨åº¦æ•°æ®éƒ¨åˆ† (outer join). åˆå§‹åˆå¹¶ Shape: {combined_data_weekly_final.shape}")

        # ğŸ”¥ ä¼˜åŒ–ï¼šé«˜æ•ˆçš„é‡å¤åˆ—æ£€æŸ¥å’Œå¤„ç†
        print(f"  ğŸ” åˆå¹¶å‰æ£€æŸ¥é‡å¤åˆ—...")

        # ä½¿ç”¨æ›´é«˜æ•ˆçš„é‡å¤æ£€æµ‹æ–¹æ³•
        columns = combined_data_weekly_final.columns
        duplicate_mask = columns.duplicated(keep=False)

        if duplicate_mask.any():
            # åªåœ¨æœ‰é‡å¤æ—¶æ‰è¿›è¡Œè¯¦ç»†åˆ†æ
            from collections import Counter
            column_counts = Counter(columns)
            duplicated_names = {name: count for name, count in column_counts.items() if count > 1}

            print(f"  ğŸš¨ å‘ç°é‡å¤åˆ—ï¼æ€»æ•°: {duplicate_mask.sum()}")
            print(f"    é‡å¤çš„åˆ—åæ•°é‡: {len(duplicated_names)}")

            # åªæ˜¾ç¤ºå‰5ä¸ªé‡å¤åˆ—åä»¥å‡å°‘è¾“å‡º
            for i, (name, count) in enumerate(duplicated_names.items()):
                if i < 5:
                    print(f"      '{name}': {count} æ¬¡")
                elif i == 5:
                    print(f"      ... è¿˜æœ‰ {len(duplicated_names) - 5} ä¸ªé‡å¤åˆ—å")

            # ç‰¹åˆ«æ£€æŸ¥ç›®æ ‡å˜é‡é‡å¤
            if actual_target_variable_name in duplicated_names:
                target_count = duplicated_names[actual_target_variable_name]
                print(f"  âš ï¸ ç›®æ ‡å˜é‡ '{actual_target_variable_name}' é‡å¤äº† {target_count-1} æ¬¡ï¼")

        # ğŸ”¥ ä¼˜åŒ–ï¼šé«˜æ•ˆå¤„ç†é‡å¤åˆ—ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨
        original_col_count = len(combined_data_weekly_final.columns)
        duplicate_mask = combined_data_weekly_final.columns.duplicated(keep='first')

        if duplicate_mask.any():
            # åªåœ¨æœ‰é‡å¤æ—¶æ‰è¿›è¡Œå¤„ç†
            removed_count = duplicate_mask.sum()
            print(f"  ğŸ”§ åœ¨æœ€ç»ˆåˆå¹¶åå› é‡å¤ç§»é™¤äº† {removed_count} åˆ—")

            # è·å–è¢«ç§»é™¤çš„åˆ—åï¼ˆä»…ç”¨äºæ—¥å¿—è®°å½•ï¼‰
            removed_cols = combined_data_weekly_final.columns[duplicate_mask].tolist()

            # é«˜æ•ˆå»é™¤é‡å¤åˆ—
            combined_data_weekly_final = combined_data_weekly_final.iloc[:, ~duplicate_mask]

            # æ‰¹é‡è®°å½•ç§»é™¤çš„åˆ—ï¼Œé¿å…é€ä¸ªæ£€æŸ¥
            for col in removed_cols:
                removed_variables_detailed_log.append({
                    'Variable': col,
                    'Reason': 'duplicate_column_final'
                })

            print(f"    æœ€ç»ˆåˆ—æ•°: {len(combined_data_weekly_final.columns)} (å‡å°‘äº† {removed_count} åˆ—)")
        else:
            print(f"  âœ… æœªå‘ç°é‡å¤åˆ—")

        print(f"  ğŸ”§ å»é‡åå½¢çŠ¶: {combined_data_weekly_final.shape}")

        # Reindex to the full calculated date range, this aligns everything to the target frequency grid
        all_data_aligned_weekly = combined_data_weekly_final.reindex(full_date_range)
        print(f"  é‡æ–°ç´¢å¼•åˆ°å®Œæ•´å‘¨åº¦æ—¥æœŸèŒƒå›´å®Œæˆ. Shape: {all_data_aligned_weekly.shape}")

        # --- Step 6: Final Weekly Data Processing ---
        print("\n--- [Data Prep V3] æ­¥éª¤ 6: æœ€ç»ˆå‘¨åº¦æ•°æ®å¤„ç† --- ")

        # 6a: Apply Time Range Filter (already implicitly done by full_date_range calculation)
        print(f"  æ—¶é—´èŒƒå›´è¿‡æ»¤å·²åœ¨ç¡®å®šæ—¥æœŸèŒƒå›´æ—¶åº”ç”¨ ({full_date_range.min().date()} to {full_date_range.max().date()})ã€‚")

        # 6b: Drop any columns that became all NaN after all processing and reindexing
        print("  ç§»é™¤åœ¨æœ€ç»ˆå¤„ç†å’Œå¯¹é½åå®Œå…¨ä¸º NaN çš„åˆ—...")
        cols_before_final_dropna = set(all_data_aligned_weekly.columns)
        all_data_aligned_weekly = all_data_aligned_weekly.dropna(axis=1, how='all')
        removed_in_final_dropna = cols_before_final_dropna - set(all_data_aligned_weekly.columns)
        if removed_in_final_dropna:
             print(f"  [!] ç§»é™¤äº† {len(removed_in_final_dropna)} ä¸ªå…¨ NaN åˆ—: {list(removed_in_final_dropna)[:10]}{'...' if len(removed_in_final_dropna)>10 else ''}")
             # Log removals
             for col in removed_in_final_dropna:
                 if not any(d['Variable'] == col for d in removed_variables_detailed_log):
                     removed_variables_detailed_log.append({'Variable': col, 'Reason': 'all_nan_final'})
        print(f"    ç§»é™¤å…¨ NaN åˆ—å Shape: {all_data_aligned_weekly.shape}")

        if all_data_aligned_weekly is None or all_data_aligned_weekly.empty:
            print("é”™è¯¯: [Data Prep] æœ€ç»ˆåˆå¹¶å’Œå¯¹é½åçš„æ•°æ®åœ¨ç§»é™¤å…¨ NaN åˆ—åä¸ºç©ºã€‚")
            return None, None, None, None

        # 6c: Weekly Stationarity Check (Skipping MONTHLY originated predictors AND Target Variable)
        print("  æ‰§è¡Œæœ€ç»ˆå‘¨åº¦æ•°æ®å¹³ç¨³æ€§æ£€æŸ¥ (è·³è¿‡æœˆåº¦æ¥æºé¢„æµ‹å˜é‡å’Œç›®æ ‡å˜é‡)...")
        weekly_transform_log = {}
        final_data_stationary = all_data_aligned_weekly.copy() # Start with current data

        # --- Define columns to skip based on origin and target status ---
        # Start with the set of columns identified as monthly predictors after their processing
        # cols_to_skip_weekly_stationarity = monthly_predictors_to_skip_weekly_stationarity.copy() # OLD
        cols_to_skip_weekly_stationarity = other_monthly_predictors_to_skip_weekly_stationarity.copy() # Use vars from OTHER monthlies (last Fri aligned)
        print(f"    åˆå§‹è·³è¿‡åˆ—è¡¨åŸºäºå…¶ä»–æœˆåº¦æ¥æºå˜é‡: {len(cols_to_skip_weekly_stationarity)} ä¸ª")
        # Add the actual target variable name
        if actual_target_variable_name in final_data_stationary.columns:
             cols_to_skip_weekly_stationarity.add(actual_target_variable_name)
             print(f"    æ ‡è®°è·³è¿‡ç›®æ ‡å˜é‡: '{actual_target_variable_name}'")
        else:
             print(f"    è­¦å‘Šï¼šç›®æ ‡å˜é‡ '{actual_target_variable_name}' ä¸åœ¨æœ€ç»ˆæ•°æ®ä¸­ï¼Œæ— æ³•åœ¨è·³è¿‡åˆ—è¡¨ä¸­æ ‡è®°ã€‚")
        # --- NEW: Add target sheet predictors (Cols C+) to skip list ---
        target_sheet_predictor_cols_in_final = target_sheet_predictor_cols.intersection(final_data_stationary.columns)
        if target_sheet_predictor_cols_in_final:
             cols_to_skip_weekly_stationarity.update(target_sheet_predictor_cols_in_final)
             print(f"    æ ‡è®°è·³è¿‡ç›®æ ‡ Sheet é¢„æµ‹å˜é‡: {len(target_sheet_predictor_cols_in_final)} ä¸ª")
        # --- END NEW ---
        # print(f"    æ ‡è®°è·³è¿‡ {len(monthly_predictors_to_skip_weekly_stationarity)} ä¸ªæœˆåº¦æ¥æºé¢„æµ‹å˜é‡ã€‚") # OLD
        print(f"    æ€»å…±å°†æ ‡è®°è·³è¿‡ {len(cols_to_skip_weekly_stationarity)} ä¸ªå˜é‡è¿›è¡Œå‘¨åº¦å¹³ç¨³æ€§æ£€æŸ¥ã€‚")

        # --- Normalize the skip list for reliable matching ---
        skip_cols_normalized = {unicodedata.normalize('NFKC', str(c)).strip().lower() for c in cols_to_skip_weekly_stationarity}
        print(f"    è§„èŒƒåŒ–åçš„è·³è¿‡åˆ—è¡¨å¤§å°: {len(skip_cols_normalized)}")


        # --- Check for and apply pre-defined stationarity rules from config if available ---
        use_config_stationarity = False
        config_stationarity_rules = {}
        try:
            # Dynamically import config to get the latest version if it changes
            import importlib
            from dym_estimate import config
            importlib.reload(config) # Reload in case it was modified

            if hasattr(config, 'PREDEFINED_STATIONARITY_TRANSFORMS') and isinstance(config.PREDEFINED_STATIONARITY_TRANSFORMS, dict):
                 # Normalize keys in the loaded rules
                 config_stationarity_rules_raw = {
                      unicodedata.normalize('NFKC', str(k)).strip().lower(): v
                      for k, v in config.PREDEFINED_STATIONARITY_TRANSFORMS.items()
                      if isinstance(v, dict) and 'status' in v # Ensure value is dict with 'status'
                 }
                 if config_stationarity_rules_raw:
                      print(f"  æ£€æµ‹åˆ°æ¥è‡ª config.py çš„é¢„å®šä¹‰å¹³ç¨³æ€§è½¬æ¢è§„åˆ™ ({len(config_stationarity_rules_raw)} æ¡)ã€‚")

                      # --- CRITICAL FIX: Filter rules BEFORE applying ---
                      # Remove rules for columns that should be skipped (target + monthly)
                      config_stationarity_rules = {
                           k: v for k, v in config_stationarity_rules_raw.items()
                           if k not in skip_cols_normalized # Use normalized skip list
                      }
                      removed_rules_count = len(config_stationarity_rules_raw) - len(config_stationarity_rules)
                      if removed_rules_count > 0:
                          print(f"    å·²ä»é¢„å®šä¹‰è§„åˆ™ä¸­ç§»é™¤ {removed_rules_count} æ¡ï¼Œå› ä¸ºå®ƒä»¬å¯¹åº”äºéœ€è¦è·³è¿‡çš„ç›®æ ‡å˜é‡æˆ–æœˆåº¦å˜é‡ã€‚")
                      # --- END CRITICAL FIX ---

                      if config_stationarity_rules: # Check if any rules remain after filtering
                           use_config_stationarity = True
                      else:
                           print("    è¿‡æ»¤åï¼Œæ²¡æœ‰é€‚ç”¨äºæ—¥/å‘¨å˜é‡çš„é¢„å®šä¹‰è§„åˆ™ã€‚å°†å›é€€åˆ° ADF æ£€éªŒã€‚")

                 else:
                      print("  config.py ä¸­ PREDEFINED_STATIONARITY_TRANSFORMS ä¸ºç©ºæˆ–æ ¼å¼æ— æ•ˆï¼Œå°†æ‰§è¡Œ ADF æ£€éªŒã€‚")
            else:
                 print("  config.py ä¸­æœªå®šä¹‰ PREDEFINED_STATIONARITY_TRANSFORMSï¼Œå°†æ‰§è¡Œ ADF æ£€éªŒã€‚")
        except ImportError:
            print("  æ— æ³•å¯¼å…¥ config.py æˆ–å…¶ä¸å­˜åœ¨ï¼Œå°†æ‰§è¡Œ ADF æ£€éªŒã€‚")
        except Exception as e_cfg_stat:
             print(f"  åŠ è½½æˆ–å¤„ç† config.py ä¸­çš„å¹³ç¨³æ€§è§„åˆ™æ—¶å‡ºé”™: {e_cfg_stat}ã€‚å°†æ‰§è¡Œ ADF æ£€éªŒã€‚")


        # --- Apply Stationarity Transformation ---
        removed_cols_info_weekly = {} # Initialize removal log specific to this step
        if use_config_stationarity:
             print(f"  åº”ç”¨è¿‡æ»¤åçš„é¢„å®šä¹‰å¹³ç¨³æ€§è§„åˆ™ ({len(config_stationarity_rules)} æ¡è§„åˆ™)...")
             # Normalize column names of the data to match rule keys
             # Keep mapping to restore original names if needed later? Or assume rules use normalized?
             # Let's assume apply_stationarity_transforms handles matching normalized rule keys to potentially non-normalized df columns if necessary
             # Or better: normalize data columns before applying
             original_columns_map_cfg = {unicodedata.normalize('NFKC', str(c)).strip().lower(): c for c in final_data_stationary.columns}
             final_data_stationary.columns = list(original_columns_map_cfg.keys())

             final_data_stationary = apply_stationarity_transforms(
                 final_data_stationary,
                 config_stationarity_rules # Pass the FILTERED rules
             )
             # Restore original column names
             final_data_stationary.columns = final_data_stationary.columns.map(original_columns_map_cfg)

             # We don't have a detailed weekly_transform_log or removed_cols_info_weekly in this case from apply_stationarity_transforms
             weekly_transform_log = {"status": "Applied filtered rules from config"}
             # We rely on apply_stationarity_transforms to keep skipped cols untouched.
        else:
             print("  é€šè¿‡ ADF æ£€éªŒç¡®å®šå¹³ç¨³æ€§ (ä»…æ—¥/å‘¨æ¥æºå˜é‡)...")
             # Use the normalized skip list directly with _ensure_stationarity
             # The function _ensure_stationarity now handles normalization internally for matching

             # Normalize column names before passing to ensure consistent matching inside the function
             original_columns_map_adf = {unicodedata.normalize('NFKC', str(c)).strip().lower(): c for c in final_data_stationary.columns}
             final_data_stationary.columns = list(original_columns_map_adf.keys())

             final_data_stationary, weekly_transform_log, removed_cols_info_weekly = _ensure_stationarity(
                 final_data_stationary,
                 skip_cols=skip_cols_normalized, # Pass the normalized skip set
                 adf_p_threshold=0.05
             )
             # Restore original column names
             final_data_stationary.columns = final_data_stationary.columns.map(original_columns_map_adf)

             # Log weekly stationarity removals (using original names)
             for reason, cols_norm in removed_cols_info_weekly.items():
                 for col_norm in cols_norm:
                     original_col_name = original_columns_map_adf.get(col_norm, col_norm) # Get original name back
                     if not any(d['Variable'] == original_col_name for d in removed_variables_detailed_log):
                         removed_variables_detailed_log.append({'Variable': original_col_name, 'Reason': f'weekly_stationarity_{reason}'})


        # --- Step 7: Final Checks and Log Combination ---
        print("\n--- [Data Prep V3] æ­¥éª¤ 7: å®Œæˆä¸æ£€æŸ¥ --- ")
        if final_data_stationary is None or final_data_stationary.empty:
            print("é”™è¯¯: [Data Prep] æœ€ç»ˆæ•°æ®åœ¨å¹³ç¨³æ€§å¤„ç†åä¸ºç©ºã€‚")
            return None, None, None, None

        print(f"  æœ€ç»ˆæ•°æ® Shape: {final_data_stationary.shape}")
        # Check for target variable existence using the ORIGINAL name
        target_exists = actual_target_variable_name in final_data_stationary.columns
        print(f"  ç›®æ ‡å˜é‡ '{actual_target_variable_name}' æ˜¯å¦å­˜åœ¨: {target_exists}")
        if not target_exists:
             # Also check normalized name in case it was normalized and not restored correctly
             norm_target_name_final_check = unicodedata.normalize('NFKC', actual_target_variable_name).strip().lower()
             temp_cols_lower = {unicodedata.normalize('NFKC', str(c)).strip().lower():c for c in final_data_stationary.columns}
             if norm_target_name_final_check in temp_cols_lower:
                 print(f"  æ³¨æ„ï¼šç›®æ ‡å˜é‡ä»¥è§„èŒƒåŒ–åç§° '{norm_target_name_final_check}' å­˜åœ¨ã€‚")
                 # Attempt to rename back to original if found normalized
                 # final_data_stationary = final_data_stationary.rename(columns={temp_cols_lower[norm_target_name_final_check]: actual_target_variable_name})
             else:
                 print(f"  ä¸¥é‡è­¦å‘Š: ç›®æ ‡å˜é‡ '{actual_target_variable_name}' åœ¨æœ€ç»ˆæ•°æ®ä¸­ä¸å­˜åœ¨ï¼")

        # Calculate predictor count after checking target existence
        final_predictor_count_output = final_data_stationary.shape[1] - (1 if target_exists else 0)

        # Combine Logs
        combined_transform_log = {
            "monthly_predictor_stationarity_checks": monthly_transform_log, # Log from monthly check
            "weekly_final_stationarity_checks": weekly_transform_log # Log from final weekly check
        }

        # --- Finalize Industry Map ---
        # Ensure map keys are normalized and only include columns present in the final data
        final_columns_in_data = set(final_data_stationary.columns)
        updated_var_industry_map = {}
        original_names_to_norm_final = {} # Map final original names to normalized

        for col_original in final_columns_in_data:
            col_norm = unicodedata.normalize('NFKC', str(col_original)).strip().lower()
            if col_norm:
                original_names_to_norm_final[col_original] = col_norm
                # Get industry from original map using normalized key, default to "Unknown"
                # The original var_industry_map was populated during loading
                industry = var_industry_map.get(col_norm, "Unknown")
                updated_var_industry_map[col_norm] = industry # Store with normalized key

        # --- Variable Count Comparison (Using normalized names for comparison) ---
        raw_predictor_count = len(raw_columns_across_all_sheets) # All predictors initially loaded (normalized)
        reference_count = len(reference_predictor_variables) # From reference sheet (normalized)
        print(f"\n--- [Data Prep] å˜é‡æ•°é‡ä¸æŒ‡æ ‡ä½“ç³»å¯¹æ¯” ---")
        print(f"  æŒ‡æ ‡ä½“ç³»å˜é‡æ•° (è§„èŒƒåŒ–): {reference_count}")
        print(f"  åŸå§‹åŠ è½½é¢„æµ‹å˜é‡æ•° (è§„èŒƒåŒ–, ä¸å«ç›®æ ‡): {raw_predictor_count}")
        print(f"  æœ€ç»ˆè¾“å‡ºé¢„æµ‹å˜é‡æ•°: {final_predictor_count_output}")

        if reference_predictor_variables:
            # Get normalized names of final predictors
            final_output_predictors_norm = {
                 norm_name for orig_name, norm_name in original_names_to_norm_final.items()
                 if orig_name != actual_target_variable_name # Exclude target variable using original name check
            }

            raw_loaded_predictors_norm = raw_columns_across_all_sheets # Already normalized, excludes target

            # --- Compare Reference vs Raw Loaded ---
            missing_in_data = reference_predictor_variables - raw_loaded_predictors_norm
            extra_in_data = raw_loaded_predictors_norm - reference_predictor_variables

            if missing_in_data:
                 # Filter out target var if it's in the reference list by mistake
                 norm_target_name_ref_check = unicodedata.normalize('NFKC', actual_target_variable_name).strip().lower()
                 missing_to_print_norm = [v for v in sorted(list(missing_in_data)) if v != norm_target_name_ref_check]
                 if missing_to_print_norm:
                      print(f"\n  [!] ä»¥ä¸‹ {len(missing_to_print_norm)} ä¸ªå˜é‡åœ¨æŒ‡æ ‡ä½“ç³»ä¸­ï¼Œä½†æœªåœ¨ä»»ä½•æ•°æ® Sheets ä¸­åŠ è½½:")
                      for i, var_norm in enumerate(missing_to_print_norm):
                           print(f"      {i+1}. {var_norm} (è§„èŒƒå)")
                 if len(missing_to_print_norm) != len(missing_in_data):
                      print("      (æ³¨: ç›®æ ‡å˜é‡ '{norm_target_name_ref_check}' è¢«ä»è¯¥åˆ—è¡¨å¿½ç•¥)")


            if extra_in_data:
                 print(f"\n  [!] è­¦å‘Š: ä»¥ä¸‹ {len(extra_in_data)} ä¸ªå˜é‡ä»æ•°æ® Sheets åŠ è½½ï¼Œä½†ä¸åœ¨æŒ‡æ ‡ä½“ç³»ä¸­:")
                 for i, var_norm in enumerate(sorted(list(extra_in_data))):
                      # Try to find original name from the final data for display
                      original_name_guess = next((orig for orig, norm in original_names_to_norm_final.items() if norm == var_norm), var_norm)
                      print(f"      {i+1}. {original_name_guess} (è§„èŒƒå: {var_norm})")


            # --- Compare Reference vs Final Output ---
            missing_in_final_output = reference_predictor_variables - final_output_predictors_norm
            if missing_in_final_output:
                 missing_final_but_loaded = missing_in_final_output & raw_loaded_predictors_norm
                 missing_final_and_never_loaded = missing_in_final_output - raw_loaded_predictors_norm

                 if missing_final_but_loaded:
                      print(f"\n  [i] ä»¥ä¸‹ {len(missing_final_but_loaded)} ä¸ªæŒ‡æ ‡ä½“ç³»ä¸­çš„å˜é‡åœ¨åŠ è½½åã€å¤„ç†è¿‡ç¨‹ä¸­è¢«ç§»é™¤:")
                      count = 0
                      for var_norm in sorted(list(missing_final_but_loaded)):
                           count += 1
                           # Find original name if possible, fallback to norm name
                           original_name_guess = next((orig for orig, norm in original_names_to_norm_final.items() if norm == var_norm), var_norm)
                           # Find removal reason from the detailed log (match normalized name)
                           reason = "Unknown (ç§»é™¤åŸå› æœªåœ¨æ—¥å¿—ä¸­æ˜ç¡®è®°å½•)"
                           for item in removed_variables_detailed_log:
                                logged_var_norm = unicodedata.normalize('NFKC', str(item.get('Variable',''))).strip().lower()
                                if logged_var_norm == var_norm:
                                    reason = item.get('Reason', 'è®°å½•ä¸­åŸå› ç¼ºå¤±')
                                    break
                           print(f"      {count}. {original_name_guess} (è§„èŒƒå: {var_norm}) - åŸå› : {reason}")

                 # No need to print the never loaded ones again if already printed above
                 # if missing_final_and_never_loaded:
                 #     print(f"    (å¦æœ‰ {len(missing_final_and_never_loaded)} ä¸ªæŒ‡æ ‡ä½“ç³»å˜é‡ä»æœªè¢«åŠ è½½)")

            else: # All reference vars (predictors) are in the final output
                 # Check if all RAW loaded predictors made it to the final output
                 removed_loaded_predictors = raw_loaded_predictors_norm - final_output_predictors_norm
                 if removed_loaded_predictors:
                      print(f"\n  [i] æ‰€æœ‰æŒ‡æ ‡ä½“ç³»ä¸­çš„å˜é‡å‡åœ¨æœ€ç»ˆè¾“å‡ºä¸­ã€‚ä½†æœ‰ {len(removed_loaded_predictors)} ä¸ªåŠ è½½çš„å˜é‡åœ¨å¤„ç†ä¸­è¢«ç§»é™¤:")
                      # (Optional: List removed variables here using similar logic as above)
                 else:
                      print("\\n  [i] æ‰€æœ‰æŒ‡æ ‡ä½“ç³»ä¸­çš„å˜é‡éƒ½å­˜åœ¨äºæœ€ç»ˆè¾“å‡ºä¸­ï¼Œä¸”æ‰€æœ‰åŠ è½½çš„å˜é‡éƒ½æœªè¢«ç§»é™¤ã€‚")


        else: # reference_predictor_variables is empty
             print("\\n  æœªèƒ½åŠ è½½æŒ‡æ ‡ä½“ç³»è¿›è¡Œå¯¹æ¯”ã€‚")

        # --- Transformation Log Summary ---
        print(f"\n--- [Data Prep] è½¬æ¢æ—¥å¿—æ‘˜è¦ --- ")
        # Summarize monthly check results
        log_monthly = combined_transform_log.get("monthly_predictor_stationarity_checks", {})
        if isinstance(log_monthly, dict) and log_monthly:
            from collections import Counter
            monthly_statuses = Counter(log.get('status', 'unknown') for log in log_monthly.values())
            print(f"  æœˆåº¦é¢„æµ‹å˜é‡æ£€æŸ¥çŠ¶æ€ (ADF): {dict(monthly_statuses)}")
        else:
            print("  æœˆåº¦é¢„æµ‹å˜é‡æ£€æŸ¥æ—¥å¿—ä¸å¯ç”¨æˆ–ä¸ºç©ºã€‚")

        # Summarize weekly check results (handle both dict and string case)
        log_weekly = combined_transform_log.get("weekly_final_stationarity_checks", {})
        if isinstance(log_weekly, dict) and log_weekly.get("status") == "Applied filtered rules from config":
             print(f"  å‘¨åº¦æœ€ç»ˆæ£€æŸ¥çŠ¶æ€: åº”ç”¨äº†æ¥è‡ª config çš„è¿‡æ»¤åé¢„å®šä¹‰è§„åˆ™ã€‚")
        elif isinstance(log_weekly, dict) and log_weekly:
             weekly_statuses = Counter(log.get('status', 'unknown') for log in log_weekly.values())
             # Filter out 'skipped_by_request' from summary count
             filtered_weekly_statuses = {k:v for k,v in weekly_statuses.items() if k != 'skipped_by_request'}
             skipped_count = weekly_statuses.get('skipped_by_request', 0)
             print(f"  å‘¨åº¦æœ€ç»ˆæ£€æŸ¥çŠ¶æ€ (ADF, ä»…æ—¥/å‘¨æº): {dict(filtered_weekly_statuses)}")
             if skipped_count > 0: print(f"    (å¦æœ‰ {skipped_count} ä¸ªå˜é‡æŒ‰è®¡åˆ’è¢«è·³è¿‡)")
        elif isinstance(log_weekly, dict) and not log_weekly: # Empty dict means ADF run but nothing to check
             print(f"  å‘¨åº¦æœ€ç»ˆæ£€æŸ¥çŠ¶æ€ (ADF): æ— éœ€æ£€æŸ¥çš„æ—¥/å‘¨å˜é‡ã€‚")
        else: # Should be a dict, but handle unexpected cases
             print(f"  å‘¨åº¦æœ€ç»ˆæ£€æŸ¥æ—¥å¿—æ ¼å¼æœªçŸ¥æˆ–ä¸å¯ç”¨: {type(log_weekly)}")

        # --- Populate the detailed removal log (already done during process) ---
        print("\n--- [Data Prep] æ­£åœ¨ç”Ÿæˆç§»é™¤å˜é‡æ—¥å¿— ---")
        print(f"  å…±è®°å½•äº† {len(removed_variables_detailed_log)} ä¸ªç§»é™¤äº‹ä»¶ã€‚")

        print(f"\n--- [Data Prep V3] æ•°æ®å‡†å¤‡å®Œæˆ --- ")
        # --- <<< æ–°å¢ï¼šæ‰“å°æœ€ç»ˆç›®æ ‡å˜é‡æœ€åå‡ è¡Œ >>> ---
        if actual_target_variable_name in final_data_stationary.columns:
            print(f"    [Debug Target Final] æœ€ç»ˆæ•°æ®ä¸­ç›®æ ‡å˜é‡ '{actual_target_variable_name}' æœ€å 5 è¡Œ:")
            print(final_data_stationary[actual_target_variable_name].tail())
        else:
            print(f"    [Debug Target Final] è­¦å‘Šï¼šæœ€ç»ˆæ•°æ®ä¸­æœªæ‰¾åˆ°ç›®æ ‡å˜é‡ '{actual_target_variable_name}'")
        # --- <<< ç»“æŸæ–°å¢ >>> ---
        # Return data with original column names, and the updated industry map with normalized keys
        return final_data_stationary, updated_var_industry_map, combined_transform_log, removed_variables_detailed_log

    except FileNotFoundError:
        print(f"é”™è¯¯: [Data Prep] Excel æ•°æ®æ–‡ä»¶ {excel_path} æœªæ‰¾åˆ°ã€‚")
        return None, None, None, None
    except Exception as err:
        print(f"é”™è¯¯: [Data Prep] æ•°æ®å‡†å¤‡è¿‡ç¨‹ä¸­å‘ç”Ÿæ„å¤–é”™è¯¯: {err}")
        import traceback
        traceback.print_exc()
        return None, None, None, None

if __name__ == '__main__':
    print(f"Testing data_preparation module (V3 Logic, Reduced Set Mode: {CREATE_REDUCED_TEST_SET})...")

    # --- MODIFICATION: Import config values --- 
    config_loaded = False
    
    # å…ˆå°è¯•ç›´æ¥ä»å½“å‰ç›®å½•å¯¼å…¥
    try:
        import config
        config_loaded = True
        config_source = "config.py"
    except ImportError:
        # å¦‚æœå¤±è´¥ï¼Œå°è¯•ä»åŒ…å¯¼å…¥
        try:
            from dym_estimate import config
            config_loaded = True
            config_source = "dym_estimate.config"
        except ImportError:
            # ä¸¤ç§æ–¹å¼éƒ½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
            config_loaded = False
    
    # æ ¹æ®é…ç½®åŠ è½½ç»“æœè®¾ç½®å˜é‡
    if config_loaded:
        DATA_START_DATE_TEST = config.DATA_START_DATE
        DATA_END_DATE_TEST = config.DATA_END_DATE
        TARGET_VAR_TEST = config.TARGET_VARIABLE 
        TARGET_SHEET_TEST = config.TARGET_SHEET_NAME
        TARGET_FREQ_TEST = config.TARGET_FREQ
        REMOVE_CONSECUTIVE_NAN_VARS_TEST = config.REMOVE_VARS_WITH_CONSECUTIVE_NANS
        EXCEL_DATA_FILE_TEST = config.EXCEL_DATA_FILE
        
        print(f"  æˆåŠŸä» {config_source} åŠ è½½è®¾ç½®:")
        print(f"    DATA_START_DATE = {DATA_START_DATE_TEST}")
        print(f"    DATA_END_DATE = {DATA_END_DATE_TEST}")
        print(f"    TARGET_VARIABLE = {TARGET_VAR_TEST}")
        print(f"    TARGET_SHEET = {TARGET_SHEET_TEST}")
        print(f"    EXCEL_DATA_FILE = {EXCEL_DATA_FILE_TEST}")
    else:
        print("  è­¦å‘Š: æ— æ³•å¯¼å…¥ dym_estimate.configã€‚å°†ä½¿ç”¨ç¡¬ç¼–ç çš„æµ‹è¯•å€¼ã€‚")
        DATA_START_DATE_TEST = '2020-01-01' # Fallback if config fails
        DATA_END_DATE_TEST = None
        TARGET_VAR_TEST = 'è§„æ¨¡ä»¥ä¸Šå·¥ä¸šå¢åŠ å€¼:å½“æœˆåŒæ¯”'
        TARGET_SHEET_TEST = 'å·¥ä¸šå¢åŠ å€¼åŒæ¯”å¢é€Ÿ_æœˆåº¦_åŒèŠ±é¡º'
        TARGET_FREQ_TEST = 'W-FRI'  # æ·»åŠ é»˜è®¤é¢‘ç‡è®¾ç½®
        REMOVE_CONSECUTIVE_NAN_VARS_TEST = True  # é»˜è®¤å¯ç”¨ç§»é™¤è¿ç»­ç¼ºå¤±å˜é‡
        # --- æ·»åŠ ï¼šconfig åŠ è½½å¤±è´¥æ—¶çš„å›é€€æ–‡ä»¶è·¯å¾„ ---
        EXCEL_DATA_FILE_TEST = os.path.join('data', 'ç»æµæ•°æ®åº“0508.xlsx')
        # --- ç»“æŸæ·»åŠ  ---
    # --- END MODIFICATION ---

    # --- åˆ é™¤ï¼šç§»é™¤æ—§çš„ç¡¬ç¼–ç æ–‡ä»¶è·¯å¾„è®¾ç½® ---
    # Use hardcoded test values for file paths etc.
    # EXCEL_DATA_FILE_TEST = os.path.join('data', 'ç»æµæ•°æ®åº“0424_å¸¦æ•°æ®æºæ ‡å¿—.xlsx')
    # --- ç»“æŸåˆ é™¤ ---
    TARGET_FREQ_TEST = 'W-FRI'
    # TARGET_SHEET_TEST = 'å·¥ä¸šå¢åŠ å€¼åŒæ¯”å¢é€Ÿ_æœˆåº¦_åŒèŠ±é¡º' # Now loaded from config
    # TARGET_VAR_TEST_FROM_CONFIG = 'è§„æ¨¡ä»¥ä¸Šå·¥ä¸šå¢åŠ å€¼:å½“æœˆåŒæ¯”' # Now loaded from config
    CONSECUTIVE_NAN_THRESHOLD_TEST = 10 # Set desired threshold
    REFERENCE_SHEET_TEST = 'æŒ‡æ ‡ä½“ç³»'
    REFERENCE_COL_TEST = 'é«˜é¢‘æŒ‡æ ‡'

    # Check test file exists
    # --- ä¿®æ”¹ï¼šç§»é™¤å¤‡ç”¨è·¯å¾„æ£€æŸ¥ --- 
    if not os.path.exists(EXCEL_DATA_FILE_TEST):
         # alt_path = os.path.join('..', 'data', 'ç»æµæ•°æ®åº“0424.xlsx')
         # if os.path.exists(alt_path):
         #     EXCEL_DATA_FILE_TEST = alt_path
         # else:
         print(f"é”™è¯¯: æµ‹è¯•æ–‡ä»¶æœªæ‰¾åˆ°äº '{EXCEL_DATA_FILE_TEST}'ã€‚è¯·æ£€æŸ¥ config.py ä¸­çš„ EXCEL_DATA_FILE è®¾ç½®æˆ–æ–‡ä»¶æ˜¯å¦å­˜åœ¨ã€‚")
         sys.exit(1)
    # --- ç»“æŸä¿®æ”¹ ---

    print(f"  æµ‹è¯•æ–‡ä»¶: {EXCEL_DATA_FILE_TEST}")
    print(f"  ç›®æ ‡é¢‘ç‡: {TARGET_FREQ_TEST}")
    print(f"  ç›®æ ‡Sheet: {TARGET_SHEET_TEST}") # Use value from config/fallback
    print(f"  ç›®æ ‡å˜é‡(é¢„æœŸBåˆ—): {TARGET_VAR_TEST}") # Use value from config/fallback
    print(f"  è¿ç»­ NaN é˜ˆå€¼: {CONSECUTIVE_NAN_THRESHOLD_TEST}")
    print(f"  æ•°æ®å¼€å§‹æ—¥æœŸ (æ¥è‡ªé…ç½®): {DATA_START_DATE_TEST}") # Print loaded date
    print(f"  æ•°æ®ç»“æŸæ—¥æœŸ (æ¥è‡ªé…ç½®): {DATA_END_DATE_TEST}") # Print loaded date

    # Call the revised prepare_data function
    prepared_data, industry_map, transform_log, removed_variables_detailed_log = prepare_data(
                                 excel_path=EXCEL_DATA_FILE_TEST,
                                 target_freq=TARGET_FREQ_TEST,
                                 target_sheet_name=TARGET_SHEET_TEST, # Use loaded value
                                 target_variable_name=TARGET_VAR_TEST, # Use loaded value
                                 consecutive_nan_threshold=CONSECUTIVE_NAN_THRESHOLD_TEST,
                                 data_start_date=DATA_START_DATE_TEST, # Use loaded value
                                 data_end_date=DATA_END_DATE_TEST,   # Use loaded value
                                 reference_sheet_name=REFERENCE_SHEET_TEST,
                                 reference_column_name=REFERENCE_COL_TEST
                                 )

    if prepared_data is not None and removed_variables_detailed_log is not None:
        # ç§»é™¤æ–‡ä»¶è¾“å‡ºåŠŸèƒ½ï¼Œæ”¹ä¸ºä»…è¿”å›å¤„ç†ç»“æœ
        print("æ•°æ®å‡†å¤‡å®Œæˆï¼Œç»“æœå°†é€šè¿‡è¿”å›å€¼ä¼ é€’ï¼Œä¸å†ä¿å­˜åˆ°æ–‡ä»¶")

        print(f"\n--- Prepared Weekly Data Info (V3 Logic, {'Reduced' if CREATE_REDUCED_TEST_SET else 'Full'}) ---")
        buffer = io.StringIO()
        prepared_data.info(buf=buffer)
        print(buffer.getvalue())

        print(f"\n--- Prepared Weekly Data Head (5 rows, 10 cols) ---")
        print(prepared_data.iloc[:5, :min(10, prepared_data.shape[1])])
        print(f"\n--- Prepared Weekly Data Tail (5 rows, 10 cols) ---")
        print(prepared_data.iloc[-5:, :min(10, prepared_data.shape[1])])

        print("æ•°æ®å‡†å¤‡å®Œæˆï¼Œæ‰€æœ‰ç»“æœé€šè¿‡è¿”å›å€¼ä¼ é€’ï¼Œä¸ä¿å­˜åˆ°æ–‡ä»¶")
    else:
        print(f"\n(V3 Logic) å‘¨åº¦æ•°æ®å‡†å¤‡åœ¨æµ‹è¯•æœŸé—´å¤±è´¥ã€‚")

